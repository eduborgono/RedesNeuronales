{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "### 1. Back-propagation (BP) from *Scratch*\n",
    "\n",
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cross_entropy(empirico, teorico):\n",
    "    emp_acotado = np.clip(empirico, 1e-12, 1.0-1e-12)\n",
    "    return -(teorico*np.log(emp_acotado)).sum()\n",
    "\n",
    "\n",
    "def mse(empirico, teorico):\n",
    "    return (1 / 2) * np.sum(np.square(np.subtract(empirico, teorico)))\n",
    "\n",
    "\n",
    "def relu(matriz):\n",
    "    return matriz * (matriz > 0)\n",
    "\n",
    "\n",
    "def relu_derivada(matriz):\n",
    "    return 1.0 * (matriz > 0)\n",
    "\n",
    "\n",
    "def sigmoid(matriz):\n",
    "    return 1.0 / (1.0 + np.exp(-matriz))\n",
    "\n",
    "\n",
    "def sigmoid_derivada(matriz):\n",
    "    return sigmoid(matriz) * (1.0 - sigmoid(matriz))\n",
    "\n",
    "\n",
    "def softmax(matriz):\n",
    "    a = np.max(matriz)\n",
    "    m = np.subtract(matriz, a)\n",
    "    e_x = np.exp(m)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def softmax_derivada(matriz):\n",
    "    return softmax(matriz) * (1 - softmax(matriz))\n",
    "\n",
    "\n",
    "class Capa(object):\n",
    "    def __init__(self, z=None, a=None, w=None, funcion_activacion=None, der_func_activacion=None):\n",
    "        self.z = z\n",
    "        self.a = a\n",
    "        self.w = w\n",
    "        self.dw = None\n",
    "        self.g_prima = None\n",
    "        self.d = None\n",
    "        self.funcion_delta = funcion_activacion\n",
    "        self.funcion_delta_prima = der_func_activacion\n",
    "\n",
    "\n",
    "class RedNeuronal(object):\n",
    "    def __init__(self, nro_entradas, k, loss=\"mse\", activacion=[sigmoid, sigmoid_derivada]):\n",
    "        self.nro_entradas = nro_entradas\n",
    "        self.k = k\n",
    "        self.loss = loss\n",
    "        self.entrada = Capa(a=np.ones((1, nro_entradas)))\n",
    "        self.capa1 = Capa(w=np.random.rand(nro_entradas, 32),\n",
    "                          funcion_activacion=activacion[0],\n",
    "                          der_func_activacion=activacion[1])\n",
    "        self.capa2 = Capa(w=np.random.rand(32, 16),\n",
    "                          funcion_activacion=activacion[0],\n",
    "                          der_func_activacion=activacion[1])\n",
    "        self.salida = Capa(w=np.random.rand(16, k),\n",
    "                           funcion_activacion=softmax,\n",
    "                           der_func_activacion=softmax_derivada)\n",
    "        \n",
    "    def forward_pass(self, vector_entrada):\n",
    "        self.entrada.a = vector_entrada\n",
    "        capas = [self.entrada, self.capa1, self.capa2, self.salida]\n",
    "        for i1 in range(len(capas)-1):\n",
    "            capas[i1 + 1].z = np.dot(capas[i1].a, capas[i1+1].w)\n",
    "            capas[i1 + 1].a = capas[i1+1].funcion_delta(capas[i1 + 1].z)\n",
    "            capas[i1 + 1].g_prima = capas[i1+1].funcion_delta_prima(capas[i1 + 1].z)\n",
    "    \n",
    "    def backward_pass(self, ys_teoricas):\n",
    "        capas = [self.entrada, self.capa1, self.capa2, self.salida]\n",
    "        if self.loss == \"mse\":\n",
    "            capas[-1].d = np.multiply(np.subtract(capas[-1].a, ys_teoricas), capas[-1].g_prima)\n",
    "        elif self.loss == \"categorical_crossentropy\":\n",
    "            capas[-1].d = capas[-1].a - ys_teoricas\n",
    "        for i1 in range(len(capas) - 2, 0, -1):\n",
    "            capas[i1].d = np.multiply(capas[i1].g_prima, np.dot(capas[i1 + 1].d, capas[i1 + 1].w.T))\n",
    "        for i2 in range(len(capas) - 1):\n",
    "            capas[i2 + 1].dw = np.dot(capas[i2].a.T, capas[i2 + 1].d)\n",
    "\n",
    "    def stochastic_gradient_descent(self, ejemplos, epochs, tasa_aprendizaje):\n",
    "        costes = list()\n",
    "        for e in range(epochs):\n",
    "            coste = 0\n",
    "            np.random.shuffle(ejemplos)\n",
    "            for v in ejemplos:\n",
    "                self.forward_pass(v[0:self.nro_entradas][np.newaxis])\n",
    "                self.backward_pass(v[self.nro_entradas:][np.newaxis])\n",
    "                capas = [self.entrada, self.capa1, self.capa2, self.salida]\n",
    "                for c in range(len(capas)-1, 0, -1):\n",
    "                    if not self.momentum:\n",
    "                        capas[c].w -= tasa_aprendizaje*capas[c].dw\n",
    "                    else:\n",
    "                        capas[c].v = 0.5*capas[c].v - tasa_aprendizaje*capas[c].dw\n",
    "                        capas[c].w += capas[c].v\n",
    "                perdida = 0\n",
    "                if self.loss == \"mse\":\n",
    "                    perdida = mse(self.salida.a, v[self.nro_entradas:][np.newaxis])\n",
    "                elif self.loss == \"categorical_crossentropy\":\n",
    "                    perdida = cross_entropy(self.salida.a, v[self.nro_entradas:][np.newaxis])\n",
    "                coste += perdida\n",
    "\n",
    "            costes.append(coste/ejemplos.shape[0])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
