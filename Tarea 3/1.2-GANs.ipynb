{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Se define el discriminador de la GAN con 3 bloque convolucionales y una tanda fully conected, con los Dropout para evitar overfitting. Se utilizará la funcion de activación Leaky ReLU, para permitir que los gradientes fluyan hacia atrás a través de la capa sin obstáculos. Se trabajará con el dataset MNIST de keras y a continuación se presentará la arquitectura que conforma al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import keras\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "img_rows, img_cols,channel = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 5,222,401\n",
      "Trainable params: 5,222,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python3.6\\lib\\site-packages\\keras\\activations.py:115: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model,Sequential\n",
    "from keras.layers import LeakyReLU,Conv2D,Dropout,Flatten,Dense\n",
    "## Discriminator\n",
    "D = Sequential()\n",
    "depth = 64\n",
    "dropout = 0.4\n",
    "input_shape = (img_rows, img_cols, channel)\n",
    "D.add(Conv2D(depth*1, (5,5), strides=2, input_shape=input_shape,padding='same', activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "D.add(Conv2D(depth*2, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "D.add(Conv2D(depth*4, (5,5), strides=2, padding='same',activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dropout(dropout))\n",
    "D.add(Flatten())\n",
    "D.add(Dense(1024,activation=LeakyReLU(alpha=0.2)))\n",
    "D.add(Dense(1,activation='sigmoid'))\n",
    "D.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) A continuación, definiremos el generador de la GAN, con una tanda fully conected y 3 bloque convolucionales transpuesta además de agregar BatchNormalization entre ellas para tener un aprendizaje más estable, cuya función de salida es sigmoidal, porque el output del generador debe variar entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12544)             1618176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,697,505\n",
      "Trainable params: 1,672,033\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization,Reshape,UpSampling2D,Conv2DTranspose,Activation\n",
    "## Generator\n",
    "G = Sequential()\n",
    "dim = 14\n",
    "input_dim= 2 #para que sea similar al vAE\n",
    "G.add(Dense(128, input_dim=input_dim))\n",
    "G.add(BatchNormalization())\n",
    "G.add(Activation('relu'))\n",
    "G.add(Dense(dim*dim*depth))\n",
    "G.add(BatchNormalization())\n",
    "G.add(Activation('relu'))\n",
    "G.add(Reshape((dim, dim, depth)))\n",
    "G.add(Conv2DTranspose(int(depth/2), (3,3), padding='same',strides=(2,2)))\n",
    "G.add(BatchNormalization())\n",
    "G.add(Activation('relu'))\n",
    "G.add(Conv2DTranspose(int(depth/2), (3,3), padding='same'))\n",
    "G.add(BatchNormalization())\n",
    "G.add(Activation('relu'))\n",
    "G.add(Conv2DTranspose(channel, (3,3), padding='same')) \n",
    "G.add(Activation('sigmoid')) \n",
    "G.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Se conectan los modelos a través del enfoque adversario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "## Discriminator model (police)\n",
    "optimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
    "DM = Sequential()\n",
    "DM.add(D)\n",
    "DM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "## Adversarial model (Generator->Discriminator)\n",
    "D.trainable=False #set the discriminator freeze  (fixed params)\n",
    "optimizer = RMSprop(lr=0.0004, clipvalue=1.0, decay=3e-8)\n",
    "AM = Sequential()\n",
    "AM.add(G)\n",
    "AM.add(D)\n",
    "AM.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)Se entrena el modelo definido con un enfoque iterativo ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python3.6\\lib\\site-packages\\keras\\engine\\training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "0: [D loss: 0.691897, acc: 0.546875]  [G loss: 3.425355, acc: 0.000000]\n",
      "1: [D loss: 0.655931, acc: 0.507812]  [G loss: 0.000006, acc: 1.000000]\n",
      "2: [D loss: 6.191518, acc: 0.500000]  [G loss: 2.967941, acc: 0.000000]\n",
      "3: [D loss: 1.760207, acc: 0.500000]  [G loss: 0.001039, acc: 1.000000]\n",
      "4: [D loss: 3.907363, acc: 0.500000]  [G loss: 1.290272, acc: 0.000000]\n",
      "5: [D loss: 0.427510, acc: 1.000000]  [G loss: 1.936907, acc: 0.000000]\n",
      "6: [D loss: 0.333792, acc: 1.000000]  [G loss: 2.607687, acc: 0.000000]\n",
      "7: [D loss: 0.290353, acc: 0.992188]  [G loss: 3.216900, acc: 0.000000]\n",
      "8: [D loss: 0.238693, acc: 0.960938]  [G loss: 1.344268, acc: 0.078125]\n",
      "9: [D loss: 0.386923, acc: 0.796875]  [G loss: 15.359627, acc: 0.000000]\n",
      "10: [D loss: 4.181634, acc: 0.500000]  [G loss: 0.625612, acc: 0.765625]\n",
      "11: [D loss: 0.430609, acc: 0.773438]  [G loss: 0.776045, acc: 0.437500]\n",
      "12: [D loss: 0.327071, acc: 0.953125]  [G loss: 1.041419, acc: 0.046875]\n",
      "13: [D loss: 0.274288, acc: 0.976562]  [G loss: 1.460608, acc: 0.000000]\n",
      "14: [D loss: 0.220656, acc: 0.992188]  [G loss: 1.450210, acc: 0.015625]\n",
      "15: [D loss: 0.218460, acc: 0.984375]  [G loss: 2.136416, acc: 0.000000]\n",
      "16: [D loss: 0.191970, acc: 0.968750]  [G loss: 1.034234, acc: 0.171875]\n",
      "17: [D loss: 0.163354, acc: 0.945312]  [G loss: 2.779077, acc: 0.000000]\n",
      "18: [D loss: 0.215649, acc: 0.937500]  [G loss: 0.193459, acc: 0.984375]\n",
      "19: [D loss: 0.416114, acc: 0.796875]  [G loss: 6.075792, acc: 0.000000]\n",
      "20: [D loss: 0.990226, acc: 0.523438]  [G loss: 0.131330, acc: 1.000000]\n",
      "21: [D loss: 0.697655, acc: 0.562500]  [G loss: 1.045693, acc: 0.031250]\n",
      "22: [D loss: 0.215982, acc: 1.000000]  [G loss: 1.298107, acc: 0.000000]\n",
      "23: [D loss: 0.152305, acc: 1.000000]  [G loss: 1.274181, acc: 0.015625]\n",
      "24: [D loss: 0.144764, acc: 1.000000]  [G loss: 1.694293, acc: 0.000000]\n",
      "25: [D loss: 0.096221, acc: 1.000000]  [G loss: 1.138055, acc: 0.281250]\n",
      "26: [D loss: 0.193844, acc: 0.921875]  [G loss: 3.498792, acc: 0.000000]\n",
      "27: [D loss: 0.309378, acc: 0.835938]  [G loss: 0.048715, acc: 1.000000]\n",
      "28: [D loss: 0.819010, acc: 0.578125]  [G loss: 3.138871, acc: 0.000000]\n",
      "29: [D loss: 0.390677, acc: 0.773438]  [G loss: 0.361108, acc: 1.000000]\n",
      "30: [D loss: 0.281767, acc: 0.906250]  [G loss: 1.613182, acc: 0.000000]\n",
      "31: [D loss: 0.102320, acc: 1.000000]  [G loss: 1.187764, acc: 0.265625]\n",
      "32: [D loss: 0.097758, acc: 1.000000]  [G loss: 1.419636, acc: 0.187500]\n",
      "33: [D loss: 0.092816, acc: 0.984375]  [G loss: 1.921876, acc: 0.109375]\n",
      "34: [D loss: 0.101258, acc: 0.984375]  [G loss: 0.223850, acc: 0.968750]\n",
      "35: [D loss: 0.189230, acc: 0.929688]  [G loss: 5.062300, acc: 0.000000]\n",
      "36: [D loss: 0.716095, acc: 0.664062]  [G loss: 0.017359, acc: 1.000000]\n",
      "37: [D loss: 1.379998, acc: 0.500000]  [G loss: 0.595177, acc: 0.781250]\n",
      "38: [D loss: 0.300411, acc: 0.992188]  [G loss: 0.792818, acc: 0.296875]\n",
      "39: [D loss: 0.195670, acc: 1.000000]  [G loss: 0.894192, acc: 0.218750]\n",
      "40: [D loss: 0.155819, acc: 1.000000]  [G loss: 1.138110, acc: 0.015625]\n",
      "41: [D loss: 0.106650, acc: 1.000000]  [G loss: 0.639249, acc: 0.687500]\n",
      "42: [D loss: 0.341192, acc: 0.828125]  [G loss: 4.462048, acc: 0.000000]\n",
      "43: [D loss: 0.703477, acc: 0.632812]  [G loss: 0.075498, acc: 1.000000]\n",
      "44: [D loss: 0.847281, acc: 0.500000]  [G loss: 0.933460, acc: 0.000000]\n",
      "45: [D loss: 0.182768, acc: 1.000000]  [G loss: 0.832487, acc: 0.312500]\n",
      "46: [D loss: 0.146683, acc: 1.000000]  [G loss: 1.130808, acc: 0.046875]\n",
      "47: [D loss: 0.112829, acc: 0.992188]  [G loss: 0.529756, acc: 0.718750]\n",
      "48: [D loss: 0.294575, acc: 0.820312]  [G loss: 3.737741, acc: 0.000000]\n",
      "49: [D loss: 0.450571, acc: 0.742188]  [G loss: 0.096534, acc: 1.000000]\n",
      "50: [D loss: 0.644486, acc: 0.507812]  [G loss: 1.735567, acc: 0.000000]\n",
      "51: [D loss: 0.206198, acc: 0.976562]  [G loss: 0.469860, acc: 0.906250]\n",
      "52: [D loss: 0.256985, acc: 0.929688]  [G loss: 2.165237, acc: 0.000000]\n",
      "53: [D loss: 0.218471, acc: 0.906250]  [G loss: 0.227022, acc: 0.984375]\n",
      "54: [D loss: 0.311575, acc: 0.851562]  [G loss: 2.692688, acc: 0.000000]\n",
      "55: [D loss: 0.250335, acc: 0.890625]  [G loss: 0.188487, acc: 0.984375]\n",
      "56: [D loss: 0.524276, acc: 0.578125]  [G loss: 2.865468, acc: 0.000000]\n",
      "57: [D loss: 0.413066, acc: 0.742188]  [G loss: 0.205868, acc: 1.000000]\n",
      "58: [D loss: 0.291369, acc: 0.859375]  [G loss: 0.994173, acc: 0.187500]\n",
      "59: [D loss: 0.153291, acc: 1.000000]  [G loss: 1.059222, acc: 0.187500]\n",
      "60: [D loss: 0.148800, acc: 0.992188]  [G loss: 0.503162, acc: 0.843750]\n",
      "61: [D loss: 0.213511, acc: 0.914062]  [G loss: 2.779837, acc: 0.000000]\n",
      "62: [D loss: 0.266438, acc: 0.890625]  [G loss: 0.033565, acc: 1.000000]\n",
      "63: [D loss: 0.966277, acc: 0.500000]  [G loss: 1.975423, acc: 0.000000]\n",
      "64: [D loss: 0.390005, acc: 0.796875]  [G loss: 0.262792, acc: 1.000000]\n",
      "65: [D loss: 0.472546, acc: 0.578125]  [G loss: 1.520823, acc: 0.015625]\n",
      "66: [D loss: 0.259853, acc: 0.929688]  [G loss: 0.400346, acc: 0.890625]\n",
      "67: [D loss: 0.305401, acc: 0.851562]  [G loss: 2.143134, acc: 0.000000]\n",
      "68: [D loss: 0.318910, acc: 0.820312]  [G loss: 0.125536, acc: 1.000000]\n",
      "69: [D loss: 0.626132, acc: 0.515625]  [G loss: 1.396888, acc: 0.046875]\n",
      "70: [D loss: 0.307626, acc: 0.882812]  [G loss: 0.305615, acc: 1.000000]\n",
      "71: [D loss: 0.353664, acc: 0.828125]  [G loss: 1.734349, acc: 0.000000]\n",
      "72: [D loss: 0.253282, acc: 0.898438]  [G loss: 0.150255, acc: 1.000000]\n",
      "73: [D loss: 0.542786, acc: 0.539062]  [G loss: 2.175812, acc: 0.000000]\n",
      "74: [D loss: 0.399597, acc: 0.757812]  [G loss: 0.225334, acc: 1.000000]\n",
      "75: [D loss: 0.592538, acc: 0.515625]  [G loss: 1.553130, acc: 0.000000]\n",
      "76: [D loss: 0.326828, acc: 0.851562]  [G loss: 0.489936, acc: 0.812500]\n",
      "77: [D loss: 0.318206, acc: 0.929688]  [G loss: 2.480297, acc: 0.000000]\n",
      "78: [D loss: 0.423464, acc: 0.773438]  [G loss: 0.125853, acc: 1.000000]\n",
      "79: [D loss: 1.000870, acc: 0.500000]  [G loss: 1.143405, acc: 0.000000]\n",
      "80: [D loss: 0.444154, acc: 0.734375]  [G loss: 0.966635, acc: 0.031250]\n",
      "81: [D loss: 0.372311, acc: 0.937500]  [G loss: 1.205311, acc: 0.000000]\n",
      "82: [D loss: 0.264741, acc: 0.953125]  [G loss: 0.597941, acc: 0.734375]\n",
      "83: [D loss: 0.461130, acc: 0.796875]  [G loss: 3.924729, acc: 0.000000]\n",
      "84: [D loss: 1.005362, acc: 0.562500]  [G loss: 0.205029, acc: 1.000000]\n",
      "85: [D loss: 0.871729, acc: 0.500000]  [G loss: 0.889589, acc: 0.031250]\n",
      "86: [D loss: 0.518183, acc: 0.867188]  [G loss: 1.048053, acc: 0.000000]\n",
      "87: [D loss: 0.432187, acc: 0.906250]  [G loss: 1.139132, acc: 0.000000]\n",
      "88: [D loss: 0.422809, acc: 0.859375]  [G loss: 0.992168, acc: 0.218750]\n",
      "89: [D loss: 0.397167, acc: 0.875000]  [G loss: 2.325321, acc: 0.000000]\n",
      "90: [D loss: 0.624016, acc: 0.593750]  [G loss: 0.096783, acc: 1.000000]\n",
      "91: [D loss: 1.081863, acc: 0.500000]  [G loss: 1.012264, acc: 0.140625]\n",
      "92: [D loss: 0.494830, acc: 0.710938]  [G loss: 0.597165, acc: 0.859375]\n",
      "93: [D loss: 0.440020, acc: 0.906250]  [G loss: 1.040795, acc: 0.203125]\n",
      "94: [D loss: 0.405291, acc: 0.851562]  [G loss: 0.581286, acc: 0.781250]\n",
      "95: [D loss: 0.381385, acc: 0.875000]  [G loss: 2.378442, acc: 0.000000]\n",
      "96: [D loss: 0.610737, acc: 0.625000]  [G loss: 0.194183, acc: 1.000000]\n",
      "97: [D loss: 0.924887, acc: 0.500000]  [G loss: 0.818278, acc: 0.093750]\n",
      "98: [D loss: 0.464925, acc: 0.890625]  [G loss: 1.216784, acc: 0.000000]\n",
      "99: [D loss: 0.440794, acc: 0.820312]  [G loss: 0.760772, acc: 0.406250]\n",
      "100: [D loss: 0.509165, acc: 0.750000]  [G loss: 1.654963, acc: 0.015625]\n",
      "101: [D loss: 0.526585, acc: 0.656250]  [G loss: 0.284931, acc: 1.000000]\n",
      "102: [D loss: 0.761050, acc: 0.507812]  [G loss: 0.742079, acc: 0.468750]\n",
      "103: [D loss: 0.493941, acc: 0.882812]  [G loss: 1.431607, acc: 0.031250]\n",
      "104: [D loss: 0.501108, acc: 0.710938]  [G loss: 0.380735, acc: 0.984375]\n",
      "105: [D loss: 0.629758, acc: 0.601562]  [G loss: 1.154199, acc: 0.093750]\n",
      "106: [D loss: 0.429312, acc: 0.820312]  [G loss: 0.435511, acc: 1.000000]\n",
      "107: [D loss: 0.512959, acc: 0.703125]  [G loss: 2.094850, acc: 0.000000]\n",
      "108: [D loss: 0.588850, acc: 0.640625]  [G loss: 0.140223, acc: 1.000000]\n",
      "109: [D loss: 0.819370, acc: 0.500000]  [G loss: 0.645848, acc: 0.625000]\n",
      "110: [D loss: 0.526124, acc: 0.890625]  [G loss: 0.884217, acc: 0.312500]\n",
      "111: [D loss: 0.462679, acc: 0.820312]  [G loss: 0.768262, acc: 0.390625]\n",
      "112: [D loss: 0.440484, acc: 0.882812]  [G loss: 1.702529, acc: 0.031250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113: [D loss: 0.550192, acc: 0.648438]  [G loss: 0.180987, acc: 1.000000]\n",
      "114: [D loss: 0.753819, acc: 0.500000]  [G loss: 0.663549, acc: 0.640625]\n",
      "115: [D loss: 0.517217, acc: 0.835938]  [G loss: 1.010108, acc: 0.328125]\n",
      "116: [D loss: 0.415675, acc: 0.843750]  [G loss: 0.598445, acc: 0.671875]\n",
      "117: [D loss: 0.464178, acc: 0.804688]  [G loss: 1.435236, acc: 0.125000]\n",
      "118: [D loss: 0.497502, acc: 0.718750]  [G loss: 0.156826, acc: 1.000000]\n",
      "119: [D loss: 0.709271, acc: 0.500000]  [G loss: 0.939816, acc: 0.234375]\n",
      "120: [D loss: 0.418204, acc: 0.875000]  [G loss: 1.116118, acc: 0.218750]\n",
      "121: [D loss: 0.396671, acc: 0.875000]  [G loss: 0.702838, acc: 0.625000]\n",
      "122: [D loss: 0.417975, acc: 0.796875]  [G loss: 3.252560, acc: 0.000000]\n",
      "123: [D loss: 1.005889, acc: 0.570312]  [G loss: 0.130641, acc: 1.000000]\n",
      "124: [D loss: 1.018844, acc: 0.500000]  [G loss: 0.684983, acc: 0.625000]\n",
      "125: [D loss: 0.553821, acc: 0.726562]  [G loss: 0.765249, acc: 0.328125]\n",
      "126: [D loss: 0.559296, acc: 0.695312]  [G loss: 0.795410, acc: 0.328125]\n",
      "127: [D loss: 0.568535, acc: 0.671875]  [G loss: 1.031904, acc: 0.203125]\n",
      "128: [D loss: 0.556550, acc: 0.703125]  [G loss: 0.822577, acc: 0.406250]\n",
      "129: [D loss: 0.486148, acc: 0.828125]  [G loss: 1.817484, acc: 0.031250]\n",
      "130: [D loss: 0.574432, acc: 0.679688]  [G loss: 0.308994, acc: 1.000000]\n",
      "131: [D loss: 0.716020, acc: 0.523438]  [G loss: 0.829648, acc: 0.484375]\n",
      "132: [D loss: 0.469296, acc: 0.843750]  [G loss: 0.695462, acc: 0.562500]\n",
      "133: [D loss: 0.460613, acc: 0.820312]  [G loss: 1.399683, acc: 0.140625]\n",
      "134: [D loss: 0.577975, acc: 0.726562]  [G loss: 0.360868, acc: 0.937500]\n",
      "135: [D loss: 0.575620, acc: 0.617188]  [G loss: 1.202380, acc: 0.421875]\n",
      "136: [D loss: 0.480247, acc: 0.828125]  [G loss: 0.606608, acc: 0.734375]\n",
      "137: [D loss: 0.549074, acc: 0.664062]  [G loss: 2.463373, acc: 0.015625]\n",
      "138: [D loss: 0.511935, acc: 0.750000]  [G loss: 0.246733, acc: 1.000000]\n",
      "139: [D loss: 0.908994, acc: 0.500000]  [G loss: 1.020845, acc: 0.078125]\n",
      "140: [D loss: 0.464800, acc: 0.859375]  [G loss: 1.225984, acc: 0.078125]\n",
      "141: [D loss: 0.546830, acc: 0.679688]  [G loss: 0.977782, acc: 0.078125]\n",
      "142: [D loss: 0.530170, acc: 0.789062]  [G loss: 1.314108, acc: 0.031250]\n",
      "143: [D loss: 0.578516, acc: 0.671875]  [G loss: 0.538724, acc: 0.796875]\n",
      "144: [D loss: 0.566173, acc: 0.695312]  [G loss: 1.253114, acc: 0.140625]\n",
      "145: [D loss: 0.503281, acc: 0.742188]  [G loss: 0.336124, acc: 0.953125]\n",
      "146: [D loss: 0.604754, acc: 0.539062]  [G loss: 0.969763, acc: 0.093750]\n",
      "147: [D loss: 0.479996, acc: 0.851562]  [G loss: 1.228892, acc: 0.078125]\n",
      "148: [D loss: 0.387976, acc: 0.875000]  [G loss: 0.793867, acc: 0.390625]\n",
      "149: [D loss: 0.460341, acc: 0.812500]  [G loss: 3.738127, acc: 0.000000]\n",
      "150: [D loss: 1.013675, acc: 0.578125]  [G loss: 0.219257, acc: 1.000000]\n",
      "151: [D loss: 0.819455, acc: 0.500000]  [G loss: 0.948961, acc: 0.078125]\n",
      "152: [D loss: 0.485692, acc: 0.867188]  [G loss: 1.146435, acc: 0.078125]\n",
      "153: [D loss: 0.480120, acc: 0.789062]  [G loss: 1.125004, acc: 0.109375]\n",
      "154: [D loss: 0.466055, acc: 0.789062]  [G loss: 1.111971, acc: 0.187500]\n",
      "155: [D loss: 0.441779, acc: 0.804688]  [G loss: 1.256785, acc: 0.046875]\n",
      "156: [D loss: 0.394937, acc: 0.851562]  [G loss: 1.047601, acc: 0.171875]\n",
      "157: [D loss: 0.461467, acc: 0.765625]  [G loss: 1.674319, acc: 0.062500]\n",
      "158: [D loss: 0.422697, acc: 0.812500]  [G loss: 0.625428, acc: 0.656250]\n",
      "159: [D loss: 0.585425, acc: 0.664062]  [G loss: 3.358443, acc: 0.000000]\n",
      "160: [D loss: 0.939872, acc: 0.578125]  [G loss: 0.496898, acc: 0.875000]\n",
      "161: [D loss: 0.596334, acc: 0.554688]  [G loss: 1.022740, acc: 0.187500]\n",
      "162: [D loss: 0.497889, acc: 0.742188]  [G loss: 1.202204, acc: 0.093750]\n",
      "163: [D loss: 0.460630, acc: 0.757812]  [G loss: 1.306307, acc: 0.109375]\n",
      "164: [D loss: 0.520866, acc: 0.726562]  [G loss: 1.102399, acc: 0.218750]\n",
      "165: [D loss: 0.622527, acc: 0.640625]  [G loss: 1.378250, acc: 0.109375]\n",
      "166: [D loss: 0.641576, acc: 0.664062]  [G loss: 0.340423, acc: 0.953125]\n",
      "167: [D loss: 0.634418, acc: 0.570312]  [G loss: 1.163700, acc: 0.140625]\n",
      "168: [D loss: 0.511568, acc: 0.757812]  [G loss: 0.512867, acc: 0.828125]\n",
      "169: [D loss: 0.507107, acc: 0.742188]  [G loss: 1.180357, acc: 0.109375]\n",
      "170: [D loss: 0.444484, acc: 0.812500]  [G loss: 0.546888, acc: 0.765625]\n",
      "171: [D loss: 0.501221, acc: 0.718750]  [G loss: 2.221113, acc: 0.015625]\n",
      "172: [D loss: 0.531434, acc: 0.703125]  [G loss: 0.259970, acc: 1.000000]\n",
      "173: [D loss: 0.633093, acc: 0.570312]  [G loss: 1.356648, acc: 0.203125]\n",
      "174: [D loss: 0.528460, acc: 0.718750]  [G loss: 0.822464, acc: 0.453125]\n",
      "175: [D loss: 0.519092, acc: 0.742188]  [G loss: 1.422597, acc: 0.078125]\n",
      "176: [D loss: 0.510175, acc: 0.718750]  [G loss: 0.637264, acc: 0.640625]\n",
      "177: [D loss: 0.495556, acc: 0.773438]  [G loss: 2.193107, acc: 0.015625]\n",
      "178: [D loss: 0.597707, acc: 0.601562]  [G loss: 0.383757, acc: 0.937500]\n",
      "179: [D loss: 0.786172, acc: 0.507812]  [G loss: 1.396991, acc: 0.109375]\n",
      "180: [D loss: 0.595149, acc: 0.671875]  [G loss: 0.740046, acc: 0.515625]\n",
      "181: [D loss: 0.582705, acc: 0.710938]  [G loss: 1.104494, acc: 0.140625]\n",
      "182: [D loss: 0.556084, acc: 0.718750]  [G loss: 1.243123, acc: 0.171875]\n",
      "183: [D loss: 0.568662, acc: 0.750000]  [G loss: 0.844738, acc: 0.437500]\n",
      "184: [D loss: 0.513342, acc: 0.781250]  [G loss: 2.084627, acc: 0.000000]\n",
      "185: [D loss: 0.723549, acc: 0.578125]  [G loss: 0.232026, acc: 1.000000]\n",
      "186: [D loss: 0.850702, acc: 0.507812]  [G loss: 1.003771, acc: 0.203125]\n",
      "187: [D loss: 0.528452, acc: 0.796875]  [G loss: 0.822156, acc: 0.343750]\n",
      "188: [D loss: 0.576283, acc: 0.742188]  [G loss: 0.726976, acc: 0.546875]\n",
      "189: [D loss: 0.559399, acc: 0.695312]  [G loss: 0.897368, acc: 0.250000]\n",
      "190: [D loss: 0.541090, acc: 0.726562]  [G loss: 0.530920, acc: 0.812500]\n",
      "191: [D loss: 0.505544, acc: 0.781250]  [G loss: 1.590382, acc: 0.000000]\n",
      "192: [D loss: 0.619828, acc: 0.664062]  [G loss: 0.273528, acc: 0.984375]\n",
      "193: [D loss: 0.649967, acc: 0.554688]  [G loss: 1.278021, acc: 0.046875]\n",
      "194: [D loss: 0.471535, acc: 0.796875]  [G loss: 1.058122, acc: 0.125000]\n",
      "195: [D loss: 0.414020, acc: 0.906250]  [G loss: 1.815643, acc: 0.015625]\n",
      "196: [D loss: 0.460419, acc: 0.781250]  [G loss: 0.516974, acc: 0.750000]\n",
      "197: [D loss: 0.648409, acc: 0.632812]  [G loss: 2.355737, acc: 0.000000]\n",
      "198: [D loss: 0.601983, acc: 0.671875]  [G loss: 0.459142, acc: 0.937500]\n",
      "199: [D loss: 0.636476, acc: 0.585938]  [G loss: 1.075112, acc: 0.093750]\n",
      "200: [D loss: 0.483447, acc: 0.781250]  [G loss: 0.843592, acc: 0.328125]\n",
      "201: [D loss: 0.531068, acc: 0.734375]  [G loss: 1.361530, acc: 0.140625]\n",
      "202: [D loss: 0.492085, acc: 0.742188]  [G loss: 0.621101, acc: 0.640625]\n",
      "203: [D loss: 0.461413, acc: 0.734375]  [G loss: 2.641873, acc: 0.000000]\n",
      "204: [D loss: 0.639672, acc: 0.625000]  [G loss: 0.328359, acc: 1.000000]\n",
      "205: [D loss: 0.666592, acc: 0.531250]  [G loss: 1.099352, acc: 0.109375]\n",
      "206: [D loss: 0.443378, acc: 0.921875]  [G loss: 1.206749, acc: 0.203125]\n",
      "207: [D loss: 0.460967, acc: 0.804688]  [G loss: 1.419899, acc: 0.203125]\n",
      "208: [D loss: 0.596535, acc: 0.679688]  [G loss: 0.989073, acc: 0.312500]\n",
      "209: [D loss: 0.548471, acc: 0.718750]  [G loss: 1.058934, acc: 0.359375]\n",
      "210: [D loss: 0.555476, acc: 0.703125]  [G loss: 2.418537, acc: 0.015625]\n",
      "211: [D loss: 0.612526, acc: 0.671875]  [G loss: 0.217371, acc: 1.000000]\n",
      "212: [D loss: 0.824076, acc: 0.500000]  [G loss: 1.686688, acc: 0.109375]\n",
      "213: [D loss: 0.509597, acc: 0.781250]  [G loss: 0.879359, acc: 0.359375]\n",
      "214: [D loss: 0.560041, acc: 0.726562]  [G loss: 1.065044, acc: 0.187500]\n",
      "215: [D loss: 0.567365, acc: 0.687500]  [G loss: 0.992341, acc: 0.390625]\n",
      "216: [D loss: 0.497159, acc: 0.687500]  [G loss: 1.266929, acc: 0.203125]\n",
      "217: [D loss: 0.481867, acc: 0.812500]  [G loss: 0.932246, acc: 0.390625]\n",
      "218: [D loss: 0.414762, acc: 0.781250]  [G loss: 1.415630, acc: 0.281250]\n",
      "219: [D loss: 0.446104, acc: 0.789062]  [G loss: 0.559237, acc: 0.671875]\n",
      "220: [D loss: 0.549251, acc: 0.695312]  [G loss: 3.651648, acc: 0.000000]\n",
      "221: [D loss: 0.901764, acc: 0.625000]  [G loss: 0.276934, acc: 1.000000]\n",
      "222: [D loss: 0.885381, acc: 0.523438]  [G loss: 0.959252, acc: 0.312500]\n",
      "223: [D loss: 0.528070, acc: 0.765625]  [G loss: 0.779397, acc: 0.562500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224: [D loss: 0.508293, acc: 0.773438]  [G loss: 0.904369, acc: 0.359375]\n",
      "225: [D loss: 0.486755, acc: 0.796875]  [G loss: 1.109941, acc: 0.265625]\n",
      "226: [D loss: 0.567208, acc: 0.664062]  [G loss: 0.631458, acc: 0.640625]\n",
      "227: [D loss: 0.496759, acc: 0.773438]  [G loss: 0.660797, acc: 0.625000]\n",
      "228: [D loss: 0.492194, acc: 0.765625]  [G loss: 0.359497, acc: 0.937500]\n",
      "229: [D loss: 0.450482, acc: 0.828125]  [G loss: 0.626853, acc: 0.640625]\n",
      "230: [D loss: 0.516892, acc: 0.734375]  [G loss: 1.242214, acc: 0.312500]\n",
      "231: [D loss: 0.448275, acc: 0.789062]  [G loss: 0.903965, acc: 0.421875]\n",
      "232: [D loss: 0.445573, acc: 0.796875]  [G loss: 1.647591, acc: 0.078125]\n",
      "233: [D loss: 0.422990, acc: 0.828125]  [G loss: 0.327416, acc: 0.906250]\n",
      "234: [D loss: 0.578530, acc: 0.679688]  [G loss: 2.734689, acc: 0.015625]\n",
      "235: [D loss: 0.739414, acc: 0.648438]  [G loss: 0.214846, acc: 0.984375]\n",
      "236: [D loss: 0.691368, acc: 0.578125]  [G loss: 1.353676, acc: 0.109375]\n",
      "237: [D loss: 0.451946, acc: 0.828125]  [G loss: 1.370019, acc: 0.109375]\n",
      "238: [D loss: 0.470238, acc: 0.820312]  [G loss: 1.064099, acc: 0.187500]\n",
      "239: [D loss: 0.444816, acc: 0.773438]  [G loss: 1.359072, acc: 0.031250]\n",
      "240: [D loss: 0.466935, acc: 0.757812]  [G loss: 0.902756, acc: 0.312500]\n",
      "241: [D loss: 0.464338, acc: 0.789062]  [G loss: 1.182879, acc: 0.203125]\n",
      "242: [D loss: 0.461464, acc: 0.804688]  [G loss: 0.900451, acc: 0.406250]\n",
      "243: [D loss: 0.467620, acc: 0.742188]  [G loss: 2.023064, acc: 0.015625]\n",
      "244: [D loss: 0.572575, acc: 0.695312]  [G loss: 0.332837, acc: 0.937500]\n",
      "245: [D loss: 0.633385, acc: 0.617188]  [G loss: 3.108990, acc: 0.000000]\n",
      "246: [D loss: 0.669380, acc: 0.687500]  [G loss: 0.759305, acc: 0.593750]\n",
      "247: [D loss: 0.551820, acc: 0.609375]  [G loss: 1.155951, acc: 0.031250]\n",
      "248: [D loss: 0.421257, acc: 0.820312]  [G loss: 1.164976, acc: 0.046875]\n",
      "249: [D loss: 0.479383, acc: 0.773438]  [G loss: 0.766616, acc: 0.468750]\n",
      "250: [D loss: 0.428937, acc: 0.796875]  [G loss: 1.252678, acc: 0.156250]\n",
      "251: [D loss: 0.458462, acc: 0.781250]  [G loss: 0.881493, acc: 0.343750]\n",
      "252: [D loss: 0.522457, acc: 0.687500]  [G loss: 1.528514, acc: 0.031250]\n",
      "253: [D loss: 0.559477, acc: 0.734375]  [G loss: 0.736750, acc: 0.515625]\n",
      "254: [D loss: 0.530554, acc: 0.671875]  [G loss: 2.027135, acc: 0.000000]\n",
      "255: [D loss: 0.524873, acc: 0.750000]  [G loss: 0.675234, acc: 0.609375]\n",
      "256: [D loss: 0.560253, acc: 0.656250]  [G loss: 2.206326, acc: 0.000000]\n",
      "257: [D loss: 0.464505, acc: 0.804688]  [G loss: 0.765358, acc: 0.484375]\n",
      "258: [D loss: 0.475929, acc: 0.679688]  [G loss: 2.180481, acc: 0.000000]\n",
      "259: [D loss: 0.428730, acc: 0.781250]  [G loss: 0.886668, acc: 0.234375]\n",
      "260: [D loss: 0.461266, acc: 0.773438]  [G loss: 1.629908, acc: 0.046875]\n",
      "261: [D loss: 0.461327, acc: 0.781250]  [G loss: 1.128610, acc: 0.109375]\n",
      "262: [D loss: 0.664669, acc: 0.625000]  [G loss: 1.156485, acc: 0.171875]\n",
      "263: [D loss: 0.532624, acc: 0.710938]  [G loss: 0.571104, acc: 0.687500]\n",
      "264: [D loss: 0.500091, acc: 0.703125]  [G loss: 0.714741, acc: 0.484375]\n",
      "265: [D loss: 0.486116, acc: 0.757812]  [G loss: 0.958576, acc: 0.437500]\n",
      "266: [D loss: 0.478985, acc: 0.781250]  [G loss: 1.057224, acc: 0.312500]\n",
      "267: [D loss: 0.479982, acc: 0.789062]  [G loss: 1.342640, acc: 0.265625]\n",
      "268: [D loss: 0.514944, acc: 0.773438]  [G loss: 0.847879, acc: 0.484375]\n",
      "269: [D loss: 0.490633, acc: 0.726562]  [G loss: 3.564068, acc: 0.000000]\n",
      "270: [D loss: 0.732824, acc: 0.640625]  [G loss: 0.253235, acc: 1.000000]\n",
      "271: [D loss: 0.869923, acc: 0.492188]  [G loss: 1.768431, acc: 0.000000]\n",
      "272: [D loss: 0.436761, acc: 0.835938]  [G loss: 1.363107, acc: 0.000000]\n",
      "273: [D loss: 0.400797, acc: 0.835938]  [G loss: 1.099609, acc: 0.078125]\n",
      "274: [D loss: 0.392437, acc: 0.781250]  [G loss: 0.939997, acc: 0.156250]\n",
      "275: [D loss: 0.496770, acc: 0.703125]  [G loss: 0.828550, acc: 0.390625]\n",
      "276: [D loss: 0.390510, acc: 0.851562]  [G loss: 0.871510, acc: 0.375000]\n",
      "277: [D loss: 0.396131, acc: 0.835938]  [G loss: 1.057833, acc: 0.343750]\n",
      "278: [D loss: 0.398666, acc: 0.835938]  [G loss: 1.780227, acc: 0.046875]\n",
      "279: [D loss: 0.406136, acc: 0.789062]  [G loss: 1.034160, acc: 0.312500]\n",
      "280: [D loss: 0.421123, acc: 0.812500]  [G loss: 1.707179, acc: 0.062500]\n",
      "281: [D loss: 0.312206, acc: 0.875000]  [G loss: 0.532917, acc: 0.796875]\n",
      "282: [D loss: 0.852281, acc: 0.617188]  [G loss: 2.698227, acc: 0.000000]\n",
      "283: [D loss: 1.062014, acc: 0.531250]  [G loss: 0.415886, acc: 0.937500]\n",
      "284: [D loss: 0.573545, acc: 0.648438]  [G loss: 1.199959, acc: 0.046875]\n",
      "285: [D loss: 0.444908, acc: 0.781250]  [G loss: 1.010556, acc: 0.203125]\n",
      "286: [D loss: 0.495843, acc: 0.710938]  [G loss: 1.199863, acc: 0.000000]\n",
      "287: [D loss: 0.392292, acc: 0.898438]  [G loss: 1.031977, acc: 0.171875]\n",
      "288: [D loss: 0.397738, acc: 0.812500]  [G loss: 1.020715, acc: 0.140625]\n",
      "289: [D loss: 0.373630, acc: 0.835938]  [G loss: 1.040225, acc: 0.281250]\n",
      "290: [D loss: 0.446652, acc: 0.765625]  [G loss: 1.367447, acc: 0.046875]\n",
      "291: [D loss: 0.431232, acc: 0.828125]  [G loss: 1.358483, acc: 0.187500]\n",
      "292: [D loss: 0.488607, acc: 0.703125]  [G loss: 1.794712, acc: 0.015625]\n",
      "293: [D loss: 0.445509, acc: 0.789062]  [G loss: 1.052554, acc: 0.265625]\n",
      "294: [D loss: 0.420792, acc: 0.796875]  [G loss: 2.603652, acc: 0.000000]\n",
      "295: [D loss: 0.529244, acc: 0.695312]  [G loss: 0.487161, acc: 0.718750]\n",
      "296: [D loss: 0.661642, acc: 0.625000]  [G loss: 2.659930, acc: 0.000000]\n",
      "297: [D loss: 0.690381, acc: 0.640625]  [G loss: 0.938662, acc: 0.312500]\n",
      "298: [D loss: 0.506290, acc: 0.687500]  [G loss: 1.181364, acc: 0.031250]\n",
      "299: [D loss: 0.570186, acc: 0.640625]  [G loss: 0.773079, acc: 0.468750]\n",
      "300: [D loss: 0.506370, acc: 0.703125]  [G loss: 0.910070, acc: 0.343750]\n",
      "301: [D loss: 0.562929, acc: 0.703125]  [G loss: 0.755083, acc: 0.468750]\n",
      "302: [D loss: 0.487962, acc: 0.726562]  [G loss: 0.946717, acc: 0.187500]\n",
      "303: [D loss: 0.472909, acc: 0.757812]  [G loss: 0.733866, acc: 0.468750]\n",
      "304: [D loss: 0.482488, acc: 0.726562]  [G loss: 1.749283, acc: 0.015625]\n",
      "305: [D loss: 0.563257, acc: 0.703125]  [G loss: 0.390130, acc: 0.953125]\n",
      "306: [D loss: 0.811161, acc: 0.570312]  [G loss: 1.821895, acc: 0.000000]\n",
      "307: [D loss: 0.695441, acc: 0.546875]  [G loss: 0.724090, acc: 0.500000]\n",
      "308: [D loss: 0.627131, acc: 0.648438]  [G loss: 1.200163, acc: 0.031250]\n",
      "309: [D loss: 0.521166, acc: 0.750000]  [G loss: 1.071519, acc: 0.109375]\n",
      "310: [D loss: 0.492813, acc: 0.765625]  [G loss: 0.901201, acc: 0.234375]\n",
      "311: [D loss: 0.514143, acc: 0.742188]  [G loss: 1.169161, acc: 0.156250]\n",
      "312: [D loss: 0.502151, acc: 0.726562]  [G loss: 0.648789, acc: 0.531250]\n",
      "313: [D loss: 0.628797, acc: 0.718750]  [G loss: 0.940454, acc: 0.234375]\n",
      "314: [D loss: 0.507860, acc: 0.710938]  [G loss: 0.550035, acc: 0.734375]\n",
      "315: [D loss: 0.520658, acc: 0.703125]  [G loss: 0.946531, acc: 0.218750]\n",
      "316: [D loss: 0.447538, acc: 0.812500]  [G loss: 0.673781, acc: 0.500000]\n",
      "317: [D loss: 0.480908, acc: 0.687500]  [G loss: 1.327656, acc: 0.062500]\n",
      "318: [D loss: 0.492674, acc: 0.742188]  [G loss: 0.587457, acc: 0.625000]\n",
      "319: [D loss: 0.660351, acc: 0.601562]  [G loss: 1.856919, acc: 0.000000]\n",
      "320: [D loss: 0.600712, acc: 0.585938]  [G loss: 0.874260, acc: 0.281250]\n",
      "321: [D loss: 0.614135, acc: 0.734375]  [G loss: 1.286353, acc: 0.015625]\n",
      "322: [D loss: 0.472048, acc: 0.796875]  [G loss: 0.885027, acc: 0.109375]\n",
      "323: [D loss: 0.485571, acc: 0.671875]  [G loss: 1.224836, acc: 0.031250]\n",
      "324: [D loss: 0.481045, acc: 0.765625]  [G loss: 0.754379, acc: 0.437500]\n",
      "325: [D loss: 0.453393, acc: 0.773438]  [G loss: 1.453439, acc: 0.000000]\n",
      "326: [D loss: 0.493250, acc: 0.718750]  [G loss: 0.419728, acc: 0.890625]\n",
      "327: [D loss: 0.563216, acc: 0.679688]  [G loss: 1.161634, acc: 0.031250]\n",
      "328: [D loss: 0.480078, acc: 0.789062]  [G loss: 0.698345, acc: 0.562500]\n",
      "329: [D loss: 0.430746, acc: 0.796875]  [G loss: 1.145563, acc: 0.093750]\n",
      "330: [D loss: 0.442896, acc: 0.828125]  [G loss: 1.058801, acc: 0.171875]\n",
      "331: [D loss: 0.447748, acc: 0.773438]  [G loss: 1.248778, acc: 0.078125]\n",
      "332: [D loss: 0.449287, acc: 0.765625]  [G loss: 1.419433, acc: 0.078125]\n",
      "333: [D loss: 0.454356, acc: 0.773438]  [G loss: 1.404404, acc: 0.078125]\n",
      "334: [D loss: 0.367923, acc: 0.859375]  [G loss: 0.793651, acc: 0.453125]\n",
      "335: [D loss: 0.471992, acc: 0.742188]  [G loss: 2.607101, acc: 0.000000]\n",
      "336: [D loss: 0.533870, acc: 0.718750]  [G loss: 0.295286, acc: 0.906250]\n",
      "337: [D loss: 1.083959, acc: 0.484375]  [G loss: 1.462887, acc: 0.062500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338: [D loss: 0.491110, acc: 0.765625]  [G loss: 1.143422, acc: 0.109375]\n",
      "339: [D loss: 0.604163, acc: 0.648438]  [G loss: 0.931546, acc: 0.203125]\n",
      "340: [D loss: 0.460077, acc: 0.781250]  [G loss: 0.980082, acc: 0.109375]\n",
      "341: [D loss: 0.603076, acc: 0.640625]  [G loss: 1.233096, acc: 0.031250]\n",
      "342: [D loss: 0.593001, acc: 0.617188]  [G loss: 0.632143, acc: 0.656250]\n",
      "343: [D loss: 0.520263, acc: 0.648438]  [G loss: 0.856292, acc: 0.406250]\n",
      "344: [D loss: 0.602455, acc: 0.687500]  [G loss: 0.569793, acc: 0.734375]\n",
      "345: [D loss: 0.558145, acc: 0.656250]  [G loss: 1.285791, acc: 0.062500]\n",
      "346: [D loss: 0.510728, acc: 0.742188]  [G loss: 0.595278, acc: 0.640625]\n",
      "347: [D loss: 0.481748, acc: 0.695312]  [G loss: 1.429889, acc: 0.093750]\n",
      "348: [D loss: 0.512731, acc: 0.781250]  [G loss: 0.738882, acc: 0.406250]\n",
      "349: [D loss: 0.512278, acc: 0.710938]  [G loss: 0.838398, acc: 0.390625]\n",
      "350: [D loss: 0.501695, acc: 0.726562]  [G loss: 0.966840, acc: 0.171875]\n",
      "351: [D loss: 0.525868, acc: 0.718750]  [G loss: 0.925247, acc: 0.171875]\n",
      "352: [D loss: 0.430045, acc: 0.812500]  [G loss: 1.165696, acc: 0.125000]\n",
      "353: [D loss: 0.415717, acc: 0.859375]  [G loss: 1.195915, acc: 0.078125]\n",
      "354: [D loss: 0.448263, acc: 0.765625]  [G loss: 2.033386, acc: 0.000000]\n",
      "355: [D loss: 0.467451, acc: 0.789062]  [G loss: 0.454711, acc: 0.796875]\n",
      "356: [D loss: 0.782904, acc: 0.554688]  [G loss: 2.502867, acc: 0.000000]\n",
      "357: [D loss: 0.752211, acc: 0.601562]  [G loss: 0.682849, acc: 0.500000]\n",
      "358: [D loss: 0.498858, acc: 0.718750]  [G loss: 1.017843, acc: 0.109375]\n",
      "359: [D loss: 0.601376, acc: 0.710938]  [G loss: 0.908596, acc: 0.156250]\n",
      "360: [D loss: 0.504322, acc: 0.734375]  [G loss: 0.703262, acc: 0.546875]\n",
      "361: [D loss: 0.534291, acc: 0.687500]  [G loss: 0.889208, acc: 0.328125]\n",
      "362: [D loss: 0.599165, acc: 0.640625]  [G loss: 0.623932, acc: 0.578125]\n",
      "363: [D loss: 0.412488, acc: 0.843750]  [G loss: 0.585105, acc: 0.671875]\n",
      "364: [D loss: 0.427823, acc: 0.812500]  [G loss: 0.737847, acc: 0.500000]\n",
      "365: [D loss: 0.423628, acc: 0.820312]  [G loss: 0.589196, acc: 0.625000]\n",
      "366: [D loss: 0.445123, acc: 0.812500]  [G loss: 1.640637, acc: 0.031250]\n",
      "367: [D loss: 0.563597, acc: 0.734375]  [G loss: 0.760377, acc: 0.437500]\n",
      "368: [D loss: 0.381050, acc: 0.867188]  [G loss: 1.124730, acc: 0.046875]\n",
      "369: [D loss: 0.320828, acc: 0.882812]  [G loss: 1.233225, acc: 0.062500]\n",
      "370: [D loss: 0.571160, acc: 0.718750]  [G loss: 1.337805, acc: 0.000000]\n",
      "371: [D loss: 0.393842, acc: 0.828125]  [G loss: 1.129251, acc: 0.156250]\n",
      "372: [D loss: 0.470674, acc: 0.781250]  [G loss: 1.764208, acc: 0.000000]\n",
      "373: [D loss: 0.406507, acc: 0.789062]  [G loss: 0.749296, acc: 0.375000]\n",
      "374: [D loss: 1.295033, acc: 0.585938]  [G loss: 1.855221, acc: 0.015625]\n",
      "375: [D loss: 0.631767, acc: 0.648438]  [G loss: 0.608911, acc: 0.578125]\n",
      "376: [D loss: 0.528497, acc: 0.695312]  [G loss: 1.052721, acc: 0.125000]\n",
      "377: [D loss: 0.447016, acc: 0.796875]  [G loss: 0.949394, acc: 0.281250]\n",
      "378: [D loss: 0.464266, acc: 0.757812]  [G loss: 1.297873, acc: 0.031250]\n",
      "379: [D loss: 0.526702, acc: 0.687500]  [G loss: 1.019480, acc: 0.125000]\n",
      "380: [D loss: 0.431773, acc: 0.750000]  [G loss: 0.887916, acc: 0.265625]\n",
      "381: [D loss: 0.501959, acc: 0.734375]  [G loss: 0.953716, acc: 0.171875]\n",
      "382: [D loss: 0.463563, acc: 0.820312]  [G loss: 1.024892, acc: 0.250000]\n",
      "383: [D loss: 0.435111, acc: 0.804688]  [G loss: 1.019079, acc: 0.343750]\n",
      "384: [D loss: 0.368068, acc: 0.812500]  [G loss: 2.031394, acc: 0.000000]\n",
      "385: [D loss: 0.590147, acc: 0.710938]  [G loss: 0.339459, acc: 0.953125]\n",
      "386: [D loss: 0.763991, acc: 0.539062]  [G loss: 1.857142, acc: 0.000000]\n",
      "387: [D loss: 0.527100, acc: 0.703125]  [G loss: 0.730605, acc: 0.546875]\n",
      "388: [D loss: 0.498697, acc: 0.726562]  [G loss: 1.254489, acc: 0.031250]\n",
      "389: [D loss: 0.487092, acc: 0.773438]  [G loss: 1.053669, acc: 0.125000]\n",
      "390: [D loss: 0.445878, acc: 0.750000]  [G loss: 1.447257, acc: 0.109375]\n",
      "391: [D loss: 0.459564, acc: 0.789062]  [G loss: 0.789717, acc: 0.390625]\n",
      "392: [D loss: 0.539689, acc: 0.695312]  [G loss: 1.392748, acc: 0.093750]\n",
      "393: [D loss: 0.467295, acc: 0.796875]  [G loss: 0.702503, acc: 0.515625]\n",
      "394: [D loss: 0.435834, acc: 0.859375]  [G loss: 1.256967, acc: 0.125000]\n",
      "395: [D loss: 0.451642, acc: 0.820312]  [G loss: 0.784127, acc: 0.406250]\n",
      "396: [D loss: 0.481328, acc: 0.765625]  [G loss: 1.300395, acc: 0.171875]\n",
      "397: [D loss: 0.387720, acc: 0.828125]  [G loss: 0.499448, acc: 0.781250]\n",
      "398: [D loss: 0.508075, acc: 0.664062]  [G loss: 1.566245, acc: 0.015625]\n",
      "399: [D loss: 0.420621, acc: 0.773438]  [G loss: 0.577672, acc: 0.625000]\n",
      "400: [D loss: 0.369875, acc: 0.773438]  [G loss: 1.472817, acc: 0.015625]\n",
      "401: [D loss: 0.395114, acc: 0.859375]  [G loss: 0.695718, acc: 0.500000]\n",
      "402: [D loss: 0.355227, acc: 0.898438]  [G loss: 1.807071, acc: 0.015625]\n",
      "403: [D loss: 0.435875, acc: 0.820312]  [G loss: 0.612171, acc: 0.718750]\n",
      "404: [D loss: 0.574904, acc: 0.664062]  [G loss: 2.468590, acc: 0.000000]\n",
      "405: [D loss: 0.567546, acc: 0.687500]  [G loss: 0.746987, acc: 0.437500]\n",
      "406: [D loss: 0.422952, acc: 0.789062]  [G loss: 1.765045, acc: 0.015625]\n",
      "407: [D loss: 0.372301, acc: 0.773438]  [G loss: 1.238743, acc: 0.078125]\n",
      "408: [D loss: 0.384339, acc: 0.820312]  [G loss: 0.948192, acc: 0.265625]\n",
      "409: [D loss: 0.374020, acc: 0.820312]  [G loss: 0.988257, acc: 0.328125]\n",
      "410: [D loss: 0.338780, acc: 0.812500]  [G loss: 1.309325, acc: 0.171875]\n",
      "411: [D loss: 0.367688, acc: 0.828125]  [G loss: 0.791404, acc: 0.468750]\n",
      "412: [D loss: 0.569223, acc: 0.695312]  [G loss: 1.584317, acc: 0.125000]\n",
      "413: [D loss: 0.392551, acc: 0.804688]  [G loss: 0.503549, acc: 0.703125]\n",
      "414: [D loss: 0.564070, acc: 0.656250]  [G loss: 3.002326, acc: 0.000000]\n",
      "415: [D loss: 0.606898, acc: 0.710938]  [G loss: 0.804097, acc: 0.390625]\n",
      "416: [D loss: 0.459522, acc: 0.796875]  [G loss: 1.374066, acc: 0.078125]\n",
      "417: [D loss: 0.456804, acc: 0.796875]  [G loss: 0.715511, acc: 0.500000]\n",
      "418: [D loss: 0.372389, acc: 0.789062]  [G loss: 1.077976, acc: 0.203125]\n",
      "419: [D loss: 0.421002, acc: 0.781250]  [G loss: 1.289156, acc: 0.078125]\n",
      "420: [D loss: 0.390456, acc: 0.820312]  [G loss: 0.795275, acc: 0.437500]\n",
      "421: [D loss: 0.446001, acc: 0.804688]  [G loss: 1.287343, acc: 0.078125]\n",
      "422: [D loss: 0.352471, acc: 0.859375]  [G loss: 1.255342, acc: 0.078125]\n",
      "423: [D loss: 0.430147, acc: 0.789062]  [G loss: 1.418541, acc: 0.062500]\n",
      "424: [D loss: 0.423652, acc: 0.796875]  [G loss: 1.180860, acc: 0.140625]\n",
      "425: [D loss: 0.574555, acc: 0.765625]  [G loss: 3.035494, acc: 0.000000]\n",
      "426: [D loss: 0.544733, acc: 0.789062]  [G loss: 0.617672, acc: 0.609375]\n",
      "427: [D loss: 0.435515, acc: 0.750000]  [G loss: 1.156149, acc: 0.109375]\n",
      "428: [D loss: 0.357282, acc: 0.843750]  [G loss: 0.923465, acc: 0.234375]\n",
      "429: [D loss: 0.304146, acc: 0.890625]  [G loss: 0.861397, acc: 0.421875]\n",
      "430: [D loss: 0.319456, acc: 0.859375]  [G loss: 1.973592, acc: 0.046875]\n",
      "431: [D loss: 0.369472, acc: 0.851562]  [G loss: 0.635708, acc: 0.593750]\n",
      "432: [D loss: 0.368092, acc: 0.789062]  [G loss: 1.664123, acc: 0.031250]\n",
      "433: [D loss: 0.533040, acc: 0.742188]  [G loss: 0.697633, acc: 0.468750]\n",
      "434: [D loss: 0.557022, acc: 0.648438]  [G loss: 2.666318, acc: 0.000000]\n",
      "435: [D loss: 0.544099, acc: 0.710938]  [G loss: 0.735962, acc: 0.515625]\n",
      "436: [D loss: 0.458765, acc: 0.773438]  [G loss: 1.863126, acc: 0.046875]\n",
      "437: [D loss: 0.460377, acc: 0.757812]  [G loss: 1.127040, acc: 0.078125]\n",
      "438: [D loss: 0.374092, acc: 0.875000]  [G loss: 1.189959, acc: 0.078125]\n",
      "439: [D loss: 0.440023, acc: 0.859375]  [G loss: 0.768252, acc: 0.484375]\n",
      "440: [D loss: 0.422923, acc: 0.750000]  [G loss: 1.538577, acc: 0.093750]\n",
      "441: [D loss: 0.534869, acc: 0.703125]  [G loss: 0.661067, acc: 0.531250]\n",
      "442: [D loss: 0.423788, acc: 0.734375]  [G loss: 1.542468, acc: 0.125000]\n",
      "443: [D loss: 0.491828, acc: 0.718750]  [G loss: 0.368709, acc: 0.875000]\n",
      "444: [D loss: 0.545945, acc: 0.664062]  [G loss: 1.424740, acc: 0.078125]\n",
      "445: [D loss: 0.455422, acc: 0.757812]  [G loss: 1.084632, acc: 0.078125]\n",
      "446: [D loss: 0.407978, acc: 0.789062]  [G loss: 1.288089, acc: 0.093750]\n",
      "447: [D loss: 0.516828, acc: 0.703125]  [G loss: 0.994356, acc: 0.156250]\n",
      "448: [D loss: 0.350173, acc: 0.867188]  [G loss: 1.454058, acc: 0.046875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449: [D loss: 0.344926, acc: 0.867188]  [G loss: 0.916441, acc: 0.359375]\n",
      "450: [D loss: 0.500903, acc: 0.765625]  [G loss: 2.530519, acc: 0.000000]\n",
      "451: [D loss: 0.751603, acc: 0.656250]  [G loss: 0.722240, acc: 0.515625]\n",
      "452: [D loss: 0.740159, acc: 0.648438]  [G loss: 1.287654, acc: 0.031250]\n",
      "453: [D loss: 0.510582, acc: 0.695312]  [G loss: 0.953362, acc: 0.125000]\n",
      "454: [D loss: 0.482544, acc: 0.765625]  [G loss: 0.907216, acc: 0.218750]\n",
      "455: [D loss: 0.388894, acc: 0.843750]  [G loss: 0.737583, acc: 0.406250]\n",
      "456: [D loss: 0.336831, acc: 0.851562]  [G loss: 0.844965, acc: 0.390625]\n",
      "457: [D loss: 0.404942, acc: 0.796875]  [G loss: 1.424140, acc: 0.078125]\n",
      "458: [D loss: 0.455474, acc: 0.789062]  [G loss: 0.617892, acc: 0.625000]\n",
      "459: [D loss: 0.425079, acc: 0.773438]  [G loss: 1.918388, acc: 0.000000]\n",
      "460: [D loss: 0.316637, acc: 0.921875]  [G loss: 1.182790, acc: 0.125000]\n",
      "461: [D loss: 0.370318, acc: 0.812500]  [G loss: 1.310867, acc: 0.093750]\n",
      "462: [D loss: 0.455661, acc: 0.750000]  [G loss: 0.953074, acc: 0.390625]\n",
      "463: [D loss: 0.458436, acc: 0.726562]  [G loss: 1.374002, acc: 0.093750]\n",
      "464: [D loss: 0.482792, acc: 0.750000]  [G loss: 1.086012, acc: 0.156250]\n",
      "465: [D loss: 0.441802, acc: 0.804688]  [G loss: 1.292916, acc: 0.125000]\n",
      "466: [D loss: 0.381767, acc: 0.812500]  [G loss: 0.687256, acc: 0.546875]\n",
      "467: [D loss: 0.521733, acc: 0.687500]  [G loss: 1.875570, acc: 0.000000]\n",
      "468: [D loss: 0.527928, acc: 0.718750]  [G loss: 0.555597, acc: 0.656250]\n",
      "469: [D loss: 0.449535, acc: 0.710938]  [G loss: 1.227397, acc: 0.031250]\n",
      "470: [D loss: 0.379328, acc: 0.828125]  [G loss: 0.939684, acc: 0.218750]\n",
      "471: [D loss: 0.442861, acc: 0.726562]  [G loss: 1.665483, acc: 0.000000]\n",
      "472: [D loss: 0.484541, acc: 0.750000]  [G loss: 0.703083, acc: 0.500000]\n",
      "473: [D loss: 0.495032, acc: 0.750000]  [G loss: 1.435620, acc: 0.125000]\n",
      "474: [D loss: 0.437135, acc: 0.781250]  [G loss: 1.070622, acc: 0.125000]\n",
      "475: [D loss: 0.346263, acc: 0.859375]  [G loss: 0.872763, acc: 0.281250]\n",
      "476: [D loss: 0.432332, acc: 0.710938]  [G loss: 1.260476, acc: 0.109375]\n",
      "477: [D loss: 0.438664, acc: 0.781250]  [G loss: 0.579991, acc: 0.656250]\n",
      "478: [D loss: 0.395760, acc: 0.765625]  [G loss: 0.858238, acc: 0.406250]\n",
      "479: [D loss: 0.428378, acc: 0.757812]  [G loss: 0.414885, acc: 0.859375]\n",
      "480: [D loss: 0.374642, acc: 0.820312]  [G loss: 1.941194, acc: 0.031250]\n",
      "481: [D loss: 0.484854, acc: 0.773438]  [G loss: 0.296966, acc: 0.921875]\n",
      "482: [D loss: 0.485439, acc: 0.773438]  [G loss: 1.187723, acc: 0.171875]\n",
      "483: [D loss: 0.364494, acc: 0.828125]  [G loss: 0.520296, acc: 0.609375]\n",
      "484: [D loss: 0.397335, acc: 0.789062]  [G loss: 1.308074, acc: 0.015625]\n",
      "485: [D loss: 0.459150, acc: 0.757812]  [G loss: 0.644172, acc: 0.593750]\n",
      "486: [D loss: 0.422229, acc: 0.757812]  [G loss: 1.434478, acc: 0.203125]\n",
      "487: [D loss: 0.455234, acc: 0.781250]  [G loss: 0.771640, acc: 0.359375]\n",
      "488: [D loss: 0.543850, acc: 0.679688]  [G loss: 1.571809, acc: 0.125000]\n",
      "489: [D loss: 0.425219, acc: 0.781250]  [G loss: 0.575350, acc: 0.656250]\n",
      "490: [D loss: 0.620615, acc: 0.625000]  [G loss: 1.920537, acc: 0.000000]\n",
      "491: [D loss: 0.554329, acc: 0.757812]  [G loss: 0.979201, acc: 0.109375]\n",
      "492: [D loss: 0.598488, acc: 0.765625]  [G loss: 1.410624, acc: 0.000000]\n",
      "493: [D loss: 0.484375, acc: 0.757812]  [G loss: 0.789585, acc: 0.390625]\n",
      "494: [D loss: 0.537612, acc: 0.703125]  [G loss: 1.168959, acc: 0.125000]\n",
      "495: [D loss: 0.460866, acc: 0.734375]  [G loss: 0.809641, acc: 0.343750]\n",
      "496: [D loss: 0.450993, acc: 0.750000]  [G loss: 0.753481, acc: 0.484375]\n",
      "497: [D loss: 0.515492, acc: 0.703125]  [G loss: 0.812140, acc: 0.421875]\n",
      "498: [D loss: 0.437012, acc: 0.789062]  [G loss: 0.996170, acc: 0.296875]\n",
      "499: [D loss: 0.497514, acc: 0.765625]  [G loss: 0.623074, acc: 0.656250]\n",
      "500: [D loss: 0.357902, acc: 0.875000]  [G loss: 0.637902, acc: 0.546875]\n",
      "501: [D loss: 0.378817, acc: 0.851562]  [G loss: 1.431466, acc: 0.203125]\n",
      "502: [D loss: 0.393984, acc: 0.828125]  [G loss: 0.484835, acc: 0.796875]\n",
      "503: [D loss: 0.444908, acc: 0.773438]  [G loss: 2.792571, acc: 0.015625]\n",
      "504: [D loss: 0.730943, acc: 0.632812]  [G loss: 0.583370, acc: 0.671875]\n",
      "505: [D loss: 0.654895, acc: 0.703125]  [G loss: 1.049570, acc: 0.187500]\n",
      "506: [D loss: 0.497352, acc: 0.750000]  [G loss: 0.721389, acc: 0.546875]\n",
      "507: [D loss: 0.556794, acc: 0.695312]  [G loss: 1.064296, acc: 0.140625]\n",
      "508: [D loss: 0.448800, acc: 0.796875]  [G loss: 0.696965, acc: 0.562500]\n",
      "509: [D loss: 0.390729, acc: 0.820312]  [G loss: 0.933435, acc: 0.328125]\n",
      "510: [D loss: 0.321713, acc: 0.898438]  [G loss: 0.933563, acc: 0.265625]\n",
      "511: [D loss: 0.481863, acc: 0.765625]  [G loss: 0.835945, acc: 0.328125]\n",
      "512: [D loss: 0.364784, acc: 0.851562]  [G loss: 1.035672, acc: 0.109375]\n",
      "513: [D loss: 0.372780, acc: 0.796875]  [G loss: 0.425258, acc: 0.765625]\n",
      "514: [D loss: 0.398207, acc: 0.796875]  [G loss: 1.076854, acc: 0.140625]\n",
      "515: [D loss: 0.411573, acc: 0.828125]  [G loss: 0.667344, acc: 0.515625]\n",
      "516: [D loss: 0.528613, acc: 0.695312]  [G loss: 1.644818, acc: 0.031250]\n",
      "517: [D loss: 0.545968, acc: 0.765625]  [G loss: 0.647128, acc: 0.609375]\n",
      "518: [D loss: 0.409950, acc: 0.781250]  [G loss: 1.059135, acc: 0.156250]\n",
      "519: [D loss: 0.379371, acc: 0.828125]  [G loss: 1.221647, acc: 0.093750]\n",
      "520: [D loss: 0.394354, acc: 0.796875]  [G loss: 0.786375, acc: 0.421875]\n",
      "521: [D loss: 0.522079, acc: 0.703125]  [G loss: 1.516613, acc: 0.015625]\n",
      "522: [D loss: 0.477362, acc: 0.742188]  [G loss: 0.532999, acc: 0.718750]\n",
      "523: [D loss: 0.577343, acc: 0.687500]  [G loss: 2.186715, acc: 0.000000]\n",
      "524: [D loss: 0.515654, acc: 0.742188]  [G loss: 0.797972, acc: 0.437500]\n",
      "525: [D loss: 0.427870, acc: 0.796875]  [G loss: 0.993078, acc: 0.156250]\n",
      "526: [D loss: 0.377549, acc: 0.828125]  [G loss: 1.089543, acc: 0.046875]\n",
      "527: [D loss: 0.383509, acc: 0.851562]  [G loss: 0.776944, acc: 0.359375]\n",
      "528: [D loss: 0.381315, acc: 0.843750]  [G loss: 1.145399, acc: 0.234375]\n",
      "529: [D loss: 0.339848, acc: 0.867188]  [G loss: 1.026356, acc: 0.296875]\n",
      "530: [D loss: 0.401156, acc: 0.812500]  [G loss: 0.859305, acc: 0.515625]\n",
      "531: [D loss: 0.491697, acc: 0.742188]  [G loss: 1.907686, acc: 0.171875]\n",
      "532: [D loss: 0.664097, acc: 0.625000]  [G loss: 0.522168, acc: 0.640625]\n",
      "533: [D loss: 0.903353, acc: 0.664062]  [G loss: 1.681562, acc: 0.015625]\n",
      "534: [D loss: 0.705754, acc: 0.617188]  [G loss: 0.348374, acc: 0.859375]\n",
      "535: [D loss: 0.792152, acc: 0.570312]  [G loss: 1.834068, acc: 0.015625]\n",
      "536: [D loss: 0.599929, acc: 0.687500]  [G loss: 0.942765, acc: 0.218750]\n",
      "537: [D loss: 0.494748, acc: 0.750000]  [G loss: 0.739150, acc: 0.437500]\n",
      "538: [D loss: 0.502060, acc: 0.734375]  [G loss: 0.776731, acc: 0.468750]\n",
      "539: [D loss: 0.486610, acc: 0.734375]  [G loss: 0.657061, acc: 0.468750]\n",
      "540: [D loss: 0.497181, acc: 0.750000]  [G loss: 0.649521, acc: 0.531250]\n",
      "541: [D loss: 0.488564, acc: 0.742188]  [G loss: 0.569520, acc: 0.640625]\n",
      "542: [D loss: 0.484938, acc: 0.742188]  [G loss: 0.497159, acc: 0.718750]\n",
      "543: [D loss: 0.521046, acc: 0.812500]  [G loss: 0.906316, acc: 0.437500]\n",
      "544: [D loss: 0.499858, acc: 0.726562]  [G loss: 0.603246, acc: 0.593750]\n",
      "545: [D loss: 0.518561, acc: 0.726562]  [G loss: 0.521591, acc: 0.718750]\n",
      "546: [D loss: 0.506154, acc: 0.734375]  [G loss: 1.060919, acc: 0.312500]\n",
      "547: [D loss: 0.474150, acc: 0.757812]  [G loss: 0.462854, acc: 0.781250]\n",
      "548: [D loss: 0.493372, acc: 0.734375]  [G loss: 1.001003, acc: 0.234375]\n",
      "549: [D loss: 0.453233, acc: 0.765625]  [G loss: 0.744195, acc: 0.515625]\n",
      "550: [D loss: 0.449542, acc: 0.773438]  [G loss: 0.738566, acc: 0.484375]\n",
      "551: [D loss: 0.450166, acc: 0.796875]  [G loss: 1.161266, acc: 0.281250]\n",
      "552: [D loss: 0.449089, acc: 0.765625]  [G loss: 0.517597, acc: 0.734375]\n",
      "553: [D loss: 0.362110, acc: 0.828125]  [G loss: 1.378631, acc: 0.093750]\n",
      "554: [D loss: 0.418781, acc: 0.765625]  [G loss: 0.763222, acc: 0.515625]\n",
      "555: [D loss: 0.428362, acc: 0.773438]  [G loss: 0.936088, acc: 0.359375]\n",
      "556: [D loss: 0.382816, acc: 0.851562]  [G loss: 0.833571, acc: 0.390625]\n",
      "557: [D loss: 0.458602, acc: 0.750000]  [G loss: 0.844914, acc: 0.468750]\n",
      "558: [D loss: 0.487272, acc: 0.781250]  [G loss: 0.499310, acc: 0.687500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559: [D loss: 0.516314, acc: 0.726562]  [G loss: 1.760810, acc: 0.031250]\n",
      "560: [D loss: 0.578035, acc: 0.710938]  [G loss: 0.203525, acc: 0.968750]\n",
      "561: [D loss: 0.620316, acc: 0.648438]  [G loss: 1.780711, acc: 0.015625]\n",
      "562: [D loss: 0.471225, acc: 0.765625]  [G loss: 0.934240, acc: 0.265625]\n",
      "563: [D loss: 0.383100, acc: 0.859375]  [G loss: 0.853323, acc: 0.328125]\n",
      "564: [D loss: 0.442161, acc: 0.781250]  [G loss: 1.067702, acc: 0.171875]\n",
      "565: [D loss: 0.393748, acc: 0.789062]  [G loss: 1.005154, acc: 0.312500]\n",
      "566: [D loss: 0.381804, acc: 0.828125]  [G loss: 1.060560, acc: 0.171875]\n",
      "567: [D loss: 0.376589, acc: 0.859375]  [G loss: 0.668342, acc: 0.562500]\n",
      "568: [D loss: 0.363943, acc: 0.820312]  [G loss: 1.210001, acc: 0.109375]\n",
      "569: [D loss: 0.428414, acc: 0.765625]  [G loss: 0.310762, acc: 0.890625]\n",
      "570: [D loss: 0.642724, acc: 0.656250]  [G loss: 1.506720, acc: 0.140625]\n",
      "571: [D loss: 0.545101, acc: 0.687500]  [G loss: 0.560069, acc: 0.546875]\n",
      "572: [D loss: 0.517357, acc: 0.734375]  [G loss: 0.871791, acc: 0.343750]\n",
      "573: [D loss: 0.338830, acc: 0.851562]  [G loss: 0.550589, acc: 0.703125]\n",
      "574: [D loss: 0.397233, acc: 0.804688]  [G loss: 0.564039, acc: 0.609375]\n",
      "575: [D loss: 0.482117, acc: 0.726562]  [G loss: 0.495428, acc: 0.734375]\n",
      "576: [D loss: 0.549851, acc: 0.718750]  [G loss: 1.037429, acc: 0.187500]\n",
      "577: [D loss: 0.410302, acc: 0.804688]  [G loss: 0.422116, acc: 0.875000]\n",
      "578: [D loss: 0.534043, acc: 0.710938]  [G loss: 2.071482, acc: 0.031250]\n",
      "579: [D loss: 0.376745, acc: 0.820312]  [G loss: 0.813113, acc: 0.406250]\n",
      "580: [D loss: 0.536142, acc: 0.742188]  [G loss: 1.415545, acc: 0.015625]\n",
      "581: [D loss: 0.354752, acc: 0.898438]  [G loss: 1.366899, acc: 0.046875]\n",
      "582: [D loss: 0.371764, acc: 0.828125]  [G loss: 1.679191, acc: 0.015625]\n",
      "583: [D loss: 0.384215, acc: 0.843750]  [G loss: 0.779458, acc: 0.390625]\n",
      "584: [D loss: 0.368600, acc: 0.828125]  [G loss: 2.547225, acc: 0.000000]\n",
      "585: [D loss: 0.402058, acc: 0.781250]  [G loss: 0.760501, acc: 0.375000]\n",
      "586: [D loss: 0.291414, acc: 0.890625]  [G loss: 1.526523, acc: 0.031250]\n",
      "587: [D loss: 0.275458, acc: 0.921875]  [G loss: 0.756246, acc: 0.375000]\n",
      "588: [D loss: 0.328042, acc: 0.851562]  [G loss: 1.459208, acc: 0.125000]\n",
      "589: [D loss: 0.396419, acc: 0.804688]  [G loss: 0.302244, acc: 0.890625]\n",
      "590: [D loss: 0.873132, acc: 0.625000]  [G loss: 2.873795, acc: 0.000000]\n",
      "591: [D loss: 0.811764, acc: 0.617188]  [G loss: 0.888830, acc: 0.328125]\n",
      "592: [D loss: 0.495526, acc: 0.734375]  [G loss: 0.893398, acc: 0.390625]\n",
      "593: [D loss: 0.400245, acc: 0.765625]  [G loss: 0.749271, acc: 0.500000]\n",
      "594: [D loss: 0.376415, acc: 0.859375]  [G loss: 0.543849, acc: 0.609375]\n",
      "595: [D loss: 0.464291, acc: 0.789062]  [G loss: 0.849488, acc: 0.406250]\n",
      "596: [D loss: 0.497651, acc: 0.765625]  [G loss: 0.690238, acc: 0.640625]\n",
      "597: [D loss: 0.417560, acc: 0.835938]  [G loss: 0.756675, acc: 0.453125]\n",
      "598: [D loss: 0.367652, acc: 0.820312]  [G loss: 0.574877, acc: 0.656250]\n",
      "599: [D loss: 0.397208, acc: 0.828125]  [G loss: 0.920799, acc: 0.343750]\n",
      "600: [D loss: 0.414458, acc: 0.820312]  [G loss: 0.211469, acc: 0.953125]\n",
      "601: [D loss: 0.536711, acc: 0.710938]  [G loss: 2.084135, acc: 0.000000]\n",
      "602: [D loss: 0.708444, acc: 0.617188]  [G loss: 0.553763, acc: 0.781250]\n",
      "603: [D loss: 0.520403, acc: 0.726562]  [G loss: 1.315116, acc: 0.000000]\n",
      "604: [D loss: 0.429047, acc: 0.773438]  [G loss: 0.842086, acc: 0.406250]\n",
      "605: [D loss: 0.400322, acc: 0.820312]  [G loss: 1.293600, acc: 0.140625]\n",
      "606: [D loss: 0.437483, acc: 0.796875]  [G loss: 0.927489, acc: 0.328125]\n",
      "607: [D loss: 0.395501, acc: 0.828125]  [G loss: 0.760489, acc: 0.484375]\n",
      "608: [D loss: 0.512271, acc: 0.734375]  [G loss: 1.285492, acc: 0.203125]\n",
      "609: [D loss: 0.496016, acc: 0.773438]  [G loss: 0.414243, acc: 0.718750]\n",
      "610: [D loss: 0.646414, acc: 0.648438]  [G loss: 1.294472, acc: 0.218750]\n",
      "611: [D loss: 0.556088, acc: 0.695312]  [G loss: 0.411485, acc: 0.765625]\n",
      "612: [D loss: 0.664821, acc: 0.593750]  [G loss: 1.594254, acc: 0.078125]\n",
      "613: [D loss: 0.641426, acc: 0.679688]  [G loss: 0.628058, acc: 0.578125]\n",
      "614: [D loss: 0.526743, acc: 0.718750]  [G loss: 0.827120, acc: 0.406250]\n",
      "615: [D loss: 0.513061, acc: 0.718750]  [G loss: 0.518796, acc: 0.671875]\n",
      "616: [D loss: 0.502438, acc: 0.687500]  [G loss: 0.833569, acc: 0.453125]\n",
      "617: [D loss: 0.522635, acc: 0.742188]  [G loss: 0.576487, acc: 0.625000]\n",
      "618: [D loss: 0.454033, acc: 0.804688]  [G loss: 0.720008, acc: 0.531250]\n",
      "619: [D loss: 0.452353, acc: 0.718750]  [G loss: 0.485181, acc: 0.781250]\n",
      "620: [D loss: 0.593083, acc: 0.625000]  [G loss: 1.088566, acc: 0.187500]\n",
      "621: [D loss: 0.549639, acc: 0.687500]  [G loss: 0.472509, acc: 0.796875]\n",
      "622: [D loss: 0.647741, acc: 0.648438]  [G loss: 1.261317, acc: 0.109375]\n",
      "623: [D loss: 0.507327, acc: 0.726562]  [G loss: 0.628522, acc: 0.671875]\n",
      "624: [D loss: 0.510420, acc: 0.726562]  [G loss: 1.322164, acc: 0.046875]\n",
      "625: [D loss: 0.519366, acc: 0.718750]  [G loss: 0.771642, acc: 0.468750]\n",
      "626: [D loss: 0.493480, acc: 0.757812]  [G loss: 1.978235, acc: 0.078125]\n",
      "627: [D loss: 0.547798, acc: 0.703125]  [G loss: 0.957367, acc: 0.312500]\n",
      "628: [D loss: 0.460833, acc: 0.789062]  [G loss: 1.583800, acc: 0.031250]\n",
      "629: [D loss: 0.417778, acc: 0.789062]  [G loss: 0.485014, acc: 0.890625]\n",
      "630: [D loss: 0.791107, acc: 0.617188]  [G loss: 1.684126, acc: 0.000000]\n",
      "631: [D loss: 0.650122, acc: 0.632812]  [G loss: 0.618063, acc: 0.609375]\n",
      "632: [D loss: 0.424649, acc: 0.765625]  [G loss: 0.790090, acc: 0.359375]\n",
      "633: [D loss: 0.455118, acc: 0.757812]  [G loss: 0.842039, acc: 0.312500]\n",
      "634: [D loss: 0.467601, acc: 0.742188]  [G loss: 1.307684, acc: 0.031250]\n",
      "635: [D loss: 0.407343, acc: 0.789062]  [G loss: 0.609683, acc: 0.671875]\n",
      "636: [D loss: 0.473424, acc: 0.718750]  [G loss: 1.127558, acc: 0.062500]\n",
      "637: [D loss: 0.525317, acc: 0.703125]  [G loss: 0.713991, acc: 0.468750]\n",
      "638: [D loss: 0.435534, acc: 0.765625]  [G loss: 1.070430, acc: 0.250000]\n",
      "639: [D loss: 0.515685, acc: 0.773438]  [G loss: 0.759415, acc: 0.421875]\n",
      "640: [D loss: 0.399991, acc: 0.867188]  [G loss: 1.062182, acc: 0.328125]\n",
      "641: [D loss: 0.481860, acc: 0.757812]  [G loss: 0.705160, acc: 0.468750]\n",
      "642: [D loss: 0.370164, acc: 0.843750]  [G loss: 1.005198, acc: 0.203125]\n",
      "643: [D loss: 0.439572, acc: 0.796875]  [G loss: 0.808055, acc: 0.390625]\n",
      "644: [D loss: 0.384897, acc: 0.843750]  [G loss: 0.353939, acc: 0.875000]\n",
      "645: [D loss: 0.605627, acc: 0.632812]  [G loss: 3.387795, acc: 0.000000]\n",
      "646: [D loss: 1.051432, acc: 0.531250]  [G loss: 0.531338, acc: 0.671875]\n",
      "647: [D loss: 0.670607, acc: 0.664062]  [G loss: 1.162533, acc: 0.046875]\n",
      "648: [D loss: 0.542379, acc: 0.742188]  [G loss: 0.855529, acc: 0.234375]\n",
      "649: [D loss: 0.456668, acc: 0.820312]  [G loss: 0.828796, acc: 0.343750]\n",
      "650: [D loss: 0.448754, acc: 0.828125]  [G loss: 0.950420, acc: 0.203125]\n",
      "651: [D loss: 0.435236, acc: 0.804688]  [G loss: 1.315424, acc: 0.109375]\n",
      "652: [D loss: 0.391490, acc: 0.835938]  [G loss: 0.726213, acc: 0.515625]\n",
      "653: [D loss: 0.525908, acc: 0.648438]  [G loss: 1.071301, acc: 0.218750]\n",
      "654: [D loss: 0.422604, acc: 0.789062]  [G loss: 0.337289, acc: 0.890625]\n",
      "655: [D loss: 0.548318, acc: 0.710938]  [G loss: 1.326962, acc: 0.234375]\n",
      "656: [D loss: 0.476897, acc: 0.726562]  [G loss: 0.304008, acc: 0.828125]\n",
      "657: [D loss: 0.763363, acc: 0.632812]  [G loss: 1.229370, acc: 0.218750]\n",
      "658: [D loss: 0.537644, acc: 0.734375]  [G loss: 0.433648, acc: 0.812500]\n",
      "659: [D loss: 0.588077, acc: 0.726562]  [G loss: 1.149841, acc: 0.265625]\n",
      "660: [D loss: 0.438968, acc: 0.781250]  [G loss: 0.668373, acc: 0.531250]\n",
      "661: [D loss: 0.380107, acc: 0.828125]  [G loss: 0.453623, acc: 0.796875]\n",
      "662: [D loss: 0.393320, acc: 0.828125]  [G loss: 1.092888, acc: 0.390625]\n",
      "663: [D loss: 0.479896, acc: 0.757812]  [G loss: 0.440611, acc: 0.812500]\n",
      "664: [D loss: 0.576668, acc: 0.710938]  [G loss: 1.591594, acc: 0.046875]\n",
      "665: [D loss: 0.537409, acc: 0.687500]  [G loss: 0.630934, acc: 0.531250]\n",
      "666: [D loss: 0.547250, acc: 0.695312]  [G loss: 0.957942, acc: 0.312500]\n",
      "667: [D loss: 0.451723, acc: 0.804688]  [G loss: 0.695939, acc: 0.562500]\n",
      "668: [D loss: 0.445513, acc: 0.734375]  [G loss: 0.964404, acc: 0.343750]\n",
      "669: [D loss: 0.495818, acc: 0.765625]  [G loss: 0.580468, acc: 0.578125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670: [D loss: 0.432383, acc: 0.828125]  [G loss: 0.952747, acc: 0.359375]\n",
      "671: [D loss: 0.404736, acc: 0.851562]  [G loss: 0.655668, acc: 0.531250]\n",
      "672: [D loss: 0.461568, acc: 0.679688]  [G loss: 1.663814, acc: 0.109375]\n",
      "673: [D loss: 0.496910, acc: 0.773438]  [G loss: 0.470320, acc: 0.734375]\n",
      "674: [D loss: 0.560833, acc: 0.718750]  [G loss: 1.858227, acc: 0.031250]\n",
      "675: [D loss: 0.534863, acc: 0.750000]  [G loss: 0.707856, acc: 0.593750]\n",
      "676: [D loss: 0.423315, acc: 0.796875]  [G loss: 1.361702, acc: 0.093750]\n",
      "677: [D loss: 0.334782, acc: 0.867188]  [G loss: 1.150959, acc: 0.171875]\n",
      "678: [D loss: 0.425832, acc: 0.789062]  [G loss: 1.319023, acc: 0.171875]\n",
      "679: [D loss: 0.380225, acc: 0.843750]  [G loss: 0.917648, acc: 0.265625]\n",
      "680: [D loss: 0.423108, acc: 0.773438]  [G loss: 2.280898, acc: 0.031250]\n",
      "681: [D loss: 0.385572, acc: 0.820312]  [G loss: 1.057933, acc: 0.328125]\n",
      "682: [D loss: 0.476183, acc: 0.742188]  [G loss: 2.629891, acc: 0.000000]\n",
      "683: [D loss: 0.606516, acc: 0.679688]  [G loss: 0.423889, acc: 0.796875]\n",
      "684: [D loss: 0.926191, acc: 0.539062]  [G loss: 1.868921, acc: 0.015625]\n",
      "685: [D loss: 0.612483, acc: 0.679688]  [G loss: 0.931317, acc: 0.218750]\n",
      "686: [D loss: 0.539947, acc: 0.703125]  [G loss: 1.140416, acc: 0.234375]\n",
      "687: [D loss: 0.449139, acc: 0.796875]  [G loss: 0.792232, acc: 0.406250]\n",
      "688: [D loss: 0.528285, acc: 0.726562]  [G loss: 0.983723, acc: 0.265625]\n",
      "689: [D loss: 0.445478, acc: 0.750000]  [G loss: 0.857635, acc: 0.437500]\n",
      "690: [D loss: 0.417078, acc: 0.796875]  [G loss: 0.853998, acc: 0.343750]\n",
      "691: [D loss: 0.462607, acc: 0.726562]  [G loss: 0.942600, acc: 0.265625]\n",
      "692: [D loss: 0.439269, acc: 0.773438]  [G loss: 0.640261, acc: 0.671875]\n",
      "693: [D loss: 0.487437, acc: 0.710938]  [G loss: 1.273187, acc: 0.156250]\n",
      "694: [D loss: 0.470802, acc: 0.789062]  [G loss: 0.740892, acc: 0.437500]\n",
      "695: [D loss: 0.384225, acc: 0.828125]  [G loss: 0.951295, acc: 0.296875]\n",
      "696: [D loss: 0.433224, acc: 0.804688]  [G loss: 0.733837, acc: 0.468750]\n",
      "697: [D loss: 0.336587, acc: 0.835938]  [G loss: 0.918316, acc: 0.343750]\n",
      "698: [D loss: 0.417081, acc: 0.804688]  [G loss: 0.500589, acc: 0.703125]\n",
      "699: [D loss: 0.528270, acc: 0.671875]  [G loss: 2.758492, acc: 0.000000]\n",
      "700: [D loss: 0.709266, acc: 0.687500]  [G loss: 0.462911, acc: 0.734375]\n",
      "701: [D loss: 0.447553, acc: 0.734375]  [G loss: 1.235922, acc: 0.203125]\n",
      "702: [D loss: 0.311821, acc: 0.898438]  [G loss: 0.709477, acc: 0.468750]\n",
      "703: [D loss: 0.519679, acc: 0.695312]  [G loss: 1.180209, acc: 0.250000]\n",
      "704: [D loss: 0.500117, acc: 0.734375]  [G loss: 0.950932, acc: 0.359375]\n",
      "705: [D loss: 0.405232, acc: 0.781250]  [G loss: 0.787422, acc: 0.453125]\n",
      "706: [D loss: 0.332891, acc: 0.875000]  [G loss: 0.835706, acc: 0.406250]\n",
      "707: [D loss: 0.411944, acc: 0.820312]  [G loss: 0.526265, acc: 0.593750]\n",
      "708: [D loss: 0.399225, acc: 0.781250]  [G loss: 1.494044, acc: 0.078125]\n",
      "709: [D loss: 0.353420, acc: 0.851562]  [G loss: 0.740915, acc: 0.468750]\n",
      "710: [D loss: 0.383322, acc: 0.812500]  [G loss: 1.435258, acc: 0.140625]\n",
      "711: [D loss: 0.368201, acc: 0.851562]  [G loss: 0.830179, acc: 0.437500]\n",
      "712: [D loss: 0.311605, acc: 0.851562]  [G loss: 1.636461, acc: 0.062500]\n",
      "713: [D loss: 0.397453, acc: 0.820312]  [G loss: 0.579358, acc: 0.546875]\n",
      "714: [D loss: 0.409677, acc: 0.765625]  [G loss: 2.040932, acc: 0.000000]\n",
      "715: [D loss: 0.578230, acc: 0.695312]  [G loss: 0.352055, acc: 0.734375]\n",
      "716: [D loss: 0.708189, acc: 0.625000]  [G loss: 1.907873, acc: 0.015625]\n",
      "717: [D loss: 0.497031, acc: 0.718750]  [G loss: 0.840110, acc: 0.343750]\n",
      "718: [D loss: 0.436233, acc: 0.796875]  [G loss: 1.103588, acc: 0.156250]\n",
      "719: [D loss: 0.471801, acc: 0.742188]  [G loss: 0.597726, acc: 0.625000]\n",
      "720: [D loss: 0.434001, acc: 0.796875]  [G loss: 0.985820, acc: 0.343750]\n",
      "721: [D loss: 0.407339, acc: 0.796875]  [G loss: 0.782462, acc: 0.437500]\n",
      "722: [D loss: 0.458034, acc: 0.773438]  [G loss: 0.742166, acc: 0.500000]\n",
      "723: [D loss: 0.381536, acc: 0.851562]  [G loss: 0.560660, acc: 0.687500]\n",
      "724: [D loss: 0.350750, acc: 0.843750]  [G loss: 1.048449, acc: 0.453125]\n",
      "725: [D loss: 0.384231, acc: 0.820312]  [G loss: 0.718597, acc: 0.484375]\n",
      "726: [D loss: 0.438764, acc: 0.820312]  [G loss: 1.475952, acc: 0.171875]\n",
      "727: [D loss: 0.393391, acc: 0.789062]  [G loss: 0.530060, acc: 0.718750]\n",
      "728: [D loss: 0.368390, acc: 0.812500]  [G loss: 1.059498, acc: 0.343750]\n",
      "729: [D loss: 0.348258, acc: 0.851562]  [G loss: 0.985461, acc: 0.359375]\n",
      "730: [D loss: 0.297978, acc: 0.851562]  [G loss: 0.772683, acc: 0.546875]\n",
      "731: [D loss: 0.275093, acc: 0.890625]  [G loss: 1.036292, acc: 0.421875]\n",
      "732: [D loss: 0.297574, acc: 0.835938]  [G loss: 0.584283, acc: 0.625000]\n",
      "733: [D loss: 0.322015, acc: 0.851562]  [G loss: 2.975610, acc: 0.000000]\n",
      "734: [D loss: 0.337171, acc: 0.843750]  [G loss: 0.478841, acc: 0.718750]\n",
      "735: [D loss: 0.599006, acc: 0.742188]  [G loss: 2.855014, acc: 0.000000]\n",
      "736: [D loss: 0.424294, acc: 0.765625]  [G loss: 1.087782, acc: 0.218750]\n",
      "737: [D loss: 0.271262, acc: 0.867188]  [G loss: 1.872337, acc: 0.046875]\n",
      "738: [D loss: 0.287540, acc: 0.921875]  [G loss: 1.613318, acc: 0.078125]\n",
      "739: [D loss: 0.215377, acc: 0.937500]  [G loss: 2.024158, acc: 0.046875]\n",
      "740: [D loss: 0.260936, acc: 0.890625]  [G loss: 1.506077, acc: 0.250000]\n",
      "741: [D loss: 0.262880, acc: 0.882812]  [G loss: 1.672194, acc: 0.171875]\n",
      "742: [D loss: 0.250484, acc: 0.929688]  [G loss: 0.983546, acc: 0.468750]\n",
      "743: [D loss: 0.470839, acc: 0.796875]  [G loss: 3.311554, acc: 0.000000]\n",
      "744: [D loss: 0.570089, acc: 0.734375]  [G loss: 0.565703, acc: 0.640625]\n",
      "745: [D loss: 0.484752, acc: 0.718750]  [G loss: 1.930342, acc: 0.015625]\n",
      "746: [D loss: 0.367161, acc: 0.835938]  [G loss: 1.032382, acc: 0.234375]\n",
      "747: [D loss: 0.409705, acc: 0.796875]  [G loss: 1.447212, acc: 0.156250]\n",
      "748: [D loss: 0.410695, acc: 0.765625]  [G loss: 1.293437, acc: 0.187500]\n",
      "749: [D loss: 0.273902, acc: 0.906250]  [G loss: 1.175338, acc: 0.265625]\n",
      "750: [D loss: 0.377590, acc: 0.835938]  [G loss: 1.127124, acc: 0.296875]\n",
      "751: [D loss: 0.340392, acc: 0.851562]  [G loss: 0.375653, acc: 0.765625]\n",
      "752: [D loss: 0.427542, acc: 0.757812]  [G loss: 1.512470, acc: 0.203125]\n",
      "753: [D loss: 0.553059, acc: 0.773438]  [G loss: 0.441055, acc: 0.718750]\n",
      "754: [D loss: 0.522903, acc: 0.726562]  [G loss: 1.981180, acc: 0.078125]\n",
      "755: [D loss: 0.384313, acc: 0.812500]  [G loss: 0.673581, acc: 0.609375]\n",
      "756: [D loss: 0.328829, acc: 0.851562]  [G loss: 1.569710, acc: 0.156250]\n",
      "757: [D loss: 0.385291, acc: 0.796875]  [G loss: 0.896303, acc: 0.390625]\n",
      "758: [D loss: 0.315740, acc: 0.851562]  [G loss: 2.272879, acc: 0.015625]\n",
      "759: [D loss: 0.317445, acc: 0.851562]  [G loss: 0.591657, acc: 0.656250]\n",
      "760: [D loss: 0.403302, acc: 0.812500]  [G loss: 2.063288, acc: 0.031250]\n",
      "761: [D loss: 0.305428, acc: 0.875000]  [G loss: 0.666496, acc: 0.609375]\n",
      "762: [D loss: 0.382812, acc: 0.781250]  [G loss: 1.894964, acc: 0.156250]\n",
      "763: [D loss: 0.329077, acc: 0.851562]  [G loss: 1.027157, acc: 0.406250]\n",
      "764: [D loss: 0.285137, acc: 0.890625]  [G loss: 0.876087, acc: 0.484375]\n",
      "765: [D loss: 0.322021, acc: 0.828125]  [G loss: 2.000661, acc: 0.125000]\n",
      "766: [D loss: 0.443958, acc: 0.781250]  [G loss: 0.400615, acc: 0.843750]\n",
      "767: [D loss: 0.432306, acc: 0.757812]  [G loss: 1.372964, acc: 0.203125]\n",
      "768: [D loss: 0.383836, acc: 0.812500]  [G loss: 0.506942, acc: 0.750000]\n",
      "769: [D loss: 0.357623, acc: 0.789062]  [G loss: 1.272441, acc: 0.250000]\n",
      "770: [D loss: 0.386693, acc: 0.812500]  [G loss: 0.525621, acc: 0.765625]\n",
      "771: [D loss: 0.399931, acc: 0.820312]  [G loss: 1.133315, acc: 0.421875]\n",
      "772: [D loss: 0.368668, acc: 0.843750]  [G loss: 1.345032, acc: 0.156250]\n",
      "773: [D loss: 0.367734, acc: 0.828125]  [G loss: 0.460212, acc: 0.796875]\n",
      "774: [D loss: 0.544067, acc: 0.718750]  [G loss: 1.521822, acc: 0.187500]\n",
      "775: [D loss: 0.414621, acc: 0.804688]  [G loss: 0.375314, acc: 0.812500]\n",
      "776: [D loss: 0.365605, acc: 0.812500]  [G loss: 1.491530, acc: 0.234375]\n",
      "777: [D loss: 0.235495, acc: 0.937500]  [G loss: 1.158141, acc: 0.265625]\n",
      "778: [D loss: 0.329536, acc: 0.875000]  [G loss: 0.489531, acc: 0.703125]\n",
      "779: [D loss: 0.361350, acc: 0.781250]  [G loss: 2.328747, acc: 0.046875]\n",
      "780: [D loss: 0.480561, acc: 0.812500]  [G loss: 0.674574, acc: 0.625000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781: [D loss: 0.436791, acc: 0.820312]  [G loss: 1.018526, acc: 0.375000]\n",
      "782: [D loss: 0.363789, acc: 0.820312]  [G loss: 1.866850, acc: 0.031250]\n",
      "783: [D loss: 0.281425, acc: 0.875000]  [G loss: 0.832873, acc: 0.437500]\n",
      "784: [D loss: 0.319013, acc: 0.851562]  [G loss: 1.130119, acc: 0.265625]\n",
      "785: [D loss: 0.276277, acc: 0.875000]  [G loss: 0.829730, acc: 0.437500]\n",
      "786: [D loss: 0.338467, acc: 0.843750]  [G loss: 0.837583, acc: 0.515625]\n",
      "787: [D loss: 0.298180, acc: 0.875000]  [G loss: 1.495074, acc: 0.156250]\n",
      "788: [D loss: 0.302946, acc: 0.859375]  [G loss: 0.329216, acc: 0.875000]\n",
      "789: [D loss: 0.303498, acc: 0.851562]  [G loss: 1.878449, acc: 0.031250]\n",
      "790: [D loss: 0.318980, acc: 0.867188]  [G loss: 0.577848, acc: 0.765625]\n",
      "791: [D loss: 0.406419, acc: 0.781250]  [G loss: 2.657514, acc: 0.000000]\n",
      "792: [D loss: 0.348889, acc: 0.828125]  [G loss: 0.926549, acc: 0.421875]\n",
      "793: [D loss: 0.237213, acc: 0.914062]  [G loss: 1.308917, acc: 0.187500]\n",
      "794: [D loss: 0.290664, acc: 0.890625]  [G loss: 1.684884, acc: 0.234375]\n",
      "795: [D loss: 0.201874, acc: 0.945312]  [G loss: 0.668828, acc: 0.656250]\n",
      "796: [D loss: 0.427303, acc: 0.750000]  [G loss: 3.561576, acc: 0.000000]\n",
      "797: [D loss: 0.544401, acc: 0.765625]  [G loss: 0.534564, acc: 0.781250]\n",
      "798: [D loss: 0.833293, acc: 0.640625]  [G loss: 2.197847, acc: 0.015625]\n",
      "799: [D loss: 0.418951, acc: 0.781250]  [G loss: 0.516856, acc: 0.671875]\n",
      "800: [D loss: 0.515751, acc: 0.757812]  [G loss: 1.565563, acc: 0.125000]\n",
      "801: [D loss: 0.474927, acc: 0.757812]  [G loss: 0.614287, acc: 0.609375]\n",
      "802: [D loss: 0.457112, acc: 0.773438]  [G loss: 1.214958, acc: 0.203125]\n",
      "803: [D loss: 0.339612, acc: 0.843750]  [G loss: 0.848977, acc: 0.484375]\n",
      "804: [D loss: 0.260620, acc: 0.906250]  [G loss: 0.844679, acc: 0.390625]\n",
      "805: [D loss: 0.541787, acc: 0.726562]  [G loss: 1.255540, acc: 0.171875]\n",
      "806: [D loss: 0.347159, acc: 0.835938]  [G loss: 0.528656, acc: 0.703125]\n",
      "807: [D loss: 0.492939, acc: 0.734375]  [G loss: 1.946581, acc: 0.078125]\n",
      "808: [D loss: 0.427680, acc: 0.781250]  [G loss: 0.814574, acc: 0.406250]\n",
      "809: [D loss: 0.382974, acc: 0.804688]  [G loss: 1.171703, acc: 0.156250]\n",
      "810: [D loss: 0.379585, acc: 0.835938]  [G loss: 0.917909, acc: 0.359375]\n",
      "811: [D loss: 0.393020, acc: 0.812500]  [G loss: 1.294941, acc: 0.234375]\n",
      "812: [D loss: 0.270039, acc: 0.906250]  [G loss: 0.659068, acc: 0.562500]\n",
      "813: [D loss: 0.365303, acc: 0.804688]  [G loss: 1.233897, acc: 0.296875]\n",
      "814: [D loss: 0.467225, acc: 0.789062]  [G loss: 0.408595, acc: 0.781250]\n",
      "815: [D loss: 0.481543, acc: 0.742188]  [G loss: 1.441214, acc: 0.125000]\n",
      "816: [D loss: 0.381406, acc: 0.812500]  [G loss: 0.530361, acc: 0.656250]\n",
      "817: [D loss: 0.282944, acc: 0.898438]  [G loss: 0.717543, acc: 0.562500]\n",
      "818: [D loss: 0.275400, acc: 0.914062]  [G loss: 0.697997, acc: 0.593750]\n",
      "819: [D loss: 0.261395, acc: 0.875000]  [G loss: 0.339210, acc: 0.875000]\n",
      "820: [D loss: 0.500781, acc: 0.742188]  [G loss: 2.122970, acc: 0.031250]\n",
      "821: [D loss: 0.656760, acc: 0.687500]  [G loss: 0.436685, acc: 0.828125]\n",
      "822: [D loss: 0.523081, acc: 0.773438]  [G loss: 1.637012, acc: 0.062500]\n",
      "823: [D loss: 0.405011, acc: 0.796875]  [G loss: 0.989821, acc: 0.296875]\n",
      "824: [D loss: 0.447090, acc: 0.781250]  [G loss: 0.877161, acc: 0.375000]\n",
      "825: [D loss: 0.467359, acc: 0.796875]  [G loss: 0.874580, acc: 0.390625]\n",
      "826: [D loss: 0.435163, acc: 0.750000]  [G loss: 1.035588, acc: 0.281250]\n",
      "827: [D loss: 0.451279, acc: 0.757812]  [G loss: 0.644130, acc: 0.578125]\n",
      "828: [D loss: 0.413055, acc: 0.781250]  [G loss: 1.228987, acc: 0.296875]\n",
      "829: [D loss: 0.430255, acc: 0.765625]  [G loss: 0.897937, acc: 0.359375]\n",
      "830: [D loss: 0.289595, acc: 0.882812]  [G loss: 1.142586, acc: 0.250000]\n",
      "831: [D loss: 0.388261, acc: 0.835938]  [G loss: 1.222713, acc: 0.312500]\n",
      "832: [D loss: 0.334733, acc: 0.843750]  [G loss: 1.187595, acc: 0.296875]\n",
      "833: [D loss: 0.405099, acc: 0.789062]  [G loss: 0.990513, acc: 0.437500]\n",
      "834: [D loss: 0.310918, acc: 0.875000]  [G loss: 1.568524, acc: 0.234375]\n",
      "835: [D loss: 0.438875, acc: 0.789062]  [G loss: 0.593487, acc: 0.625000]\n",
      "836: [D loss: 0.498368, acc: 0.789062]  [G loss: 0.918279, acc: 0.515625]\n",
      "837: [D loss: 0.308947, acc: 0.875000]  [G loss: 0.770178, acc: 0.484375]\n",
      "838: [D loss: 0.339839, acc: 0.835938]  [G loss: 1.127316, acc: 0.390625]\n",
      "839: [D loss: 0.449263, acc: 0.742188]  [G loss: 1.044004, acc: 0.312500]\n",
      "840: [D loss: 0.425338, acc: 0.734375]  [G loss: 0.392595, acc: 0.796875]\n",
      "841: [D loss: 0.372462, acc: 0.804688]  [G loss: 1.737689, acc: 0.062500]\n",
      "842: [D loss: 0.309093, acc: 0.867188]  [G loss: 0.746055, acc: 0.593750]\n",
      "843: [D loss: 0.420869, acc: 0.726562]  [G loss: 0.803659, acc: 0.437500]\n",
      "844: [D loss: 0.327702, acc: 0.867188]  [G loss: 1.172561, acc: 0.171875]\n",
      "845: [D loss: 0.322415, acc: 0.851562]  [G loss: 1.143319, acc: 0.203125]\n",
      "846: [D loss: 0.511495, acc: 0.796875]  [G loss: 0.930745, acc: 0.406250]\n",
      "847: [D loss: 0.545341, acc: 0.765625]  [G loss: 0.764960, acc: 0.421875]\n",
      "848: [D loss: 0.339393, acc: 0.851562]  [G loss: 1.254634, acc: 0.171875]\n",
      "849: [D loss: 0.321884, acc: 0.867188]  [G loss: 0.535769, acc: 0.640625]\n",
      "850: [D loss: 0.241289, acc: 0.843750]  [G loss: 0.476106, acc: 0.671875]\n",
      "851: [D loss: 0.302088, acc: 0.843750]  [G loss: 1.148397, acc: 0.312500]\n",
      "852: [D loss: 0.398466, acc: 0.789062]  [G loss: 1.508330, acc: 0.140625]\n",
      "853: [D loss: 0.364397, acc: 0.812500]  [G loss: 0.844529, acc: 0.421875]\n",
      "854: [D loss: 0.284996, acc: 0.875000]  [G loss: 3.127019, acc: 0.000000]\n",
      "855: [D loss: 0.538560, acc: 0.789062]  [G loss: 0.544419, acc: 0.640625]\n",
      "856: [D loss: 0.536497, acc: 0.695312]  [G loss: 2.801675, acc: 0.000000]\n",
      "857: [D loss: 0.530363, acc: 0.757812]  [G loss: 0.756374, acc: 0.468750]\n",
      "858: [D loss: 0.331480, acc: 0.851562]  [G loss: 1.406439, acc: 0.062500]\n",
      "859: [D loss: 0.278757, acc: 0.882812]  [G loss: 0.827275, acc: 0.359375]\n",
      "860: [D loss: 0.246332, acc: 0.906250]  [G loss: 0.743697, acc: 0.531250]\n",
      "861: [D loss: 0.274822, acc: 0.875000]  [G loss: 0.746486, acc: 0.531250]\n",
      "862: [D loss: 0.309622, acc: 0.851562]  [G loss: 1.683635, acc: 0.046875]\n",
      "863: [D loss: 0.346708, acc: 0.804688]  [G loss: 0.278042, acc: 0.968750]\n",
      "864: [D loss: 0.105445, acc: 0.968750]  [G loss: 0.380009, acc: 0.843750]\n",
      "865: [D loss: 0.344187, acc: 0.812500]  [G loss: 1.002030, acc: 0.406250]\n",
      "866: [D loss: 0.298868, acc: 0.898438]  [G loss: 0.402573, acc: 0.781250]\n",
      "867: [D loss: 0.293653, acc: 0.851562]  [G loss: 1.242547, acc: 0.312500]\n",
      "868: [D loss: 0.302147, acc: 0.859375]  [G loss: 0.394200, acc: 0.859375]\n",
      "869: [D loss: 0.222521, acc: 0.914062]  [G loss: 0.529017, acc: 0.734375]\n",
      "870: [D loss: 0.339978, acc: 0.851562]  [G loss: 0.539679, acc: 0.734375]\n",
      "871: [D loss: 0.586547, acc: 0.726562]  [G loss: 2.203731, acc: 0.015625]\n",
      "872: [D loss: 0.382971, acc: 0.835938]  [G loss: 0.493109, acc: 0.718750]\n",
      "873: [D loss: 0.538468, acc: 0.734375]  [G loss: 1.295039, acc: 0.093750]\n",
      "874: [D loss: 0.450021, acc: 0.796875]  [G loss: 0.705142, acc: 0.531250]\n",
      "875: [D loss: 0.377408, acc: 0.835938]  [G loss: 1.643798, acc: 0.031250]\n",
      "876: [D loss: 0.383488, acc: 0.796875]  [G loss: 0.666142, acc: 0.609375]\n",
      "877: [D loss: 0.394032, acc: 0.781250]  [G loss: 1.817227, acc: 0.015625]\n",
      "878: [D loss: 0.331191, acc: 0.835938]  [G loss: 1.187288, acc: 0.156250]\n",
      "879: [D loss: 0.311874, acc: 0.867188]  [G loss: 1.761027, acc: 0.031250]\n",
      "880: [D loss: 0.420304, acc: 0.804688]  [G loss: 1.571968, acc: 0.062500]\n",
      "881: [D loss: 0.364682, acc: 0.843750]  [G loss: 1.373631, acc: 0.109375]\n",
      "882: [D loss: 0.222435, acc: 0.898438]  [G loss: 1.101374, acc: 0.265625]\n",
      "883: [D loss: 0.350218, acc: 0.835938]  [G loss: 1.739694, acc: 0.093750]\n",
      "884: [D loss: 0.276619, acc: 0.867188]  [G loss: 0.593319, acc: 0.656250]\n",
      "885: [D loss: 0.519213, acc: 0.742188]  [G loss: 2.180155, acc: 0.062500]\n",
      "886: [D loss: 0.481471, acc: 0.757812]  [G loss: 0.541112, acc: 0.703125]\n",
      "887: [D loss: 0.576224, acc: 0.703125]  [G loss: 1.770992, acc: 0.062500]\n",
      "888: [D loss: 0.433943, acc: 0.773438]  [G loss: 1.001109, acc: 0.250000]\n",
      "889: [D loss: 0.353229, acc: 0.804688]  [G loss: 0.983008, acc: 0.281250]\n",
      "890: [D loss: 0.348981, acc: 0.859375]  [G loss: 0.707235, acc: 0.531250]\n",
      "891: [D loss: 0.340499, acc: 0.867188]  [G loss: 0.859892, acc: 0.500000]\n",
      "892: [D loss: 0.361219, acc: 0.820312]  [G loss: 0.502350, acc: 0.687500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893: [D loss: 0.399984, acc: 0.812500]  [G loss: 0.817004, acc: 0.484375]\n",
      "894: [D loss: 0.466008, acc: 0.781250]  [G loss: 0.557857, acc: 0.625000]\n",
      "895: [D loss: 0.407881, acc: 0.804688]  [G loss: 0.781653, acc: 0.453125]\n",
      "896: [D loss: 0.517097, acc: 0.757812]  [G loss: 0.694129, acc: 0.531250]\n",
      "897: [D loss: 0.454968, acc: 0.742188]  [G loss: 0.631061, acc: 0.656250]\n",
      "898: [D loss: 0.508160, acc: 0.742188]  [G loss: 1.560223, acc: 0.125000]\n",
      "899: [D loss: 0.516421, acc: 0.695312]  [G loss: 0.262399, acc: 0.906250]\n",
      "900: [D loss: 0.487209, acc: 0.796875]  [G loss: 0.997883, acc: 0.250000]\n",
      "901: [D loss: 0.432448, acc: 0.796875]  [G loss: 0.338305, acc: 0.828125]\n",
      "902: [D loss: 0.392266, acc: 0.773438]  [G loss: 1.220462, acc: 0.296875]\n",
      "903: [D loss: 0.485947, acc: 0.726562]  [G loss: 0.490808, acc: 0.750000]\n",
      "904: [D loss: 0.403750, acc: 0.804688]  [G loss: 0.593426, acc: 0.656250]\n",
      "905: [D loss: 0.412143, acc: 0.773438]  [G loss: 0.882518, acc: 0.375000]\n",
      "906: [D loss: 0.386917, acc: 0.812500]  [G loss: 0.230089, acc: 0.937500]\n",
      "907: [D loss: 0.564501, acc: 0.726562]  [G loss: 1.404584, acc: 0.093750]\n",
      "908: [D loss: 0.493729, acc: 0.718750]  [G loss: 0.668844, acc: 0.546875]\n",
      "909: [D loss: 0.475272, acc: 0.718750]  [G loss: 1.491516, acc: 0.046875]\n",
      "910: [D loss: 0.451537, acc: 0.757812]  [G loss: 0.811148, acc: 0.390625]\n",
      "911: [D loss: 0.360326, acc: 0.859375]  [G loss: 0.819273, acc: 0.421875]\n",
      "912: [D loss: 0.410163, acc: 0.820312]  [G loss: 0.649355, acc: 0.562500]\n",
      "913: [D loss: 0.402521, acc: 0.765625]  [G loss: 0.510768, acc: 0.687500]\n",
      "914: [D loss: 0.288281, acc: 0.882812]  [G loss: 1.568229, acc: 0.093750]\n",
      "915: [D loss: 0.533099, acc: 0.726562]  [G loss: 0.232785, acc: 0.968750]\n",
      "916: [D loss: 0.557245, acc: 0.703125]  [G loss: 1.290611, acc: 0.015625]\n",
      "917: [D loss: 0.305564, acc: 0.828125]  [G loss: 0.513078, acc: 0.718750]\n",
      "918: [D loss: 0.335836, acc: 0.843750]  [G loss: 0.571608, acc: 0.750000]\n",
      "919: [D loss: 0.363545, acc: 0.835938]  [G loss: 0.620546, acc: 0.609375]\n",
      "920: [D loss: 0.365216, acc: 0.851562]  [G loss: 0.631607, acc: 0.593750]\n",
      "921: [D loss: 0.412855, acc: 0.789062]  [G loss: 1.143988, acc: 0.234375]\n",
      "922: [D loss: 0.341763, acc: 0.859375]  [G loss: 0.561442, acc: 0.687500]\n",
      "923: [D loss: 0.460116, acc: 0.750000]  [G loss: 2.004235, acc: 0.015625]\n",
      "924: [D loss: 0.439413, acc: 0.773438]  [G loss: 0.359087, acc: 0.812500]\n",
      "925: [D loss: 0.541712, acc: 0.695312]  [G loss: 2.073803, acc: 0.000000]\n",
      "926: [D loss: 0.386000, acc: 0.773438]  [G loss: 0.632470, acc: 0.625000]\n",
      "927: [D loss: 0.449986, acc: 0.804688]  [G loss: 1.426014, acc: 0.125000]\n",
      "928: [D loss: 0.237705, acc: 0.890625]  [G loss: 0.616556, acc: 0.578125]\n",
      "929: [D loss: 0.521558, acc: 0.718750]  [G loss: 1.627401, acc: 0.015625]\n",
      "930: [D loss: 0.536393, acc: 0.718750]  [G loss: 0.939230, acc: 0.328125]\n",
      "931: [D loss: 0.380927, acc: 0.835938]  [G loss: 0.825649, acc: 0.484375]\n",
      "932: [D loss: 0.343029, acc: 0.875000]  [G loss: 0.797192, acc: 0.437500]\n",
      "933: [D loss: 0.365227, acc: 0.851562]  [G loss: 1.224880, acc: 0.078125]\n",
      "934: [D loss: 0.489515, acc: 0.773438]  [G loss: 0.986316, acc: 0.296875]\n",
      "935: [D loss: 0.390169, acc: 0.804688]  [G loss: 1.115522, acc: 0.250000]\n",
      "936: [D loss: 0.368154, acc: 0.859375]  [G loss: 0.595335, acc: 0.640625]\n",
      "937: [D loss: 0.539893, acc: 0.757812]  [G loss: 1.827948, acc: 0.062500]\n",
      "938: [D loss: 0.450328, acc: 0.804688]  [G loss: 0.600965, acc: 0.578125]\n",
      "939: [D loss: 0.471882, acc: 0.757812]  [G loss: 1.332684, acc: 0.046875]\n",
      "940: [D loss: 0.430810, acc: 0.789062]  [G loss: 0.639588, acc: 0.562500]\n",
      "941: [D loss: 0.378712, acc: 0.820312]  [G loss: 1.239358, acc: 0.171875]\n",
      "942: [D loss: 0.392720, acc: 0.859375]  [G loss: 0.548438, acc: 0.671875]\n",
      "943: [D loss: 0.472950, acc: 0.757812]  [G loss: 1.296823, acc: 0.140625]\n",
      "944: [D loss: 0.382701, acc: 0.796875]  [G loss: 0.447280, acc: 0.734375]\n",
      "945: [D loss: 0.225692, acc: 0.906250]  [G loss: 0.508060, acc: 0.750000]\n",
      "946: [D loss: 0.342430, acc: 0.843750]  [G loss: 0.949259, acc: 0.421875]\n",
      "947: [D loss: 0.421642, acc: 0.796875]  [G loss: 1.151231, acc: 0.296875]\n",
      "948: [D loss: 0.410308, acc: 0.789062]  [G loss: 0.433245, acc: 0.734375]\n",
      "949: [D loss: 0.566641, acc: 0.710938]  [G loss: 1.692738, acc: 0.078125]\n",
      "950: [D loss: 0.507881, acc: 0.726562]  [G loss: 0.479124, acc: 0.734375]\n",
      "951: [D loss: 0.661796, acc: 0.671875]  [G loss: 1.425826, acc: 0.015625]\n",
      "952: [D loss: 0.450133, acc: 0.773438]  [G loss: 0.817702, acc: 0.406250]\n",
      "953: [D loss: 0.465429, acc: 0.781250]  [G loss: 0.668986, acc: 0.546875]\n",
      "954: [D loss: 0.375970, acc: 0.851562]  [G loss: 0.775024, acc: 0.453125]\n",
      "955: [D loss: 0.397358, acc: 0.828125]  [G loss: 0.862479, acc: 0.421875]\n",
      "956: [D loss: 0.393620, acc: 0.773438]  [G loss: 0.596581, acc: 0.656250]\n",
      "957: [D loss: 0.431404, acc: 0.773438]  [G loss: 0.870791, acc: 0.406250]\n",
      "958: [D loss: 0.445483, acc: 0.804688]  [G loss: 0.614959, acc: 0.578125]\n",
      "959: [D loss: 0.390386, acc: 0.796875]  [G loss: 1.253736, acc: 0.203125]\n",
      "960: [D loss: 0.382300, acc: 0.828125]  [G loss: 0.336241, acc: 0.875000]\n",
      "961: [D loss: 0.636827, acc: 0.687500]  [G loss: 1.381817, acc: 0.062500]\n",
      "962: [D loss: 0.542696, acc: 0.726562]  [G loss: 0.455292, acc: 0.812500]\n",
      "963: [D loss: 0.553119, acc: 0.742188]  [G loss: 1.083132, acc: 0.265625]\n",
      "964: [D loss: 0.510405, acc: 0.703125]  [G loss: 0.721128, acc: 0.515625]\n",
      "965: [D loss: 0.436567, acc: 0.789062]  [G loss: 0.876077, acc: 0.359375]\n",
      "966: [D loss: 0.354856, acc: 0.867188]  [G loss: 0.761339, acc: 0.515625]\n",
      "967: [D loss: 0.513940, acc: 0.734375]  [G loss: 1.162665, acc: 0.078125]\n",
      "968: [D loss: 0.443220, acc: 0.773438]  [G loss: 0.611289, acc: 0.625000]\n",
      "969: [D loss: 0.450488, acc: 0.765625]  [G loss: 1.068904, acc: 0.203125]\n",
      "970: [D loss: 0.370240, acc: 0.835938]  [G loss: 0.865386, acc: 0.468750]\n",
      "971: [D loss: 0.358700, acc: 0.804688]  [G loss: 1.085702, acc: 0.296875]\n",
      "972: [D loss: 0.329104, acc: 0.890625]  [G loss: 0.799038, acc: 0.484375]\n",
      "973: [D loss: 0.281776, acc: 0.859375]  [G loss: 1.652031, acc: 0.078125]\n",
      "974: [D loss: 0.315021, acc: 0.828125]  [G loss: 0.612560, acc: 0.625000]\n",
      "975: [D loss: 0.373899, acc: 0.820312]  [G loss: 1.346393, acc: 0.093750]\n",
      "976: [D loss: 0.368978, acc: 0.820312]  [G loss: 0.234275, acc: 0.921875]\n",
      "977: [D loss: 0.546028, acc: 0.726562]  [G loss: 1.501820, acc: 0.062500]\n",
      "978: [D loss: 0.419372, acc: 0.812500]  [G loss: 0.546811, acc: 0.687500]\n",
      "979: [D loss: 0.305242, acc: 0.867188]  [G loss: 0.921439, acc: 0.375000]\n",
      "980: [D loss: 0.452765, acc: 0.796875]  [G loss: 0.904237, acc: 0.375000]\n",
      "981: [D loss: 0.267772, acc: 0.921875]  [G loss: 0.988848, acc: 0.390625]\n",
      "982: [D loss: 0.440517, acc: 0.773438]  [G loss: 1.876511, acc: 0.031250]\n",
      "983: [D loss: 0.430152, acc: 0.796875]  [G loss: 0.283658, acc: 0.875000]\n",
      "984: [D loss: 0.732175, acc: 0.679688]  [G loss: 2.381588, acc: 0.015625]\n",
      "985: [D loss: 0.539050, acc: 0.710938]  [G loss: 0.327511, acc: 0.859375]\n",
      "986: [D loss: 0.543888, acc: 0.710938]  [G loss: 1.794670, acc: 0.000000]\n",
      "987: [D loss: 0.409321, acc: 0.796875]  [G loss: 1.039019, acc: 0.218750]\n",
      "988: [D loss: 0.364998, acc: 0.804688]  [G loss: 1.259751, acc: 0.187500]\n",
      "989: [D loss: 0.374530, acc: 0.851562]  [G loss: 1.076566, acc: 0.218750]\n",
      "990: [D loss: 0.383598, acc: 0.843750]  [G loss: 1.127033, acc: 0.234375]\n",
      "991: [D loss: 0.310763, acc: 0.867188]  [G loss: 0.805899, acc: 0.453125]\n",
      "992: [D loss: 0.333258, acc: 0.828125]  [G loss: 1.601943, acc: 0.109375]\n",
      "993: [D loss: 0.373669, acc: 0.812500]  [G loss: 0.452174, acc: 0.765625]\n",
      "994: [D loss: 0.452803, acc: 0.742188]  [G loss: 1.728602, acc: 0.171875]\n",
      "995: [D loss: 0.440634, acc: 0.804688]  [G loss: 0.312135, acc: 0.859375]\n",
      "996: [D loss: 0.422387, acc: 0.789062]  [G loss: 1.156274, acc: 0.343750]\n",
      "997: [D loss: 0.365469, acc: 0.843750]  [G loss: 0.911298, acc: 0.421875]\n",
      "998: [D loss: 0.236796, acc: 0.898438]  [G loss: 0.574503, acc: 0.656250]\n",
      "999: [D loss: 0.391774, acc: 0.804688]  [G loss: 1.046429, acc: 0.328125]\n",
      "1000: [D loss: 0.285704, acc: 0.859375]  [G loss: 0.546614, acc: 0.687500]\n",
      "1001: [D loss: 0.395317, acc: 0.828125]  [G loss: 1.613471, acc: 0.156250]\n",
      "1002: [D loss: 0.373484, acc: 0.828125]  [G loss: 0.419660, acc: 0.812500]\n",
      "1003: [D loss: 0.587698, acc: 0.695312]  [G loss: 2.235700, acc: 0.046875]\n",
      "1004: [D loss: 0.445666, acc: 0.773438]  [G loss: 0.780971, acc: 0.562500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005: [D loss: 0.630946, acc: 0.734375]  [G loss: 1.541859, acc: 0.140625]\n",
      "1006: [D loss: 0.319561, acc: 0.851562]  [G loss: 0.765155, acc: 0.484375]\n",
      "1007: [D loss: 0.331675, acc: 0.828125]  [G loss: 0.858156, acc: 0.421875]\n",
      "1008: [D loss: 0.506486, acc: 0.726562]  [G loss: 1.097740, acc: 0.234375]\n",
      "1009: [D loss: 0.382416, acc: 0.812500]  [G loss: 0.448207, acc: 0.765625]\n",
      "1010: [D loss: 0.469425, acc: 0.773438]  [G loss: 1.095953, acc: 0.343750]\n",
      "1011: [D loss: 0.322921, acc: 0.851562]  [G loss: 0.579511, acc: 0.687500]\n",
      "1012: [D loss: 0.333833, acc: 0.867188]  [G loss: 0.718041, acc: 0.562500]\n",
      "1013: [D loss: 0.506841, acc: 0.734375]  [G loss: 0.867866, acc: 0.453125]\n",
      "1014: [D loss: 0.286911, acc: 0.875000]  [G loss: 0.584199, acc: 0.593750]\n",
      "1015: [D loss: 0.321552, acc: 0.867188]  [G loss: 0.697020, acc: 0.625000]\n",
      "1016: [D loss: 0.435938, acc: 0.781250]  [G loss: 0.581459, acc: 0.687500]\n",
      "1017: [D loss: 0.265562, acc: 0.867188]  [G loss: 0.671773, acc: 0.593750]\n",
      "1018: [D loss: 0.315962, acc: 0.867188]  [G loss: 1.929303, acc: 0.125000]\n",
      "1019: [D loss: 0.331762, acc: 0.851562]  [G loss: 0.661660, acc: 0.593750]\n",
      "1020: [D loss: 0.307666, acc: 0.875000]  [G loss: 1.045655, acc: 0.375000]\n",
      "1021: [D loss: 0.340797, acc: 0.843750]  [G loss: 0.354449, acc: 0.828125]\n",
      "1022: [D loss: 0.402293, acc: 0.804688]  [G loss: 3.281891, acc: 0.000000]\n",
      "1023: [D loss: 0.500063, acc: 0.804688]  [G loss: 0.440096, acc: 0.828125]\n",
      "1024: [D loss: 0.497150, acc: 0.757812]  [G loss: 1.471023, acc: 0.109375]\n",
      "1025: [D loss: 0.450660, acc: 0.750000]  [G loss: 0.673324, acc: 0.578125]\n",
      "1026: [D loss: 0.323534, acc: 0.804688]  [G loss: 1.059994, acc: 0.203125]\n",
      "1027: [D loss: 0.272020, acc: 0.882812]  [G loss: 0.787586, acc: 0.437500]\n",
      "1028: [D loss: 0.296844, acc: 0.843750]  [G loss: 1.069711, acc: 0.359375]\n",
      "1029: [D loss: 0.365747, acc: 0.851562]  [G loss: 1.863018, acc: 0.109375]\n",
      "1030: [D loss: 0.393966, acc: 0.773438]  [G loss: 0.734065, acc: 0.531250]\n",
      "1031: [D loss: 0.399139, acc: 0.812500]  [G loss: 1.436507, acc: 0.156250]\n",
      "1032: [D loss: 0.394020, acc: 0.796875]  [G loss: 0.785670, acc: 0.421875]\n",
      "1033: [D loss: 0.392159, acc: 0.773438]  [G loss: 1.590098, acc: 0.125000]\n",
      "1034: [D loss: 0.365650, acc: 0.789062]  [G loss: 0.486392, acc: 0.750000]\n",
      "1035: [D loss: 0.409872, acc: 0.812500]  [G loss: 1.486902, acc: 0.093750]\n",
      "1036: [D loss: 0.343418, acc: 0.843750]  [G loss: 0.734001, acc: 0.468750]\n",
      "1037: [D loss: 0.382154, acc: 0.789062]  [G loss: 2.604006, acc: 0.000000]\n",
      "1038: [D loss: 0.370965, acc: 0.851562]  [G loss: 1.125423, acc: 0.250000]\n",
      "1039: [D loss: 0.204870, acc: 0.929688]  [G loss: 1.228485, acc: 0.187500]\n",
      "1040: [D loss: 0.360971, acc: 0.835938]  [G loss: 0.592641, acc: 0.656250]\n",
      "1041: [D loss: 0.395487, acc: 0.835938]  [G loss: 1.963836, acc: 0.078125]\n",
      "1042: [D loss: 0.413842, acc: 0.804688]  [G loss: 0.907828, acc: 0.359375]\n",
      "1043: [D loss: 0.293990, acc: 0.867188]  [G loss: 0.805310, acc: 0.484375]\n",
      "1044: [D loss: 0.499931, acc: 0.757812]  [G loss: 1.794652, acc: 0.046875]\n",
      "1045: [D loss: 0.365964, acc: 0.843750]  [G loss: 0.920313, acc: 0.328125]\n",
      "1046: [D loss: 0.384822, acc: 0.796875]  [G loss: 2.576178, acc: 0.000000]\n",
      "1047: [D loss: 0.423663, acc: 0.773438]  [G loss: 0.411133, acc: 0.796875]\n",
      "1048: [D loss: 0.827044, acc: 0.671875]  [G loss: 2.184305, acc: 0.015625]\n",
      "1049: [D loss: 0.522756, acc: 0.765625]  [G loss: 0.794632, acc: 0.500000]\n",
      "1050: [D loss: 0.406571, acc: 0.812500]  [G loss: 1.001738, acc: 0.250000]\n",
      "1051: [D loss: 0.289511, acc: 0.882812]  [G loss: 0.851002, acc: 0.390625]\n",
      "1052: [D loss: 0.299442, acc: 0.843750]  [G loss: 0.618619, acc: 0.593750]\n",
      "1053: [D loss: 0.266926, acc: 0.890625]  [G loss: 0.959586, acc: 0.406250]\n",
      "1054: [D loss: 0.296914, acc: 0.867188]  [G loss: 0.535595, acc: 0.703125]\n",
      "1055: [D loss: 0.349262, acc: 0.851562]  [G loss: 1.043423, acc: 0.359375]\n",
      "1056: [D loss: 0.398605, acc: 0.820312]  [G loss: 0.495805, acc: 0.656250]\n",
      "1057: [D loss: 0.470404, acc: 0.734375]  [G loss: 2.332175, acc: 0.109375]\n",
      "1058: [D loss: 0.389483, acc: 0.812500]  [G loss: 0.708983, acc: 0.515625]\n",
      "1059: [D loss: 0.292784, acc: 0.867188]  [G loss: 1.088902, acc: 0.281250]\n",
      "1060: [D loss: 0.372422, acc: 0.851562]  [G loss: 1.314080, acc: 0.234375]\n",
      "1061: [D loss: 0.329692, acc: 0.859375]  [G loss: 0.844074, acc: 0.375000]\n",
      "1062: [D loss: 0.336599, acc: 0.867188]  [G loss: 1.420081, acc: 0.234375]\n",
      "1063: [D loss: 0.368321, acc: 0.867188]  [G loss: 0.693635, acc: 0.515625]\n",
      "1064: [D loss: 0.394861, acc: 0.789062]  [G loss: 1.929239, acc: 0.062500]\n",
      "1065: [D loss: 0.584979, acc: 0.726562]  [G loss: 0.395789, acc: 0.781250]\n",
      "1066: [D loss: 0.601626, acc: 0.718750]  [G loss: 1.750468, acc: 0.140625]\n",
      "1067: [D loss: 0.520985, acc: 0.734375]  [G loss: 1.373097, acc: 0.218750]\n",
      "1068: [D loss: 0.360689, acc: 0.867188]  [G loss: 0.970573, acc: 0.359375]\n",
      "1069: [D loss: 0.436760, acc: 0.742188]  [G loss: 0.994079, acc: 0.359375]\n",
      "1070: [D loss: 0.440650, acc: 0.796875]  [G loss: 0.728828, acc: 0.468750]\n",
      "1071: [D loss: 0.401611, acc: 0.804688]  [G loss: 0.914663, acc: 0.421875]\n",
      "1072: [D loss: 0.432734, acc: 0.804688]  [G loss: 0.769001, acc: 0.468750]\n",
      "1073: [D loss: 0.331368, acc: 0.828125]  [G loss: 0.508860, acc: 0.671875]\n",
      "1074: [D loss: 0.378653, acc: 0.843750]  [G loss: 0.911934, acc: 0.500000]\n",
      "1075: [D loss: 0.410628, acc: 0.781250]  [G loss: 0.451318, acc: 0.765625]\n",
      "1076: [D loss: 0.494485, acc: 0.757812]  [G loss: 2.182352, acc: 0.000000]\n",
      "1077: [D loss: 0.498885, acc: 0.679688]  [G loss: 0.425489, acc: 0.765625]\n",
      "1078: [D loss: 0.398357, acc: 0.742188]  [G loss: 0.729923, acc: 0.609375]\n",
      "1079: [D loss: 0.356204, acc: 0.843750]  [G loss: 0.468237, acc: 0.765625]\n",
      "1080: [D loss: 0.354710, acc: 0.859375]  [G loss: 0.734954, acc: 0.515625]\n",
      "1081: [D loss: 0.359722, acc: 0.843750]  [G loss: 0.697029, acc: 0.578125]\n",
      "1082: [D loss: 0.350872, acc: 0.828125]  [G loss: 0.607925, acc: 0.562500]\n",
      "1083: [D loss: 0.322603, acc: 0.867188]  [G loss: 1.117599, acc: 0.375000]\n",
      "1084: [D loss: 0.417985, acc: 0.828125]  [G loss: 0.189935, acc: 0.921875]\n",
      "1085: [D loss: 0.484885, acc: 0.726562]  [G loss: 1.150046, acc: 0.328125]\n",
      "1086: [D loss: 0.668336, acc: 0.726562]  [G loss: 0.442296, acc: 0.781250]\n",
      "1087: [D loss: 0.362052, acc: 0.804688]  [G loss: 0.620057, acc: 0.578125]\n",
      "1088: [D loss: 0.455417, acc: 0.750000]  [G loss: 1.492123, acc: 0.093750]\n",
      "1089: [D loss: 0.525946, acc: 0.710938]  [G loss: 0.820922, acc: 0.515625]\n",
      "1090: [D loss: 0.292921, acc: 0.851562]  [G loss: 0.336554, acc: 0.890625]\n",
      "1091: [D loss: 0.440548, acc: 0.742188]  [G loss: 0.860075, acc: 0.531250]\n",
      "1092: [D loss: 0.528506, acc: 0.726562]  [G loss: 0.973541, acc: 0.312500]\n",
      "1093: [D loss: 0.373766, acc: 0.843750]  [G loss: 0.767183, acc: 0.484375]\n",
      "1094: [D loss: 0.392263, acc: 0.789062]  [G loss: 1.241319, acc: 0.218750]\n",
      "1095: [D loss: 0.373592, acc: 0.828125]  [G loss: 0.787984, acc: 0.421875]\n",
      "1096: [D loss: 0.232638, acc: 0.882812]  [G loss: 0.368530, acc: 0.828125]\n",
      "1097: [D loss: 0.540633, acc: 0.718750]  [G loss: 2.096019, acc: 0.000000]\n",
      "1098: [D loss: 0.363824, acc: 0.828125]  [G loss: 0.687657, acc: 0.531250]\n",
      "1099: [D loss: 0.288568, acc: 0.859375]  [G loss: 1.035108, acc: 0.328125]\n",
      "1100: [D loss: 0.383525, acc: 0.828125]  [G loss: 0.499974, acc: 0.718750]\n",
      "1101: [D loss: 0.551989, acc: 0.710938]  [G loss: 1.968795, acc: 0.000000]\n",
      "1102: [D loss: 0.375040, acc: 0.796875]  [G loss: 0.862403, acc: 0.453125]\n",
      "1103: [D loss: 0.392976, acc: 0.851562]  [G loss: 1.283940, acc: 0.187500]\n",
      "1104: [D loss: 0.467287, acc: 0.757812]  [G loss: 0.807790, acc: 0.468750]\n",
      "1105: [D loss: 0.351192, acc: 0.843750]  [G loss: 1.239262, acc: 0.265625]\n",
      "1106: [D loss: 0.346587, acc: 0.835938]  [G loss: 0.729801, acc: 0.531250]\n",
      "1107: [D loss: 0.416649, acc: 0.796875]  [G loss: 1.267041, acc: 0.171875]\n",
      "1108: [D loss: 0.407585, acc: 0.781250]  [G loss: 0.706661, acc: 0.562500]\n",
      "1109: [D loss: 0.201969, acc: 0.953125]  [G loss: 0.564071, acc: 0.687500]\n",
      "1110: [D loss: 0.536748, acc: 0.765625]  [G loss: 2.117742, acc: 0.000000]\n",
      "1111: [D loss: 0.424868, acc: 0.773438]  [G loss: 0.547569, acc: 0.750000]\n",
      "1112: [D loss: 0.435935, acc: 0.750000]  [G loss: 1.915626, acc: 0.031250]\n",
      "1113: [D loss: 0.304771, acc: 0.843750]  [G loss: 1.058772, acc: 0.312500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114: [D loss: 0.329616, acc: 0.843750]  [G loss: 1.053769, acc: 0.234375]\n",
      "1115: [D loss: 0.320998, acc: 0.843750]  [G loss: 1.547784, acc: 0.093750]\n",
      "1116: [D loss: 0.308455, acc: 0.875000]  [G loss: 0.779131, acc: 0.453125]\n",
      "1117: [D loss: 0.439484, acc: 0.781250]  [G loss: 2.430920, acc: 0.000000]\n",
      "1118: [D loss: 0.515221, acc: 0.734375]  [G loss: 0.655185, acc: 0.562500]\n",
      "1119: [D loss: 0.402005, acc: 0.789062]  [G loss: 1.871939, acc: 0.000000]\n",
      "1120: [D loss: 0.393333, acc: 0.789062]  [G loss: 0.708504, acc: 0.484375]\n",
      "1121: [D loss: 0.439137, acc: 0.804688]  [G loss: 1.184146, acc: 0.171875]\n",
      "1122: [D loss: 0.395894, acc: 0.851562]  [G loss: 0.951396, acc: 0.312500]\n",
      "1123: [D loss: 0.326239, acc: 0.851562]  [G loss: 1.041196, acc: 0.265625]\n",
      "1124: [D loss: 0.308673, acc: 0.851562]  [G loss: 1.370302, acc: 0.125000]\n",
      "1125: [D loss: 0.347425, acc: 0.859375]  [G loss: 0.667807, acc: 0.640625]\n",
      "1126: [D loss: 0.419518, acc: 0.804688]  [G loss: 2.194983, acc: 0.046875]\n",
      "1127: [D loss: 0.591145, acc: 0.687500]  [G loss: 0.274395, acc: 0.890625]\n",
      "1128: [D loss: 0.718876, acc: 0.648438]  [G loss: 1.021566, acc: 0.421875]\n",
      "1129: [D loss: 0.448445, acc: 0.765625]  [G loss: 0.624868, acc: 0.656250]\n",
      "1130: [D loss: 0.372028, acc: 0.820312]  [G loss: 0.834820, acc: 0.437500]\n",
      "1131: [D loss: 0.335074, acc: 0.843750]  [G loss: 0.714327, acc: 0.500000]\n",
      "1132: [D loss: 0.243423, acc: 0.914062]  [G loss: 0.549593, acc: 0.656250]\n",
      "1133: [D loss: 0.430438, acc: 0.781250]  [G loss: 0.696985, acc: 0.500000]\n",
      "1134: [D loss: 0.417478, acc: 0.804688]  [G loss: 0.734588, acc: 0.468750]\n",
      "1135: [D loss: 0.236234, acc: 0.921875]  [G loss: 0.633456, acc: 0.671875]\n",
      "1136: [D loss: 0.401824, acc: 0.804688]  [G loss: 0.937304, acc: 0.375000]\n",
      "1137: [D loss: 0.400440, acc: 0.843750]  [G loss: 1.379896, acc: 0.265625]\n",
      "1138: [D loss: 0.427954, acc: 0.812500]  [G loss: 1.072878, acc: 0.312500]\n",
      "1139: [D loss: 0.403986, acc: 0.789062]  [G loss: 1.195422, acc: 0.343750]\n",
      "1140: [D loss: 0.510519, acc: 0.718750]  [G loss: 0.698436, acc: 0.593750]\n",
      "1141: [D loss: 0.433808, acc: 0.750000]  [G loss: 0.995710, acc: 0.296875]\n",
      "1142: [D loss: 0.360090, acc: 0.828125]  [G loss: 0.290997, acc: 0.890625]\n",
      "1143: [D loss: 0.493474, acc: 0.750000]  [G loss: 1.757172, acc: 0.046875]\n",
      "1144: [D loss: 0.632314, acc: 0.695312]  [G loss: 0.467047, acc: 0.718750]\n",
      "1145: [D loss: 0.513308, acc: 0.742188]  [G loss: 1.166007, acc: 0.296875]\n",
      "1146: [D loss: 0.429905, acc: 0.812500]  [G loss: 0.598685, acc: 0.640625]\n",
      "1147: [D loss: 0.403325, acc: 0.773438]  [G loss: 0.894177, acc: 0.296875]\n",
      "1148: [D loss: 0.374039, acc: 0.843750]  [G loss: 1.062850, acc: 0.171875]\n",
      "1149: [D loss: 0.428594, acc: 0.789062]  [G loss: 0.542717, acc: 0.718750]\n",
      "1150: [D loss: 0.518414, acc: 0.710938]  [G loss: 1.325494, acc: 0.046875]\n",
      "1151: [D loss: 0.389938, acc: 0.796875]  [G loss: 0.548868, acc: 0.687500]\n",
      "1152: [D loss: 0.356316, acc: 0.812500]  [G loss: 0.771174, acc: 0.531250]\n",
      "1153: [D loss: 0.517409, acc: 0.773438]  [G loss: 0.827446, acc: 0.406250]\n",
      "1154: [D loss: 0.359605, acc: 0.859375]  [G loss: 0.762816, acc: 0.500000]\n",
      "1155: [D loss: 0.393088, acc: 0.835938]  [G loss: 0.890096, acc: 0.390625]\n",
      "1156: [D loss: 0.300252, acc: 0.835938]  [G loss: 0.489785, acc: 0.718750]\n",
      "1157: [D loss: 0.559748, acc: 0.687500]  [G loss: 1.506788, acc: 0.109375]\n",
      "1158: [D loss: 0.335778, acc: 0.812500]  [G loss: 0.748713, acc: 0.515625]\n",
      "1159: [D loss: 0.427882, acc: 0.781250]  [G loss: 1.536244, acc: 0.093750]\n",
      "1160: [D loss: 0.315054, acc: 0.882812]  [G loss: 0.520671, acc: 0.750000]\n",
      "1161: [D loss: 0.454756, acc: 0.796875]  [G loss: 0.955945, acc: 0.453125]\n",
      "1162: [D loss: 0.429179, acc: 0.789062]  [G loss: 0.453030, acc: 0.828125]\n",
      "1163: [D loss: 0.574028, acc: 0.726562]  [G loss: 1.291039, acc: 0.156250]\n",
      "1164: [D loss: 0.483475, acc: 0.773438]  [G loss: 0.449405, acc: 0.750000]\n",
      "1165: [D loss: 0.613289, acc: 0.734375]  [G loss: 0.923541, acc: 0.453125]\n",
      "1166: [D loss: 0.574937, acc: 0.703125]  [G loss: 0.639254, acc: 0.531250]\n",
      "1167: [D loss: 0.374178, acc: 0.812500]  [G loss: 0.655037, acc: 0.500000]\n",
      "1168: [D loss: 0.508618, acc: 0.726562]  [G loss: 0.643235, acc: 0.546875]\n",
      "1169: [D loss: 0.376453, acc: 0.820312]  [G loss: 0.437961, acc: 0.734375]\n",
      "1170: [D loss: 0.410441, acc: 0.812500]  [G loss: 0.514398, acc: 0.625000]\n",
      "1171: [D loss: 0.347286, acc: 0.867188]  [G loss: 0.435526, acc: 0.812500]\n",
      "1172: [D loss: 0.393833, acc: 0.796875]  [G loss: 1.144493, acc: 0.250000]\n",
      "1173: [D loss: 0.358963, acc: 0.835938]  [G loss: 0.218080, acc: 0.984375]\n",
      "1174: [D loss: 0.609301, acc: 0.726562]  [G loss: 1.154372, acc: 0.265625]\n",
      "1175: [D loss: 0.482535, acc: 0.734375]  [G loss: 0.753925, acc: 0.484375]\n",
      "1176: [D loss: 0.321635, acc: 0.875000]  [G loss: 0.988691, acc: 0.234375]\n",
      "1177: [D loss: 0.342401, acc: 0.890625]  [G loss: 0.909722, acc: 0.390625]\n",
      "1178: [D loss: 0.397440, acc: 0.828125]  [G loss: 1.513767, acc: 0.109375]\n",
      "1179: [D loss: 0.364535, acc: 0.835938]  [G loss: 0.643334, acc: 0.656250]\n",
      "1180: [D loss: 0.432681, acc: 0.828125]  [G loss: 1.044615, acc: 0.328125]\n",
      "1181: [D loss: 0.445834, acc: 0.789062]  [G loss: 1.238275, acc: 0.250000]\n",
      "1182: [D loss: 0.327073, acc: 0.851562]  [G loss: 0.794152, acc: 0.500000]\n",
      "1183: [D loss: 0.382322, acc: 0.796875]  [G loss: 1.105986, acc: 0.296875]\n",
      "1184: [D loss: 0.431269, acc: 0.773438]  [G loss: 0.614176, acc: 0.578125]\n",
      "1185: [D loss: 0.437130, acc: 0.781250]  [G loss: 2.148026, acc: 0.031250]\n",
      "1186: [D loss: 0.464424, acc: 0.765625]  [G loss: 0.436486, acc: 0.765625]\n",
      "1187: [D loss: 0.409638, acc: 0.804688]  [G loss: 1.296433, acc: 0.234375]\n",
      "1188: [D loss: 0.393678, acc: 0.820312]  [G loss: 0.684965, acc: 0.640625]\n",
      "1189: [D loss: 0.341695, acc: 0.859375]  [G loss: 0.845790, acc: 0.421875]\n",
      "1190: [D loss: 0.289689, acc: 0.890625]  [G loss: 1.080410, acc: 0.281250]\n",
      "1191: [D loss: 0.261952, acc: 0.890625]  [G loss: 0.677528, acc: 0.562500]\n",
      "1192: [D loss: 0.365058, acc: 0.773438]  [G loss: 2.366765, acc: 0.000000]\n",
      "1193: [D loss: 0.536647, acc: 0.726562]  [G loss: 0.346682, acc: 0.875000]\n",
      "1194: [D loss: 0.441520, acc: 0.773438]  [G loss: 1.493970, acc: 0.015625]\n",
      "1195: [D loss: 0.380016, acc: 0.812500]  [G loss: 0.849610, acc: 0.421875]\n",
      "1196: [D loss: 0.313700, acc: 0.882812]  [G loss: 1.475374, acc: 0.156250]\n",
      "1197: [D loss: 0.340585, acc: 0.851562]  [G loss: 1.235764, acc: 0.187500]\n",
      "1198: [D loss: 0.304412, acc: 0.875000]  [G loss: 1.061131, acc: 0.281250]\n",
      "1199: [D loss: 0.323041, acc: 0.890625]  [G loss: 1.076204, acc: 0.343750]\n",
      "1200: [D loss: 0.215042, acc: 0.921875]  [G loss: 0.685501, acc: 0.578125]\n",
      "1201: [D loss: 0.362023, acc: 0.781250]  [G loss: 2.339065, acc: 0.015625]\n",
      "1202: [D loss: 0.417836, acc: 0.765625]  [G loss: 0.380035, acc: 0.828125]\n",
      "1203: [D loss: 0.609403, acc: 0.703125]  [G loss: 2.029109, acc: 0.078125]\n",
      "1204: [D loss: 0.411920, acc: 0.804688]  [G loss: 0.646145, acc: 0.640625]\n",
      "1205: [D loss: 0.418320, acc: 0.781250]  [G loss: 0.920933, acc: 0.437500]\n",
      "1206: [D loss: 0.340363, acc: 0.835938]  [G loss: 0.777815, acc: 0.546875]\n",
      "1207: [D loss: 0.463857, acc: 0.796875]  [G loss: 0.845028, acc: 0.484375]\n",
      "1208: [D loss: 0.320692, acc: 0.882812]  [G loss: 1.653244, acc: 0.046875]\n",
      "1209: [D loss: 0.427456, acc: 0.828125]  [G loss: 0.696145, acc: 0.453125]\n",
      "1210: [D loss: 0.354495, acc: 0.835938]  [G loss: 1.655914, acc: 0.093750]\n",
      "1211: [D loss: 0.317594, acc: 0.835938]  [G loss: 0.646497, acc: 0.578125]\n",
      "1212: [D loss: 0.395619, acc: 0.781250]  [G loss: 1.417376, acc: 0.203125]\n",
      "1213: [D loss: 0.387888, acc: 0.812500]  [G loss: 0.660514, acc: 0.515625]\n",
      "1214: [D loss: 0.372446, acc: 0.851562]  [G loss: 1.854267, acc: 0.062500]\n",
      "1215: [D loss: 0.345374, acc: 0.828125]  [G loss: 0.850749, acc: 0.406250]\n",
      "1216: [D loss: 0.299801, acc: 0.898438]  [G loss: 1.167311, acc: 0.234375]\n",
      "1217: [D loss: 0.310817, acc: 0.882812]  [G loss: 0.822404, acc: 0.515625]\n",
      "1218: [D loss: 0.473831, acc: 0.773438]  [G loss: 1.142209, acc: 0.296875]\n",
      "1219: [D loss: 0.265355, acc: 0.921875]  [G loss: 1.137500, acc: 0.281250]\n",
      "1220: [D loss: 0.332693, acc: 0.843750]  [G loss: 0.575387, acc: 0.593750]\n",
      "1221: [D loss: 0.320176, acc: 0.828125]  [G loss: 2.470082, acc: 0.062500]\n",
      "1222: [D loss: 0.445683, acc: 0.796875]  [G loss: 0.465807, acc: 0.796875]\n",
      "1223: [D loss: 0.467565, acc: 0.742188]  [G loss: 2.529753, acc: 0.000000]\n",
      "1224: [D loss: 0.487022, acc: 0.796875]  [G loss: 0.663436, acc: 0.531250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225: [D loss: 0.434723, acc: 0.820312]  [G loss: 1.878247, acc: 0.125000]\n",
      "1226: [D loss: 0.417198, acc: 0.820312]  [G loss: 0.745614, acc: 0.546875]\n",
      "1227: [D loss: 0.394004, acc: 0.851562]  [G loss: 1.637929, acc: 0.093750]\n",
      "1228: [D loss: 0.387086, acc: 0.835938]  [G loss: 0.709065, acc: 0.531250]\n",
      "1229: [D loss: 0.451950, acc: 0.773438]  [G loss: 1.064738, acc: 0.390625]\n",
      "1230: [D loss: 0.386291, acc: 0.843750]  [G loss: 0.891233, acc: 0.421875]\n",
      "1231: [D loss: 0.356744, acc: 0.828125]  [G loss: 0.695963, acc: 0.531250]\n",
      "1232: [D loss: 0.329123, acc: 0.851562]  [G loss: 1.344721, acc: 0.203125]\n",
      "1233: [D loss: 0.397974, acc: 0.781250]  [G loss: 0.387026, acc: 0.828125]\n",
      "1234: [D loss: 0.404793, acc: 0.828125]  [G loss: 1.355180, acc: 0.250000]\n",
      "1235: [D loss: 0.231588, acc: 0.914062]  [G loss: 1.173547, acc: 0.328125]\n",
      "1236: [D loss: 0.339887, acc: 0.859375]  [G loss: 0.761394, acc: 0.468750]\n",
      "1237: [D loss: 0.376609, acc: 0.859375]  [G loss: 1.582219, acc: 0.171875]\n",
      "1238: [D loss: 0.261305, acc: 0.875000]  [G loss: 0.919447, acc: 0.453125]\n",
      "1239: [D loss: 0.353909, acc: 0.890625]  [G loss: 2.230177, acc: 0.062500]\n",
      "1240: [D loss: 0.252817, acc: 0.906250]  [G loss: 0.999093, acc: 0.484375]\n",
      "1241: [D loss: 0.328157, acc: 0.843750]  [G loss: 1.129595, acc: 0.343750]\n",
      "1242: [D loss: 0.294823, acc: 0.859375]  [G loss: 1.415083, acc: 0.250000]\n",
      "1243: [D loss: 0.200454, acc: 0.929688]  [G loss: 1.400050, acc: 0.265625]\n",
      "1244: [D loss: 0.312676, acc: 0.875000]  [G loss: 1.351158, acc: 0.265625]\n",
      "1245: [D loss: 0.425908, acc: 0.804688]  [G loss: 0.298274, acc: 0.843750]\n",
      "1246: [D loss: 0.627888, acc: 0.671875]  [G loss: 2.885456, acc: 0.000000]\n",
      "1247: [D loss: 0.479575, acc: 0.789062]  [G loss: 0.371398, acc: 0.843750]\n",
      "1248: [D loss: 0.415576, acc: 0.773438]  [G loss: 1.903551, acc: 0.062500]\n",
      "1249: [D loss: 0.380834, acc: 0.812500]  [G loss: 0.932395, acc: 0.390625]\n",
      "1250: [D loss: 0.339214, acc: 0.851562]  [G loss: 1.082886, acc: 0.281250]\n",
      "1251: [D loss: 0.344931, acc: 0.843750]  [G loss: 0.876328, acc: 0.421875]\n",
      "1252: [D loss: 0.305213, acc: 0.843750]  [G loss: 1.007922, acc: 0.343750]\n",
      "1253: [D loss: 0.291065, acc: 0.875000]  [G loss: 0.596904, acc: 0.671875]\n",
      "1254: [D loss: 0.328344, acc: 0.835938]  [G loss: 1.512976, acc: 0.203125]\n",
      "1255: [D loss: 0.396137, acc: 0.789062]  [G loss: 0.403856, acc: 0.812500]\n",
      "1256: [D loss: 0.315563, acc: 0.843750]  [G loss: 1.213810, acc: 0.234375]\n",
      "1257: [D loss: 0.344474, acc: 0.851562]  [G loss: 1.033178, acc: 0.421875]\n",
      "1258: [D loss: 0.365913, acc: 0.851562]  [G loss: 0.482163, acc: 0.765625]\n",
      "1259: [D loss: 0.384365, acc: 0.835938]  [G loss: 1.897829, acc: 0.031250]\n",
      "1260: [D loss: 0.424053, acc: 0.812500]  [G loss: 0.312658, acc: 0.875000]\n",
      "1261: [D loss: 0.365679, acc: 0.820312]  [G loss: 1.202737, acc: 0.203125]\n",
      "1262: [D loss: 0.252061, acc: 0.906250]  [G loss: 0.913899, acc: 0.453125]\n",
      "1263: [D loss: 0.228204, acc: 0.914062]  [G loss: 0.500101, acc: 0.750000]\n",
      "1264: [D loss: 0.366217, acc: 0.843750]  [G loss: 1.604507, acc: 0.156250]\n",
      "1265: [D loss: 0.279396, acc: 0.835938]  [G loss: 1.113830, acc: 0.187500]\n",
      "1266: [D loss: 0.269621, acc: 0.914062]  [G loss: 0.850888, acc: 0.375000]\n",
      "1267: [D loss: 0.259805, acc: 0.875000]  [G loss: 1.275158, acc: 0.140625]\n",
      "1268: [D loss: 0.276998, acc: 0.890625]  [G loss: 0.826873, acc: 0.484375]\n",
      "1269: [D loss: 0.266848, acc: 0.890625]  [G loss: 0.969790, acc: 0.390625]\n",
      "1270: [D loss: 0.338433, acc: 0.843750]  [G loss: 1.071805, acc: 0.296875]\n",
      "1271: [D loss: 0.276762, acc: 0.882812]  [G loss: 0.470787, acc: 0.734375]\n",
      "1272: [D loss: 0.343342, acc: 0.867188]  [G loss: 1.665337, acc: 0.250000]\n",
      "1273: [D loss: 0.461142, acc: 0.765625]  [G loss: 0.078697, acc: 1.000000]\n",
      "1274: [D loss: 0.556675, acc: 0.710938]  [G loss: 1.622198, acc: 0.187500]\n",
      "1275: [D loss: 0.287514, acc: 0.867188]  [G loss: 0.774991, acc: 0.531250]\n",
      "1276: [D loss: 0.304856, acc: 0.843750]  [G loss: 1.040949, acc: 0.406250]\n",
      "1277: [D loss: 0.327663, acc: 0.875000]  [G loss: 0.930344, acc: 0.296875]\n",
      "1278: [D loss: 0.331127, acc: 0.851562]  [G loss: 1.605906, acc: 0.078125]\n",
      "1279: [D loss: 0.249384, acc: 0.875000]  [G loss: 0.841714, acc: 0.484375]\n",
      "1280: [D loss: 0.331351, acc: 0.867188]  [G loss: 1.226693, acc: 0.218750]\n",
      "1281: [D loss: 0.276348, acc: 0.898438]  [G loss: 0.472971, acc: 0.796875]\n",
      "1282: [D loss: 0.363782, acc: 0.812500]  [G loss: 1.859239, acc: 0.140625]\n",
      "1283: [D loss: 0.343707, acc: 0.835938]  [G loss: 0.498505, acc: 0.687500]\n",
      "1284: [D loss: 0.497703, acc: 0.710938]  [G loss: 2.327503, acc: 0.000000]\n",
      "1285: [D loss: 0.571846, acc: 0.750000]  [G loss: 0.532923, acc: 0.687500]\n",
      "1286: [D loss: 0.457178, acc: 0.750000]  [G loss: 1.344947, acc: 0.187500]\n",
      "1287: [D loss: 0.364749, acc: 0.859375]  [G loss: 0.908453, acc: 0.437500]\n",
      "1288: [D loss: 0.369018, acc: 0.835938]  [G loss: 1.072034, acc: 0.375000]\n",
      "1289: [D loss: 0.318627, acc: 0.859375]  [G loss: 0.999513, acc: 0.296875]\n",
      "1290: [D loss: 0.296471, acc: 0.890625]  [G loss: 1.344463, acc: 0.265625]\n",
      "1291: [D loss: 0.244639, acc: 0.890625]  [G loss: 1.147461, acc: 0.421875]\n",
      "1292: [D loss: 0.346027, acc: 0.875000]  [G loss: 1.308021, acc: 0.265625]\n",
      "1293: [D loss: 0.341387, acc: 0.820312]  [G loss: 2.301126, acc: 0.062500]\n",
      "1294: [D loss: 0.402034, acc: 0.820312]  [G loss: 0.683294, acc: 0.625000]\n",
      "1295: [D loss: 0.389267, acc: 0.789062]  [G loss: 2.004465, acc: 0.125000]\n",
      "1296: [D loss: 0.461218, acc: 0.796875]  [G loss: 0.570971, acc: 0.656250]\n",
      "1297: [D loss: 0.375259, acc: 0.781250]  [G loss: 1.157498, acc: 0.296875]\n",
      "1298: [D loss: 0.341032, acc: 0.843750]  [G loss: 1.143355, acc: 0.281250]\n",
      "1299: [D loss: 0.472864, acc: 0.765625]  [G loss: 1.153033, acc: 0.390625]\n",
      "1300: [D loss: 0.479263, acc: 0.765625]  [G loss: 1.010231, acc: 0.328125]\n",
      "1301: [D loss: 0.447238, acc: 0.781250]  [G loss: 1.186623, acc: 0.250000]\n",
      "1302: [D loss: 0.370604, acc: 0.820312]  [G loss: 1.023255, acc: 0.437500]\n",
      "1303: [D loss: 0.479779, acc: 0.734375]  [G loss: 2.230309, acc: 0.046875]\n",
      "1304: [D loss: 0.442945, acc: 0.804688]  [G loss: 0.522233, acc: 0.718750]\n",
      "1305: [D loss: 0.617866, acc: 0.710938]  [G loss: 1.557800, acc: 0.093750]\n",
      "1306: [D loss: 0.474933, acc: 0.781250]  [G loss: 0.787410, acc: 0.515625]\n",
      "1307: [D loss: 0.386390, acc: 0.835938]  [G loss: 0.755437, acc: 0.468750]\n",
      "1308: [D loss: 0.349600, acc: 0.843750]  [G loss: 0.743225, acc: 0.500000]\n",
      "1309: [D loss: 0.306007, acc: 0.851562]  [G loss: 0.888407, acc: 0.375000]\n",
      "1310: [D loss: 0.357967, acc: 0.789062]  [G loss: 1.058099, acc: 0.328125]\n",
      "1311: [D loss: 0.445001, acc: 0.796875]  [G loss: 0.607245, acc: 0.609375]\n",
      "1312: [D loss: 0.461337, acc: 0.781250]  [G loss: 1.644030, acc: 0.046875]\n",
      "1313: [D loss: 0.418581, acc: 0.804688]  [G loss: 0.361185, acc: 0.828125]\n",
      "1314: [D loss: 0.617295, acc: 0.687500]  [G loss: 2.165761, acc: 0.015625]\n",
      "1315: [D loss: 0.608771, acc: 0.710938]  [G loss: 0.528579, acc: 0.671875]\n",
      "1316: [D loss: 0.248387, acc: 0.898438]  [G loss: 0.497590, acc: 0.671875]\n",
      "1317: [D loss: 0.287493, acc: 0.898438]  [G loss: 0.349258, acc: 0.843750]\n",
      "1318: [D loss: 0.324688, acc: 0.812500]  [G loss: 0.757240, acc: 0.468750]\n",
      "1319: [D loss: 0.286873, acc: 0.906250]  [G loss: 0.616710, acc: 0.625000]\n",
      "1320: [D loss: 0.241820, acc: 0.914062]  [G loss: 0.401235, acc: 0.812500]\n",
      "1321: [D loss: 0.339472, acc: 0.804688]  [G loss: 1.080936, acc: 0.218750]\n",
      "1322: [D loss: 0.260520, acc: 0.914062]  [G loss: 0.418465, acc: 0.765625]\n",
      "1323: [D loss: 0.678284, acc: 0.632812]  [G loss: 2.276555, acc: 0.015625]\n",
      "1324: [D loss: 0.694507, acc: 0.664062]  [G loss: 0.664312, acc: 0.515625]\n",
      "1325: [D loss: 0.468629, acc: 0.765625]  [G loss: 2.065769, acc: 0.015625]\n",
      "1326: [D loss: 0.383110, acc: 0.796875]  [G loss: 1.403280, acc: 0.187500]\n",
      "1327: [D loss: 0.353319, acc: 0.828125]  [G loss: 1.973769, acc: 0.000000]\n",
      "1328: [D loss: 0.319742, acc: 0.859375]  [G loss: 0.936565, acc: 0.312500]\n",
      "1329: [D loss: 0.404978, acc: 0.796875]  [G loss: 1.723660, acc: 0.078125]\n",
      "1330: [D loss: 0.332250, acc: 0.890625]  [G loss: 0.780434, acc: 0.468750]\n",
      "1331: [D loss: 0.535269, acc: 0.773438]  [G loss: 1.567237, acc: 0.140625]\n",
      "1332: [D loss: 0.448092, acc: 0.804688]  [G loss: 0.836798, acc: 0.500000]\n",
      "1333: [D loss: 0.372344, acc: 0.835938]  [G loss: 0.781882, acc: 0.500000]\n",
      "1334: [D loss: 0.462438, acc: 0.765625]  [G loss: 1.901303, acc: 0.093750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1335: [D loss: 0.519934, acc: 0.742188]  [G loss: 0.485115, acc: 0.718750]\n",
      "1336: [D loss: 0.464147, acc: 0.851562]  [G loss: 0.873652, acc: 0.421875]\n",
      "1337: [D loss: 0.458779, acc: 0.796875]  [G loss: 0.877357, acc: 0.500000]\n",
      "1338: [D loss: 0.380781, acc: 0.812500]  [G loss: 0.669820, acc: 0.625000]\n",
      "1339: [D loss: 0.482500, acc: 0.781250]  [G loss: 1.059300, acc: 0.281250]\n",
      "1340: [D loss: 0.386178, acc: 0.804688]  [G loss: 0.511191, acc: 0.687500]\n",
      "1341: [D loss: 0.506892, acc: 0.703125]  [G loss: 1.512776, acc: 0.156250]\n",
      "1342: [D loss: 0.444274, acc: 0.742188]  [G loss: 0.285560, acc: 0.875000]\n",
      "1343: [D loss: 0.438324, acc: 0.773438]  [G loss: 1.081554, acc: 0.328125]\n",
      "1344: [D loss: 0.447513, acc: 0.765625]  [G loss: 0.985574, acc: 0.250000]\n",
      "1345: [D loss: 0.449501, acc: 0.773438]  [G loss: 0.562736, acc: 0.671875]\n",
      "1346: [D loss: 0.534364, acc: 0.742188]  [G loss: 0.857449, acc: 0.390625]\n",
      "1347: [D loss: 0.431106, acc: 0.796875]  [G loss: 0.561190, acc: 0.656250]\n",
      "1348: [D loss: 0.494923, acc: 0.750000]  [G loss: 1.320268, acc: 0.250000]\n",
      "1349: [D loss: 0.537553, acc: 0.726562]  [G loss: 1.032336, acc: 0.203125]\n",
      "1350: [D loss: 0.362625, acc: 0.843750]  [G loss: 0.989999, acc: 0.375000]\n",
      "1351: [D loss: 0.470938, acc: 0.796875]  [G loss: 1.364392, acc: 0.140625]\n",
      "1352: [D loss: 0.429832, acc: 0.789062]  [G loss: 0.615610, acc: 0.609375]\n",
      "1353: [D loss: 0.587794, acc: 0.703125]  [G loss: 1.656502, acc: 0.046875]\n",
      "1354: [D loss: 0.576515, acc: 0.695312]  [G loss: 0.467209, acc: 0.781250]\n",
      "1355: [D loss: 0.441367, acc: 0.789062]  [G loss: 1.239761, acc: 0.187500]\n",
      "1356: [D loss: 0.511339, acc: 0.765625]  [G loss: 0.701158, acc: 0.562500]\n",
      "1357: [D loss: 0.471695, acc: 0.773438]  [G loss: 0.892283, acc: 0.265625]\n",
      "1358: [D loss: 0.387890, acc: 0.820312]  [G loss: 0.884666, acc: 0.359375]\n",
      "1359: [D loss: 0.466234, acc: 0.773438]  [G loss: 0.867526, acc: 0.296875]\n",
      "1360: [D loss: 0.387370, acc: 0.851562]  [G loss: 0.702854, acc: 0.593750]\n",
      "1361: [D loss: 0.480479, acc: 0.796875]  [G loss: 0.564459, acc: 0.703125]\n",
      "1362: [D loss: 0.530345, acc: 0.750000]  [G loss: 1.898525, acc: 0.031250]\n",
      "1363: [D loss: 0.446865, acc: 0.781250]  [G loss: 0.448472, acc: 0.796875]\n",
      "1364: [D loss: 0.451743, acc: 0.765625]  [G loss: 2.070735, acc: 0.000000]\n",
      "1365: [D loss: 0.491930, acc: 0.757812]  [G loss: 0.722803, acc: 0.531250]\n",
      "1366: [D loss: 0.525643, acc: 0.750000]  [G loss: 1.483517, acc: 0.140625]\n",
      "1367: [D loss: 0.394023, acc: 0.820312]  [G loss: 0.948112, acc: 0.359375]\n",
      "1368: [D loss: 0.428645, acc: 0.796875]  [G loss: 1.747371, acc: 0.078125]\n",
      "1369: [D loss: 0.398733, acc: 0.835938]  [G loss: 0.941697, acc: 0.343750]\n",
      "1370: [D loss: 0.251429, acc: 0.921875]  [G loss: 0.985617, acc: 0.328125]\n",
      "1371: [D loss: 0.389100, acc: 0.835938]  [G loss: 1.542742, acc: 0.046875]\n",
      "1372: [D loss: 0.616952, acc: 0.648438]  [G loss: 1.037496, acc: 0.281250]\n",
      "1373: [D loss: 0.333247, acc: 0.890625]  [G loss: 0.884224, acc: 0.468750]\n",
      "1374: [D loss: 0.428715, acc: 0.765625]  [G loss: 0.704448, acc: 0.468750]\n",
      "1375: [D loss: 0.404630, acc: 0.875000]  [G loss: 1.284763, acc: 0.187500]\n",
      "1376: [D loss: 0.473979, acc: 0.781250]  [G loss: 0.883469, acc: 0.421875]\n",
      "1377: [D loss: 0.467230, acc: 0.781250]  [G loss: 1.166267, acc: 0.296875]\n",
      "1378: [D loss: 0.454513, acc: 0.781250]  [G loss: 0.644757, acc: 0.656250]\n",
      "1379: [D loss: 0.437297, acc: 0.804688]  [G loss: 2.122502, acc: 0.015625]\n",
      "1380: [D loss: 0.572394, acc: 0.726562]  [G loss: 0.502719, acc: 0.750000]\n",
      "1381: [D loss: 0.473791, acc: 0.773438]  [G loss: 2.227991, acc: 0.015625]\n",
      "1382: [D loss: 0.455341, acc: 0.765625]  [G loss: 0.933891, acc: 0.390625]\n",
      "1383: [D loss: 0.465163, acc: 0.820312]  [G loss: 1.665382, acc: 0.000000]\n",
      "1384: [D loss: 0.407359, acc: 0.812500]  [G loss: 0.888835, acc: 0.312500]\n",
      "1385: [D loss: 0.371013, acc: 0.835938]  [G loss: 0.985117, acc: 0.250000]\n",
      "1386: [D loss: 0.308679, acc: 0.843750]  [G loss: 1.161166, acc: 0.234375]\n",
      "1387: [D loss: 0.354066, acc: 0.851562]  [G loss: 1.030152, acc: 0.328125]\n",
      "1388: [D loss: 0.317984, acc: 0.882812]  [G loss: 0.947168, acc: 0.421875]\n",
      "1389: [D loss: 0.420589, acc: 0.789062]  [G loss: 1.511499, acc: 0.093750]\n",
      "1390: [D loss: 0.413133, acc: 0.796875]  [G loss: 0.707063, acc: 0.609375]\n",
      "1391: [D loss: 0.433447, acc: 0.757812]  [G loss: 1.553492, acc: 0.203125]\n",
      "1392: [D loss: 0.424097, acc: 0.789062]  [G loss: 0.548120, acc: 0.687500]\n",
      "1393: [D loss: 0.441524, acc: 0.835938]  [G loss: 1.238392, acc: 0.218750]\n",
      "1394: [D loss: 0.474892, acc: 0.750000]  [G loss: 0.989821, acc: 0.296875]\n",
      "1395: [D loss: 0.496265, acc: 0.750000]  [G loss: 0.780224, acc: 0.500000]\n",
      "1396: [D loss: 0.410663, acc: 0.828125]  [G loss: 1.056410, acc: 0.312500]\n",
      "1397: [D loss: 0.395876, acc: 0.867188]  [G loss: 0.661383, acc: 0.500000]\n",
      "1398: [D loss: 0.445981, acc: 0.812500]  [G loss: 1.229385, acc: 0.218750]\n",
      "1399: [D loss: 0.465947, acc: 0.757812]  [G loss: 0.606673, acc: 0.609375]\n",
      "1400: [D loss: 0.422435, acc: 0.796875]  [G loss: 1.706353, acc: 0.093750]\n",
      "1401: [D loss: 0.507413, acc: 0.750000]  [G loss: 0.492744, acc: 0.718750]\n",
      "1402: [D loss: 0.404934, acc: 0.812500]  [G loss: 1.024410, acc: 0.437500]\n",
      "1403: [D loss: 0.469389, acc: 0.726562]  [G loss: 0.701427, acc: 0.531250]\n",
      "1404: [D loss: 0.388605, acc: 0.835938]  [G loss: 1.096679, acc: 0.359375]\n",
      "1405: [D loss: 0.426929, acc: 0.781250]  [G loss: 0.800850, acc: 0.453125]\n",
      "1406: [D loss: 0.362863, acc: 0.875000]  [G loss: 0.516652, acc: 0.640625]\n",
      "1407: [D loss: 0.435172, acc: 0.781250]  [G loss: 2.069438, acc: 0.109375]\n",
      "1408: [D loss: 0.467876, acc: 0.742188]  [G loss: 0.471362, acc: 0.750000]\n",
      "1409: [D loss: 0.587137, acc: 0.703125]  [G loss: 2.142995, acc: 0.078125]\n",
      "1410: [D loss: 0.334015, acc: 0.828125]  [G loss: 1.193513, acc: 0.312500]\n",
      "1411: [D loss: 0.302191, acc: 0.890625]  [G loss: 1.101124, acc: 0.250000]\n",
      "1412: [D loss: 0.299556, acc: 0.875000]  [G loss: 0.861991, acc: 0.343750]\n",
      "1413: [D loss: 0.279337, acc: 0.890625]  [G loss: 0.845293, acc: 0.421875]\n",
      "1414: [D loss: 0.340640, acc: 0.859375]  [G loss: 1.140768, acc: 0.359375]\n",
      "1415: [D loss: 0.377980, acc: 0.835938]  [G loss: 1.640844, acc: 0.187500]\n",
      "1416: [D loss: 0.429256, acc: 0.757812]  [G loss: 0.405075, acc: 0.828125]\n",
      "1417: [D loss: 0.384481, acc: 0.750000]  [G loss: 2.069933, acc: 0.078125]\n",
      "1418: [D loss: 0.512614, acc: 0.734375]  [G loss: 0.404775, acc: 0.828125]\n",
      "1419: [D loss: 0.705305, acc: 0.562500]  [G loss: 1.884312, acc: 0.046875]\n",
      "1420: [D loss: 0.383899, acc: 0.835938]  [G loss: 1.046903, acc: 0.343750]\n",
      "1421: [D loss: 0.360424, acc: 0.812500]  [G loss: 1.380701, acc: 0.234375]\n",
      "1422: [D loss: 0.325313, acc: 0.859375]  [G loss: 1.518636, acc: 0.156250]\n",
      "1423: [D loss: 0.330397, acc: 0.835938]  [G loss: 1.028016, acc: 0.421875]\n",
      "1424: [D loss: 0.321624, acc: 0.859375]  [G loss: 1.914360, acc: 0.031250]\n",
      "1425: [D loss: 0.298883, acc: 0.851562]  [G loss: 1.071800, acc: 0.359375]\n",
      "1426: [D loss: 0.214113, acc: 0.890625]  [G loss: 1.258276, acc: 0.312500]\n",
      "1427: [D loss: 0.277254, acc: 0.882812]  [G loss: 1.814435, acc: 0.203125]\n",
      "1428: [D loss: 0.215700, acc: 0.937500]  [G loss: 1.367520, acc: 0.265625]\n",
      "1429: [D loss: 0.215829, acc: 0.929688]  [G loss: 1.175122, acc: 0.343750]\n",
      "1430: [D loss: 0.277434, acc: 0.890625]  [G loss: 2.773128, acc: 0.015625]\n",
      "1431: [D loss: 0.359411, acc: 0.843750]  [G loss: 0.738072, acc: 0.593750]\n",
      "1432: [D loss: 0.407170, acc: 0.789062]  [G loss: 3.270310, acc: 0.046875]\n",
      "1433: [D loss: 0.622214, acc: 0.742188]  [G loss: 0.517246, acc: 0.718750]\n",
      "1434: [D loss: 0.500924, acc: 0.750000]  [G loss: 2.059631, acc: 0.140625]\n",
      "1435: [D loss: 0.351851, acc: 0.828125]  [G loss: 0.931612, acc: 0.515625]\n",
      "1436: [D loss: 0.267412, acc: 0.898438]  [G loss: 0.892409, acc: 0.468750]\n",
      "1437: [D loss: 0.349212, acc: 0.812500]  [G loss: 1.053030, acc: 0.390625]\n",
      "1438: [D loss: 0.301451, acc: 0.851562]  [G loss: 1.009241, acc: 0.437500]\n",
      "1439: [D loss: 0.352602, acc: 0.796875]  [G loss: 0.766150, acc: 0.546875]\n",
      "1440: [D loss: 0.374654, acc: 0.781250]  [G loss: 1.547203, acc: 0.171875]\n",
      "1441: [D loss: 0.306035, acc: 0.843750]  [G loss: 0.554767, acc: 0.671875]\n",
      "1442: [D loss: 0.411304, acc: 0.789062]  [G loss: 1.663714, acc: 0.203125]\n",
      "1443: [D loss: 0.369415, acc: 0.812500]  [G loss: 0.640787, acc: 0.625000]\n",
      "1444: [D loss: 0.372335, acc: 0.812500]  [G loss: 1.352945, acc: 0.203125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445: [D loss: 0.385950, acc: 0.820312]  [G loss: 0.826414, acc: 0.531250]\n",
      "1446: [D loss: 0.287764, acc: 0.875000]  [G loss: 0.670240, acc: 0.546875]\n",
      "1447: [D loss: 0.359035, acc: 0.851562]  [G loss: 0.745492, acc: 0.640625]\n",
      "1448: [D loss: 0.331297, acc: 0.843750]  [G loss: 1.562763, acc: 0.171875]\n",
      "1449: [D loss: 0.312578, acc: 0.851562]  [G loss: 0.836155, acc: 0.562500]\n",
      "1450: [D loss: 0.446230, acc: 0.796875]  [G loss: 1.698958, acc: 0.109375]\n",
      "1451: [D loss: 0.363248, acc: 0.843750]  [G loss: 0.460229, acc: 0.765625]\n",
      "1452: [D loss: 0.599167, acc: 0.734375]  [G loss: 1.758678, acc: 0.093750]\n",
      "1453: [D loss: 0.414553, acc: 0.789062]  [G loss: 0.394427, acc: 0.812500]\n",
      "1454: [D loss: 0.475815, acc: 0.718750]  [G loss: 1.541682, acc: 0.171875]\n",
      "1455: [D loss: 0.477085, acc: 0.796875]  [G loss: 0.508690, acc: 0.734375]\n",
      "1456: [D loss: 0.617589, acc: 0.718750]  [G loss: 1.277516, acc: 0.359375]\n",
      "1457: [D loss: 0.354399, acc: 0.820312]  [G loss: 0.807246, acc: 0.515625]\n",
      "1458: [D loss: 0.411672, acc: 0.828125]  [G loss: 1.071953, acc: 0.218750]\n",
      "1459: [D loss: 0.406857, acc: 0.867188]  [G loss: 0.899228, acc: 0.359375]\n",
      "1460: [D loss: 0.306469, acc: 0.890625]  [G loss: 0.982294, acc: 0.281250]\n",
      "1461: [D loss: 0.418372, acc: 0.796875]  [G loss: 0.979784, acc: 0.375000]\n",
      "1462: [D loss: 0.353714, acc: 0.828125]  [G loss: 0.467075, acc: 0.718750]\n",
      "1463: [D loss: 0.386446, acc: 0.820312]  [G loss: 1.583682, acc: 0.218750]\n",
      "1464: [D loss: 0.412512, acc: 0.789062]  [G loss: 0.334905, acc: 0.875000]\n",
      "1465: [D loss: 0.460394, acc: 0.765625]  [G loss: 1.653725, acc: 0.078125]\n",
      "1466: [D loss: 0.391213, acc: 0.843750]  [G loss: 0.976340, acc: 0.328125]\n",
      "1467: [D loss: 0.265952, acc: 0.929688]  [G loss: 1.242627, acc: 0.250000]\n",
      "1468: [D loss: 0.344046, acc: 0.851562]  [G loss: 1.382001, acc: 0.156250]\n",
      "1469: [D loss: 0.299373, acc: 0.859375]  [G loss: 0.856538, acc: 0.421875]\n",
      "1470: [D loss: 0.315116, acc: 0.898438]  [G loss: 1.242179, acc: 0.187500]\n",
      "1471: [D loss: 0.371145, acc: 0.820312]  [G loss: 0.886465, acc: 0.484375]\n",
      "1472: [D loss: 0.344345, acc: 0.859375]  [G loss: 1.314357, acc: 0.109375]\n",
      "1473: [D loss: 0.341813, acc: 0.835938]  [G loss: 0.407690, acc: 0.796875]\n",
      "1474: [D loss: 0.464528, acc: 0.781250]  [G loss: 1.473487, acc: 0.171875]\n",
      "1475: [D loss: 0.294931, acc: 0.906250]  [G loss: 0.846681, acc: 0.406250]\n",
      "1476: [D loss: 0.481630, acc: 0.734375]  [G loss: 2.079994, acc: 0.046875]\n",
      "1477: [D loss: 0.476885, acc: 0.773438]  [G loss: 0.379499, acc: 0.796875]\n",
      "1478: [D loss: 0.544922, acc: 0.742188]  [G loss: 2.152019, acc: 0.015625]\n",
      "1479: [D loss: 0.467005, acc: 0.773438]  [G loss: 1.056640, acc: 0.328125]\n",
      "1480: [D loss: 0.370163, acc: 0.843750]  [G loss: 1.335735, acc: 0.125000]\n",
      "1481: [D loss: 0.306604, acc: 0.882812]  [G loss: 0.914813, acc: 0.437500]\n",
      "1482: [D loss: 0.301615, acc: 0.875000]  [G loss: 1.362322, acc: 0.234375]\n",
      "1483: [D loss: 0.326671, acc: 0.843750]  [G loss: 0.707970, acc: 0.531250]\n",
      "1484: [D loss: 0.419023, acc: 0.828125]  [G loss: 1.293455, acc: 0.234375]\n",
      "1485: [D loss: 0.321328, acc: 0.867188]  [G loss: 0.559031, acc: 0.703125]\n",
      "1486: [D loss: 0.307255, acc: 0.898438]  [G loss: 0.872140, acc: 0.546875]\n",
      "1487: [D loss: 0.271169, acc: 0.882812]  [G loss: 0.718352, acc: 0.593750]\n",
      "1488: [D loss: 0.337193, acc: 0.835938]  [G loss: 1.264117, acc: 0.437500]\n",
      "1489: [D loss: 0.218767, acc: 0.882812]  [G loss: 0.375151, acc: 0.796875]\n",
      "1490: [D loss: 0.476579, acc: 0.750000]  [G loss: 2.278568, acc: 0.125000]\n",
      "1491: [D loss: 0.307509, acc: 0.820312]  [G loss: 0.790610, acc: 0.562500]\n",
      "1492: [D loss: 0.196287, acc: 0.921875]  [G loss: 0.873520, acc: 0.437500]\n",
      "1493: [D loss: 0.355484, acc: 0.828125]  [G loss: 1.074608, acc: 0.343750]\n",
      "1494: [D loss: 0.334702, acc: 0.875000]  [G loss: 1.066246, acc: 0.343750]\n",
      "1495: [D loss: 0.283325, acc: 0.898438]  [G loss: 1.693264, acc: 0.171875]\n",
      "1496: [D loss: 0.268020, acc: 0.898438]  [G loss: 0.737048, acc: 0.546875]\n",
      "1497: [D loss: 0.266897, acc: 0.890625]  [G loss: 1.226180, acc: 0.296875]\n",
      "1498: [D loss: 0.349608, acc: 0.843750]  [G loss: 1.755846, acc: 0.218750]\n",
      "1499: [D loss: 0.283996, acc: 0.898438]  [G loss: 0.403336, acc: 0.734375]\n",
      "1500: [D loss: 0.469697, acc: 0.718750]  [G loss: 3.819928, acc: 0.000000]\n",
      "1501: [D loss: 0.558847, acc: 0.765625]  [G loss: 0.856259, acc: 0.406250]\n",
      "1502: [D loss: 0.837148, acc: 0.773438]  [G loss: 2.237519, acc: 0.125000]\n",
      "1503: [D loss: 0.341061, acc: 0.851562]  [G loss: 0.743689, acc: 0.546875]\n",
      "1504: [D loss: 0.447734, acc: 0.765625]  [G loss: 2.181162, acc: 0.078125]\n",
      "1505: [D loss: 0.340208, acc: 0.867188]  [G loss: 0.724065, acc: 0.531250]\n",
      "1506: [D loss: 0.208347, acc: 0.929688]  [G loss: 0.759175, acc: 0.593750]\n",
      "1507: [D loss: 0.806882, acc: 0.671875]  [G loss: 2.832884, acc: 0.000000]\n",
      "1508: [D loss: 0.357389, acc: 0.851562]  [G loss: 1.298956, acc: 0.281250]\n",
      "1509: [D loss: 0.457864, acc: 0.726562]  [G loss: 1.511531, acc: 0.250000]\n",
      "1510: [D loss: 0.346631, acc: 0.843750]  [G loss: 0.930040, acc: 0.421875]\n",
      "1511: [D loss: 0.459090, acc: 0.757812]  [G loss: 1.332316, acc: 0.171875]\n",
      "1512: [D loss: 0.291924, acc: 0.882812]  [G loss: 0.915541, acc: 0.437500]\n",
      "1513: [D loss: 0.502024, acc: 0.765625]  [G loss: 1.555334, acc: 0.125000]\n",
      "1514: [D loss: 0.397791, acc: 0.812500]  [G loss: 0.798296, acc: 0.500000]\n",
      "1515: [D loss: 0.372073, acc: 0.804688]  [G loss: 1.159261, acc: 0.328125]\n",
      "1516: [D loss: 0.382495, acc: 0.835938]  [G loss: 0.937249, acc: 0.406250]\n",
      "1517: [D loss: 0.351251, acc: 0.835938]  [G loss: 1.139857, acc: 0.406250]\n",
      "1518: [D loss: 0.353448, acc: 0.843750]  [G loss: 0.674248, acc: 0.640625]\n",
      "1519: [D loss: 0.518987, acc: 0.765625]  [G loss: 1.640667, acc: 0.125000]\n",
      "1520: [D loss: 0.327069, acc: 0.867188]  [G loss: 0.885587, acc: 0.421875]\n",
      "1521: [D loss: 0.385720, acc: 0.843750]  [G loss: 1.909238, acc: 0.031250]\n",
      "1522: [D loss: 0.351173, acc: 0.843750]  [G loss: 0.993482, acc: 0.203125]\n",
      "1523: [D loss: 0.441226, acc: 0.820312]  [G loss: 1.402376, acc: 0.093750]\n",
      "1524: [D loss: 0.266611, acc: 0.890625]  [G loss: 1.275093, acc: 0.109375]\n",
      "1525: [D loss: 0.303666, acc: 0.875000]  [G loss: 1.424124, acc: 0.203125]\n",
      "1526: [D loss: 0.306526, acc: 0.890625]  [G loss: 1.542979, acc: 0.187500]\n",
      "1527: [D loss: 0.228942, acc: 0.929688]  [G loss: 1.786098, acc: 0.109375]\n",
      "1528: [D loss: 0.369451, acc: 0.875000]  [G loss: 1.701080, acc: 0.062500]\n",
      "1529: [D loss: 0.297171, acc: 0.843750]  [G loss: 1.267989, acc: 0.328125]\n",
      "1530: [D loss: 0.310033, acc: 0.843750]  [G loss: 1.825841, acc: 0.156250]\n",
      "1531: [D loss: 0.365990, acc: 0.843750]  [G loss: 1.097193, acc: 0.390625]\n",
      "1532: [D loss: 0.473812, acc: 0.734375]  [G loss: 1.991487, acc: 0.171875]\n",
      "1533: [D loss: 0.504247, acc: 0.726562]  [G loss: 0.538804, acc: 0.687500]\n",
      "1534: [D loss: 0.587818, acc: 0.781250]  [G loss: 1.931188, acc: 0.046875]\n",
      "1535: [D loss: 0.382488, acc: 0.820312]  [G loss: 0.947543, acc: 0.453125]\n",
      "1536: [D loss: 0.315907, acc: 0.843750]  [G loss: 1.177506, acc: 0.265625]\n",
      "1537: [D loss: 0.382927, acc: 0.781250]  [G loss: 0.971405, acc: 0.359375]\n",
      "1538: [D loss: 0.358862, acc: 0.828125]  [G loss: 1.425928, acc: 0.218750]\n",
      "1539: [D loss: 0.405710, acc: 0.781250]  [G loss: 0.524496, acc: 0.671875]\n",
      "1540: [D loss: 0.441935, acc: 0.796875]  [G loss: 1.498261, acc: 0.171875]\n",
      "1541: [D loss: 0.343703, acc: 0.867188]  [G loss: 0.683231, acc: 0.531250]\n",
      "1542: [D loss: 0.465656, acc: 0.765625]  [G loss: 1.625711, acc: 0.125000]\n",
      "1543: [D loss: 0.321886, acc: 0.828125]  [G loss: 0.927651, acc: 0.375000]\n",
      "1544: [D loss: 0.343471, acc: 0.851562]  [G loss: 1.120775, acc: 0.421875]\n",
      "1545: [D loss: 0.287568, acc: 0.875000]  [G loss: 0.696308, acc: 0.593750]\n",
      "1546: [D loss: 0.295700, acc: 0.882812]  [G loss: 1.525526, acc: 0.234375]\n",
      "1547: [D loss: 0.280734, acc: 0.867188]  [G loss: 0.613045, acc: 0.640625]\n",
      "1548: [D loss: 0.355269, acc: 0.843750]  [G loss: 1.108593, acc: 0.312500]\n",
      "1549: [D loss: 0.308992, acc: 0.867188]  [G loss: 0.852340, acc: 0.468750]\n",
      "1550: [D loss: 0.529510, acc: 0.781250]  [G loss: 2.326773, acc: 0.046875]\n",
      "1551: [D loss: 0.410222, acc: 0.835938]  [G loss: 0.704282, acc: 0.703125]\n",
      "1552: [D loss: 0.477973, acc: 0.750000]  [G loss: 1.524227, acc: 0.203125]\n",
      "1553: [D loss: 0.417986, acc: 0.781250]  [G loss: 0.620983, acc: 0.656250]\n",
      "1554: [D loss: 0.383361, acc: 0.820312]  [G loss: 2.366311, acc: 0.031250]\n",
      "1555: [D loss: 0.402825, acc: 0.796875]  [G loss: 0.869293, acc: 0.375000]\n",
      "1556: [D loss: 0.330620, acc: 0.851562]  [G loss: 1.191577, acc: 0.265625]\n",
      "1557: [D loss: 0.315597, acc: 0.875000]  [G loss: 1.412897, acc: 0.187500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558: [D loss: 0.281684, acc: 0.882812]  [G loss: 1.847903, acc: 0.093750]\n",
      "1559: [D loss: 0.440521, acc: 0.812500]  [G loss: 0.794156, acc: 0.484375]\n",
      "1560: [D loss: 0.339699, acc: 0.828125]  [G loss: 0.971746, acc: 0.343750]\n",
      "1561: [D loss: 0.304105, acc: 0.843750]  [G loss: 0.790399, acc: 0.531250]\n",
      "1562: [D loss: 0.256769, acc: 0.914062]  [G loss: 0.947654, acc: 0.390625]\n",
      "1563: [D loss: 0.359016, acc: 0.828125]  [G loss: 1.125169, acc: 0.343750]\n",
      "1564: [D loss: 0.351985, acc: 0.875000]  [G loss: 0.399608, acc: 0.796875]\n",
      "1565: [D loss: 0.376911, acc: 0.843750]  [G loss: 1.989765, acc: 0.015625]\n",
      "1566: [D loss: 0.445049, acc: 0.750000]  [G loss: 0.634017, acc: 0.671875]\n",
      "1567: [D loss: 0.365475, acc: 0.804688]  [G loss: 1.720049, acc: 0.093750]\n",
      "1568: [D loss: 0.307217, acc: 0.851562]  [G loss: 0.721104, acc: 0.500000]\n",
      "1569: [D loss: 0.279361, acc: 0.828125]  [G loss: 1.494375, acc: 0.156250]\n",
      "1570: [D loss: 0.411828, acc: 0.789062]  [G loss: 0.651343, acc: 0.593750]\n",
      "1571: [D loss: 0.288196, acc: 0.914062]  [G loss: 1.326000, acc: 0.156250]\n",
      "1572: [D loss: 0.299129, acc: 0.851562]  [G loss: 1.110652, acc: 0.218750]\n",
      "1573: [D loss: 0.423970, acc: 0.765625]  [G loss: 1.417442, acc: 0.187500]\n",
      "1574: [D loss: 0.297124, acc: 0.867188]  [G loss: 1.398228, acc: 0.156250]\n",
      "1575: [D loss: 0.299866, acc: 0.851562]  [G loss: 1.713954, acc: 0.109375]\n",
      "1576: [D loss: 0.360848, acc: 0.859375]  [G loss: 0.685207, acc: 0.578125]\n",
      "1577: [D loss: 0.338441, acc: 0.828125]  [G loss: 2.126720, acc: 0.015625]\n",
      "1578: [D loss: 0.304810, acc: 0.820312]  [G loss: 1.257125, acc: 0.265625]\n",
      "1579: [D loss: 0.399439, acc: 0.812500]  [G loss: 1.658911, acc: 0.187500]\n",
      "1580: [D loss: 0.291408, acc: 0.851562]  [G loss: 0.964038, acc: 0.484375]\n",
      "1581: [D loss: 0.327646, acc: 0.875000]  [G loss: 2.043393, acc: 0.093750]\n",
      "1582: [D loss: 0.382939, acc: 0.773438]  [G loss: 0.561488, acc: 0.640625]\n",
      "1583: [D loss: 0.494033, acc: 0.742188]  [G loss: 2.479635, acc: 0.031250]\n",
      "1584: [D loss: 0.423775, acc: 0.789062]  [G loss: 0.602698, acc: 0.703125]\n",
      "1585: [D loss: 0.494840, acc: 0.734375]  [G loss: 2.813147, acc: 0.000000]\n",
      "1586: [D loss: 0.443696, acc: 0.765625]  [G loss: 1.245464, acc: 0.125000]\n",
      "1587: [D loss: 0.347874, acc: 0.843750]  [G loss: 1.502087, acc: 0.125000]\n",
      "1588: [D loss: 0.384090, acc: 0.835938]  [G loss: 0.733299, acc: 0.578125]\n",
      "1589: [D loss: 0.325226, acc: 0.835938]  [G loss: 1.467132, acc: 0.078125]\n",
      "1590: [D loss: 0.353580, acc: 0.843750]  [G loss: 0.821479, acc: 0.421875]\n",
      "1591: [D loss: 0.335857, acc: 0.843750]  [G loss: 0.921342, acc: 0.296875]\n",
      "1592: [D loss: 0.437494, acc: 0.742188]  [G loss: 1.228524, acc: 0.218750]\n",
      "1593: [D loss: 0.381868, acc: 0.828125]  [G loss: 1.192289, acc: 0.312500]\n",
      "1594: [D loss: 0.468966, acc: 0.851562]  [G loss: 1.687151, acc: 0.062500]\n",
      "1595: [D loss: 0.412271, acc: 0.796875]  [G loss: 0.607909, acc: 0.593750]\n",
      "1596: [D loss: 0.508060, acc: 0.734375]  [G loss: 1.754902, acc: 0.046875]\n",
      "1597: [D loss: 0.325421, acc: 0.851562]  [G loss: 1.030450, acc: 0.328125]\n",
      "1598: [D loss: 0.344698, acc: 0.843750]  [G loss: 0.911174, acc: 0.406250]\n",
      "1599: [D loss: 0.292574, acc: 0.898438]  [G loss: 1.774487, acc: 0.078125]\n",
      "1600: [D loss: 0.285939, acc: 0.843750]  [G loss: 0.733380, acc: 0.546875]\n",
      "1601: [D loss: 0.279739, acc: 0.843750]  [G loss: 1.324303, acc: 0.203125]\n",
      "1602: [D loss: 0.270599, acc: 0.882812]  [G loss: 1.241050, acc: 0.171875]\n",
      "1603: [D loss: 0.332696, acc: 0.890625]  [G loss: 0.692036, acc: 0.515625]\n",
      "1604: [D loss: 0.311278, acc: 0.859375]  [G loss: 1.718218, acc: 0.093750]\n",
      "1605: [D loss: 0.307839, acc: 0.867188]  [G loss: 0.783696, acc: 0.515625]\n",
      "1606: [D loss: 0.455472, acc: 0.750000]  [G loss: 1.392577, acc: 0.265625]\n",
      "1607: [D loss: 0.287623, acc: 0.867188]  [G loss: 0.531737, acc: 0.703125]\n",
      "1608: [D loss: 0.411820, acc: 0.789062]  [G loss: 1.176215, acc: 0.296875]\n",
      "1609: [D loss: 0.416869, acc: 0.820312]  [G loss: 0.965511, acc: 0.406250]\n",
      "1610: [D loss: 0.347162, acc: 0.843750]  [G loss: 0.813554, acc: 0.593750]\n",
      "1611: [D loss: 0.299208, acc: 0.859375]  [G loss: 0.961489, acc: 0.375000]\n",
      "1612: [D loss: 0.276383, acc: 0.843750]  [G loss: 0.843541, acc: 0.500000]\n",
      "1613: [D loss: 0.270543, acc: 0.875000]  [G loss: 1.112653, acc: 0.375000]\n",
      "1614: [D loss: 0.277098, acc: 0.890625]  [G loss: 0.500063, acc: 0.687500]\n",
      "1615: [D loss: 0.348146, acc: 0.843750]  [G loss: 1.962323, acc: 0.031250]\n",
      "1616: [D loss: 0.326356, acc: 0.828125]  [G loss: 0.919019, acc: 0.390625]\n",
      "1617: [D loss: 0.309758, acc: 0.843750]  [G loss: 1.436627, acc: 0.328125]\n",
      "1618: [D loss: 0.341708, acc: 0.843750]  [G loss: 0.566648, acc: 0.625000]\n",
      "1619: [D loss: 0.325708, acc: 0.835938]  [G loss: 2.150116, acc: 0.062500]\n",
      "1620: [D loss: 0.389179, acc: 0.835938]  [G loss: 0.628897, acc: 0.609375]\n",
      "1621: [D loss: 0.322829, acc: 0.882812]  [G loss: 1.387388, acc: 0.203125]\n",
      "1622: [D loss: 0.322549, acc: 0.828125]  [G loss: 0.259874, acc: 0.890625]\n",
      "1623: [D loss: 0.371821, acc: 0.812500]  [G loss: 1.154703, acc: 0.328125]\n",
      "1624: [D loss: 0.316148, acc: 0.828125]  [G loss: 1.076559, acc: 0.343750]\n",
      "1625: [D loss: 0.279968, acc: 0.890625]  [G loss: 0.882468, acc: 0.453125]\n",
      "1626: [D loss: 0.214676, acc: 0.921875]  [G loss: 0.635328, acc: 0.562500]\n",
      "1627: [D loss: 0.350911, acc: 0.820312]  [G loss: 2.428643, acc: 0.031250]\n",
      "1628: [D loss: 0.388514, acc: 0.789062]  [G loss: 0.929943, acc: 0.390625]\n",
      "1629: [D loss: 0.268812, acc: 0.898438]  [G loss: 1.272438, acc: 0.218750]\n",
      "1630: [D loss: 0.349057, acc: 0.851562]  [G loss: 1.349143, acc: 0.171875]\n",
      "1631: [D loss: 0.366090, acc: 0.828125]  [G loss: 1.135474, acc: 0.312500]\n",
      "1632: [D loss: 0.264634, acc: 0.890625]  [G loss: 0.636209, acc: 0.609375]\n",
      "1633: [D loss: 0.223242, acc: 0.906250]  [G loss: 0.887182, acc: 0.531250]\n",
      "1634: [D loss: 0.331361, acc: 0.851562]  [G loss: 0.834448, acc: 0.609375]\n",
      "1635: [D loss: 0.375194, acc: 0.804688]  [G loss: 1.514728, acc: 0.296875]\n",
      "1636: [D loss: 0.209149, acc: 0.882812]  [G loss: 0.342342, acc: 0.812500]\n",
      "1637: [D loss: 0.182891, acc: 0.921875]  [G loss: 0.534913, acc: 0.718750]\n",
      "1638: [D loss: 0.269081, acc: 0.867188]  [G loss: 0.871867, acc: 0.562500]\n",
      "1639: [D loss: 0.302005, acc: 0.851562]  [G loss: 0.708450, acc: 0.625000]\n",
      "1640: [D loss: 0.305855, acc: 0.867188]  [G loss: 1.668873, acc: 0.234375]\n",
      "1641: [D loss: 0.357655, acc: 0.835938]  [G loss: 1.202969, acc: 0.312500]\n",
      "1642: [D loss: 0.242666, acc: 0.890625]  [G loss: 0.474071, acc: 0.734375]\n",
      "1643: [D loss: 0.339852, acc: 0.835938]  [G loss: 3.339062, acc: 0.000000]\n",
      "1644: [D loss: 0.506153, acc: 0.796875]  [G loss: 0.858123, acc: 0.578125]\n",
      "1645: [D loss: 0.435555, acc: 0.843750]  [G loss: 2.238170, acc: 0.140625]\n",
      "1646: [D loss: 0.289597, acc: 0.859375]  [G loss: 1.423254, acc: 0.265625]\n",
      "1647: [D loss: 0.252813, acc: 0.914062]  [G loss: 0.889633, acc: 0.453125]\n",
      "1648: [D loss: 0.319729, acc: 0.882812]  [G loss: 1.340175, acc: 0.312500]\n",
      "1649: [D loss: 0.250789, acc: 0.914062]  [G loss: 1.053313, acc: 0.390625]\n",
      "1650: [D loss: 0.299441, acc: 0.859375]  [G loss: 0.978814, acc: 0.421875]\n",
      "1651: [D loss: 0.279078, acc: 0.867188]  [G loss: 1.438548, acc: 0.296875]\n",
      "1652: [D loss: 0.351788, acc: 0.859375]  [G loss: 0.783328, acc: 0.546875]\n",
      "1653: [D loss: 0.444292, acc: 0.789062]  [G loss: 1.923440, acc: 0.203125]\n",
      "1654: [D loss: 0.454666, acc: 0.796875]  [G loss: 0.671644, acc: 0.640625]\n",
      "1655: [D loss: 0.365326, acc: 0.812500]  [G loss: 1.640860, acc: 0.203125]\n",
      "1656: [D loss: 0.230950, acc: 0.875000]  [G loss: 0.630591, acc: 0.578125]\n",
      "1657: [D loss: 0.289682, acc: 0.859375]  [G loss: 1.344741, acc: 0.218750]\n",
      "1658: [D loss: 0.330638, acc: 0.843750]  [G loss: 0.922317, acc: 0.546875]\n",
      "1659: [D loss: 0.267499, acc: 0.898438]  [G loss: 0.677170, acc: 0.531250]\n",
      "1660: [D loss: 0.178228, acc: 0.945312]  [G loss: 0.542097, acc: 0.703125]\n",
      "1661: [D loss: 0.249621, acc: 0.859375]  [G loss: 1.566159, acc: 0.218750]\n",
      "1662: [D loss: 0.355331, acc: 0.820312]  [G loss: 0.595557, acc: 0.625000]\n",
      "1663: [D loss: 0.304150, acc: 0.867188]  [G loss: 2.175778, acc: 0.046875]\n",
      "1664: [D loss: 0.437812, acc: 0.796875]  [G loss: 0.644032, acc: 0.593750]\n",
      "1665: [D loss: 0.357657, acc: 0.820312]  [G loss: 2.522837, acc: 0.031250]\n",
      "1666: [D loss: 0.361912, acc: 0.820312]  [G loss: 0.955510, acc: 0.375000]\n",
      "1667: [D loss: 0.314123, acc: 0.851562]  [G loss: 1.290709, acc: 0.218750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668: [D loss: 0.348265, acc: 0.851562]  [G loss: 1.110600, acc: 0.234375]\n",
      "1669: [D loss: 0.235181, acc: 0.890625]  [G loss: 0.360653, acc: 0.812500]\n",
      "1670: [D loss: 0.465909, acc: 0.789062]  [G loss: 1.414019, acc: 0.156250]\n",
      "1671: [D loss: 0.310002, acc: 0.859375]  [G loss: 0.530075, acc: 0.671875]\n",
      "1672: [D loss: 0.427922, acc: 0.765625]  [G loss: 1.782041, acc: 0.031250]\n",
      "1673: [D loss: 0.377528, acc: 0.843750]  [G loss: 0.644896, acc: 0.578125]\n",
      "1674: [D loss: 0.313402, acc: 0.867188]  [G loss: 1.378433, acc: 0.250000]\n",
      "1675: [D loss: 0.279771, acc: 0.921875]  [G loss: 0.952096, acc: 0.406250]\n",
      "1676: [D loss: 0.271076, acc: 0.906250]  [G loss: 0.898728, acc: 0.421875]\n",
      "1677: [D loss: 0.286678, acc: 0.867188]  [G loss: 0.463422, acc: 0.734375]\n",
      "1678: [D loss: 0.278590, acc: 0.867188]  [G loss: 0.907746, acc: 0.453125]\n",
      "1679: [D loss: 0.324628, acc: 0.867188]  [G loss: 0.690930, acc: 0.546875]\n",
      "1680: [D loss: 0.284264, acc: 0.890625]  [G loss: 1.250323, acc: 0.343750]\n",
      "1681: [D loss: 0.240165, acc: 0.921875]  [G loss: 1.683801, acc: 0.078125]\n",
      "1682: [D loss: 0.202055, acc: 0.890625]  [G loss: 0.558823, acc: 0.687500]\n",
      "1683: [D loss: 0.325600, acc: 0.875000]  [G loss: 0.970477, acc: 0.593750]\n",
      "1684: [D loss: 0.384407, acc: 0.843750]  [G loss: 1.230248, acc: 0.250000]\n",
      "1685: [D loss: 0.237335, acc: 0.898438]  [G loss: 0.697967, acc: 0.562500]\n",
      "1686: [D loss: 0.334389, acc: 0.843750]  [G loss: 0.897958, acc: 0.484375]\n",
      "1687: [D loss: 0.248944, acc: 0.875000]  [G loss: 0.638284, acc: 0.609375]\n",
      "1688: [D loss: 0.243598, acc: 0.875000]  [G loss: 0.792353, acc: 0.468750]\n",
      "1689: [D loss: 0.250934, acc: 0.882812]  [G loss: 0.916456, acc: 0.515625]\n",
      "1690: [D loss: 0.207906, acc: 0.906250]  [G loss: 0.826058, acc: 0.578125]\n",
      "1691: [D loss: 0.209775, acc: 0.929688]  [G loss: 0.495612, acc: 0.718750]\n",
      "1692: [D loss: 0.336695, acc: 0.843750]  [G loss: 3.811840, acc: 0.000000]\n",
      "1693: [D loss: 0.449098, acc: 0.843750]  [G loss: 0.992446, acc: 0.359375]\n",
      "1694: [D loss: 0.368259, acc: 0.835938]  [G loss: 1.516796, acc: 0.109375]\n",
      "1695: [D loss: 0.309126, acc: 0.867188]  [G loss: 1.173301, acc: 0.265625]\n",
      "1696: [D loss: 0.261432, acc: 0.882812]  [G loss: 0.908964, acc: 0.406250]\n",
      "1697: [D loss: 0.275592, acc: 0.906250]  [G loss: 1.425007, acc: 0.296875]\n",
      "1698: [D loss: 0.216257, acc: 0.921875]  [G loss: 1.168294, acc: 0.390625]\n",
      "1699: [D loss: 0.219978, acc: 0.882812]  [G loss: 1.619952, acc: 0.140625]\n",
      "1700: [D loss: 0.223555, acc: 0.921875]  [G loss: 1.256336, acc: 0.312500]\n",
      "1701: [D loss: 0.256814, acc: 0.898438]  [G loss: 0.894383, acc: 0.546875]\n",
      "1702: [D loss: 0.256773, acc: 0.914062]  [G loss: 1.756912, acc: 0.171875]\n",
      "1703: [D loss: 0.190086, acc: 0.898438]  [G loss: 0.891944, acc: 0.468750]\n",
      "1704: [D loss: 0.294938, acc: 0.859375]  [G loss: 1.872659, acc: 0.234375]\n",
      "1705: [D loss: 0.307128, acc: 0.843750]  [G loss: 0.583580, acc: 0.593750]\n",
      "1706: [D loss: 0.335389, acc: 0.867188]  [G loss: 2.487396, acc: 0.062500]\n",
      "1707: [D loss: 0.422620, acc: 0.828125]  [G loss: 0.656710, acc: 0.593750]\n",
      "1708: [D loss: 0.313095, acc: 0.835938]  [G loss: 1.082623, acc: 0.359375]\n",
      "1709: [D loss: 0.196987, acc: 0.898438]  [G loss: 0.793932, acc: 0.484375]\n",
      "1710: [D loss: 0.237905, acc: 0.929688]  [G loss: 1.121758, acc: 0.421875]\n",
      "1711: [D loss: 0.260864, acc: 0.898438]  [G loss: 0.731532, acc: 0.578125]\n",
      "1712: [D loss: 0.277039, acc: 0.835938]  [G loss: 1.613570, acc: 0.265625]\n",
      "1713: [D loss: 0.275368, acc: 0.906250]  [G loss: 0.980396, acc: 0.453125]\n",
      "1714: [D loss: 0.244063, acc: 0.898438]  [G loss: 1.034050, acc: 0.437500]\n",
      "1715: [D loss: 0.305654, acc: 0.859375]  [G loss: 0.965263, acc: 0.453125]\n",
      "1716: [D loss: 0.289242, acc: 0.875000]  [G loss: 0.743670, acc: 0.531250]\n",
      "1717: [D loss: 0.193169, acc: 0.898438]  [G loss: 0.962750, acc: 0.437500]\n",
      "1718: [D loss: 0.307666, acc: 0.890625]  [G loss: 1.147925, acc: 0.421875]\n",
      "1719: [D loss: 0.190965, acc: 0.953125]  [G loss: 2.038118, acc: 0.109375]\n",
      "1720: [D loss: 0.373392, acc: 0.859375]  [G loss: 0.891272, acc: 0.484375]\n",
      "1721: [D loss: 0.201025, acc: 0.929688]  [G loss: 0.394769, acc: 0.828125]\n",
      "1722: [D loss: 0.109839, acc: 0.945312]  [G loss: 0.464448, acc: 0.750000]\n",
      "1723: [D loss: 0.159370, acc: 0.953125]  [G loss: 1.138547, acc: 0.406250]\n",
      "1724: [D loss: 0.207888, acc: 0.898438]  [G loss: 0.604244, acc: 0.656250]\n",
      "1725: [D loss: 0.151951, acc: 0.914062]  [G loss: 1.182603, acc: 0.343750]\n",
      "1726: [D loss: 0.252335, acc: 0.906250]  [G loss: 0.265844, acc: 0.859375]\n",
      "1727: [D loss: 0.296344, acc: 0.882812]  [G loss: 2.088931, acc: 0.109375]\n",
      "1728: [D loss: 0.323209, acc: 0.867188]  [G loss: 0.211624, acc: 0.937500]\n",
      "1729: [D loss: 0.535828, acc: 0.773438]  [G loss: 3.456889, acc: 0.109375]\n",
      "1730: [D loss: 0.417673, acc: 0.820312]  [G loss: 1.349025, acc: 0.359375]\n",
      "1731: [D loss: 0.184129, acc: 0.937500]  [G loss: 0.974523, acc: 0.421875]\n",
      "1732: [D loss: 0.405541, acc: 0.773438]  [G loss: 1.706553, acc: 0.187500]\n",
      "1733: [D loss: 0.270617, acc: 0.875000]  [G loss: 1.428560, acc: 0.312500]\n",
      "1734: [D loss: 0.227650, acc: 0.906250]  [G loss: 0.755807, acc: 0.625000]\n",
      "1735: [D loss: 0.274654, acc: 0.867188]  [G loss: 1.668260, acc: 0.203125]\n",
      "1736: [D loss: 0.184282, acc: 0.921875]  [G loss: 1.513028, acc: 0.250000]\n",
      "1737: [D loss: 0.264459, acc: 0.875000]  [G loss: 0.748950, acc: 0.562500]\n",
      "1738: [D loss: 0.411962, acc: 0.796875]  [G loss: 2.499418, acc: 0.062500]\n",
      "1739: [D loss: 0.366079, acc: 0.835938]  [G loss: 0.592750, acc: 0.687500]\n",
      "1740: [D loss: 0.605052, acc: 0.734375]  [G loss: 1.720172, acc: 0.234375]\n",
      "1741: [D loss: 0.394483, acc: 0.820312]  [G loss: 0.469454, acc: 0.718750]\n",
      "1742: [D loss: 0.376861, acc: 0.820312]  [G loss: 1.264496, acc: 0.328125]\n",
      "1743: [D loss: 0.257331, acc: 0.875000]  [G loss: 0.575666, acc: 0.625000]\n",
      "1744: [D loss: 0.243643, acc: 0.921875]  [G loss: 0.530965, acc: 0.687500]\n",
      "1745: [D loss: 0.236390, acc: 0.898438]  [G loss: 0.826795, acc: 0.515625]\n",
      "1746: [D loss: 0.179096, acc: 0.929688]  [G loss: 0.557100, acc: 0.687500]\n",
      "1747: [D loss: 0.231030, acc: 0.898438]  [G loss: 0.531810, acc: 0.687500]\n",
      "1748: [D loss: 0.292052, acc: 0.859375]  [G loss: 0.672471, acc: 0.656250]\n",
      "1749: [D loss: 0.297854, acc: 0.867188]  [G loss: 1.123855, acc: 0.437500]\n",
      "1750: [D loss: 0.258023, acc: 0.890625]  [G loss: 0.216991, acc: 0.890625]\n",
      "1751: [D loss: 0.466841, acc: 0.773438]  [G loss: 1.986372, acc: 0.250000]\n",
      "1752: [D loss: 0.813952, acc: 0.679688]  [G loss: 0.483155, acc: 0.687500]\n",
      "1753: [D loss: 0.430577, acc: 0.812500]  [G loss: 2.118043, acc: 0.109375]\n",
      "1754: [D loss: 0.422326, acc: 0.812500]  [G loss: 0.908514, acc: 0.375000]\n",
      "1755: [D loss: 0.406466, acc: 0.820312]  [G loss: 1.096549, acc: 0.312500]\n",
      "1756: [D loss: 0.350065, acc: 0.867188]  [G loss: 1.225118, acc: 0.312500]\n",
      "1757: [D loss: 0.361606, acc: 0.828125]  [G loss: 0.807030, acc: 0.531250]\n",
      "1758: [D loss: 0.359454, acc: 0.796875]  [G loss: 0.994971, acc: 0.390625]\n",
      "1759: [D loss: 0.248063, acc: 0.859375]  [G loss: 0.712118, acc: 0.562500]\n",
      "1760: [D loss: 0.257979, acc: 0.898438]  [G loss: 0.484788, acc: 0.703125]\n",
      "1761: [D loss: 0.253434, acc: 0.898438]  [G loss: 0.477413, acc: 0.718750]\n",
      "1762: [D loss: 0.191682, acc: 0.914062]  [G loss: 0.410537, acc: 0.781250]\n",
      "1763: [D loss: 0.360786, acc: 0.804688]  [G loss: 0.644349, acc: 0.640625]\n",
      "1764: [D loss: 0.246486, acc: 0.875000]  [G loss: 0.480775, acc: 0.703125]\n",
      "1765: [D loss: 0.300474, acc: 0.851562]  [G loss: 0.244303, acc: 0.921875]\n",
      "1766: [D loss: 0.389331, acc: 0.804688]  [G loss: 1.650506, acc: 0.343750]\n",
      "1767: [D loss: 0.582002, acc: 0.734375]  [G loss: 0.193155, acc: 0.968750]\n",
      "1768: [D loss: 0.260752, acc: 0.867188]  [G loss: 0.733064, acc: 0.593750]\n",
      "1769: [D loss: 0.309409, acc: 0.835938]  [G loss: 0.801068, acc: 0.546875]\n",
      "1770: [D loss: 0.240454, acc: 0.929688]  [G loss: 0.312591, acc: 0.875000]\n",
      "1771: [D loss: 0.297580, acc: 0.867188]  [G loss: 0.389672, acc: 0.828125]\n",
      "1772: [D loss: 0.242723, acc: 0.890625]  [G loss: 0.369201, acc: 0.812500]\n",
      "1773: [D loss: 0.327711, acc: 0.875000]  [G loss: 1.159466, acc: 0.453125]\n",
      "1774: [D loss: 0.298480, acc: 0.875000]  [G loss: 0.495083, acc: 0.734375]\n",
      "1775: [D loss: 0.344249, acc: 0.851562]  [G loss: 0.936936, acc: 0.484375]\n",
      "1776: [D loss: 0.341787, acc: 0.867188]  [G loss: 0.503716, acc: 0.765625]\n",
      "1777: [D loss: 0.354091, acc: 0.820312]  [G loss: 1.537596, acc: 0.156250]\n",
      "1778: [D loss: 0.306088, acc: 0.875000]  [G loss: 0.486071, acc: 0.734375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1779: [D loss: 0.424575, acc: 0.773438]  [G loss: 1.389429, acc: 0.343750]\n",
      "1780: [D loss: 0.346948, acc: 0.820312]  [G loss: 0.837547, acc: 0.562500]\n",
      "1781: [D loss: 0.173323, acc: 0.937500]  [G loss: 0.465970, acc: 0.734375]\n",
      "1782: [D loss: 0.456255, acc: 0.835938]  [G loss: 2.338911, acc: 0.015625]\n",
      "1783: [D loss: 0.462042, acc: 0.750000]  [G loss: 0.638474, acc: 0.687500]\n",
      "1784: [D loss: 0.394502, acc: 0.796875]  [G loss: 1.411474, acc: 0.171875]\n",
      "1785: [D loss: 0.401950, acc: 0.789062]  [G loss: 0.600499, acc: 0.671875]\n",
      "1786: [D loss: 0.347836, acc: 0.820312]  [G loss: 1.424842, acc: 0.140625]\n",
      "1787: [D loss: 0.258511, acc: 0.851562]  [G loss: 0.392572, acc: 0.750000]\n",
      "1788: [D loss: 0.603298, acc: 0.750000]  [G loss: 1.352403, acc: 0.187500]\n",
      "1789: [D loss: 0.297178, acc: 0.859375]  [G loss: 0.750962, acc: 0.531250]\n",
      "1790: [D loss: 0.349866, acc: 0.820312]  [G loss: 1.004341, acc: 0.328125]\n",
      "1791: [D loss: 0.318626, acc: 0.820312]  [G loss: 0.760949, acc: 0.562500]\n",
      "1792: [D loss: 0.266193, acc: 0.898438]  [G loss: 0.947198, acc: 0.421875]\n",
      "1793: [D loss: 0.295065, acc: 0.875000]  [G loss: 0.779007, acc: 0.484375]\n",
      "1794: [D loss: 0.422144, acc: 0.812500]  [G loss: 0.819457, acc: 0.468750]\n",
      "1795: [D loss: 0.156846, acc: 0.953125]  [G loss: 0.551802, acc: 0.609375]\n",
      "1796: [D loss: 0.309934, acc: 0.820312]  [G loss: 0.692587, acc: 0.562500]\n",
      "1797: [D loss: 0.277988, acc: 0.875000]  [G loss: 0.380266, acc: 0.812500]\n",
      "1798: [D loss: 0.144542, acc: 0.960938]  [G loss: 0.203076, acc: 0.906250]\n",
      "1799: [D loss: 0.415004, acc: 0.804688]  [G loss: 1.088083, acc: 0.343750]\n",
      "1800: [D loss: 0.433237, acc: 0.757812]  [G loss: 0.502755, acc: 0.812500]\n",
      "1801: [D loss: 0.317518, acc: 0.882812]  [G loss: 0.671957, acc: 0.640625]\n",
      "1802: [D loss: 0.290954, acc: 0.898438]  [G loss: 0.602750, acc: 0.703125]\n",
      "1803: [D loss: 0.373692, acc: 0.828125]  [G loss: 1.312791, acc: 0.234375]\n",
      "1804: [D loss: 0.341662, acc: 0.859375]  [G loss: 0.190410, acc: 0.937500]\n",
      "1805: [D loss: 0.545719, acc: 0.757812]  [G loss: 0.956968, acc: 0.390625]\n",
      "1806: [D loss: 0.352297, acc: 0.835938]  [G loss: 0.956786, acc: 0.453125]\n",
      "1807: [D loss: 0.354464, acc: 0.843750]  [G loss: 2.014535, acc: 0.015625]\n",
      "1808: [D loss: 0.309063, acc: 0.875000]  [G loss: 0.732749, acc: 0.531250]\n",
      "1809: [D loss: 0.476180, acc: 0.742188]  [G loss: 2.226028, acc: 0.046875]\n",
      "1810: [D loss: 0.320506, acc: 0.851562]  [G loss: 0.862115, acc: 0.562500]\n",
      "1811: [D loss: 0.259236, acc: 0.859375]  [G loss: 1.151618, acc: 0.343750]\n",
      "1812: [D loss: 0.360047, acc: 0.828125]  [G loss: 0.794726, acc: 0.500000]\n",
      "1813: [D loss: 0.368622, acc: 0.804688]  [G loss: 1.828579, acc: 0.046875]\n",
      "1814: [D loss: 0.375309, acc: 0.851562]  [G loss: 1.118042, acc: 0.328125]\n",
      "1815: [D loss: 0.221234, acc: 0.882812]  [G loss: 0.504709, acc: 0.687500]\n",
      "1816: [D loss: 0.347800, acc: 0.804688]  [G loss: 1.072441, acc: 0.375000]\n",
      "1817: [D loss: 0.218393, acc: 0.929688]  [G loss: 0.438028, acc: 0.750000]\n",
      "1818: [D loss: 0.171851, acc: 0.921875]  [G loss: 0.755769, acc: 0.578125]\n",
      "1819: [D loss: 0.362630, acc: 0.820312]  [G loss: 0.871905, acc: 0.500000]\n",
      "1820: [D loss: 0.210679, acc: 0.882812]  [G loss: 0.341288, acc: 0.796875]\n",
      "1821: [D loss: 0.480122, acc: 0.726562]  [G loss: 2.540587, acc: 0.015625]\n",
      "1822: [D loss: 0.437055, acc: 0.773438]  [G loss: 0.554954, acc: 0.625000]\n",
      "1823: [D loss: 0.423887, acc: 0.796875]  [G loss: 1.733389, acc: 0.062500]\n",
      "1824: [D loss: 0.317023, acc: 0.796875]  [G loss: 0.554530, acc: 0.703125]\n",
      "1825: [D loss: 0.241742, acc: 0.875000]  [G loss: 0.735262, acc: 0.546875]\n",
      "1826: [D loss: 0.278712, acc: 0.851562]  [G loss: 0.925202, acc: 0.468750]\n",
      "1827: [D loss: 0.232663, acc: 0.914062]  [G loss: 1.226449, acc: 0.296875]\n",
      "1828: [D loss: 0.231010, acc: 0.890625]  [G loss: 0.739473, acc: 0.546875]\n",
      "1829: [D loss: 0.342993, acc: 0.890625]  [G loss: 0.966425, acc: 0.390625]\n",
      "1830: [D loss: 0.254023, acc: 0.890625]  [G loss: 1.040987, acc: 0.359375]\n",
      "1831: [D loss: 0.249434, acc: 0.898438]  [G loss: 0.803682, acc: 0.593750]\n",
      "1832: [D loss: 0.231600, acc: 0.890625]  [G loss: 1.670778, acc: 0.156250]\n",
      "1833: [D loss: 0.424086, acc: 0.812500]  [G loss: 0.457806, acc: 0.718750]\n",
      "1834: [D loss: 0.432950, acc: 0.750000]  [G loss: 2.590160, acc: 0.000000]\n",
      "1835: [D loss: 0.471071, acc: 0.765625]  [G loss: 0.481954, acc: 0.765625]\n",
      "1836: [D loss: 0.261667, acc: 0.898438]  [G loss: 1.125863, acc: 0.250000]\n",
      "1837: [D loss: 0.401654, acc: 0.757812]  [G loss: 0.708037, acc: 0.531250]\n",
      "1838: [D loss: 0.243959, acc: 0.882812]  [G loss: 0.882061, acc: 0.343750]\n",
      "1839: [D loss: 0.309582, acc: 0.843750]  [G loss: 1.298272, acc: 0.171875]\n",
      "1840: [D loss: 0.313119, acc: 0.843750]  [G loss: 0.435439, acc: 0.765625]\n",
      "1841: [D loss: 0.590682, acc: 0.671875]  [G loss: 2.447953, acc: 0.000000]\n",
      "1842: [D loss: 0.366538, acc: 0.820312]  [G loss: 0.985995, acc: 0.328125]\n",
      "1843: [D loss: 0.333410, acc: 0.851562]  [G loss: 1.382835, acc: 0.125000]\n",
      "1844: [D loss: 0.295973, acc: 0.859375]  [G loss: 1.225410, acc: 0.265625]\n",
      "1845: [D loss: 0.347505, acc: 0.812500]  [G loss: 1.257749, acc: 0.234375]\n",
      "1846: [D loss: 0.332704, acc: 0.812500]  [G loss: 1.209459, acc: 0.296875]\n",
      "1847: [D loss: 0.363563, acc: 0.820312]  [G loss: 0.806628, acc: 0.531250]\n",
      "1848: [D loss: 0.259165, acc: 0.875000]  [G loss: 0.638841, acc: 0.625000]\n",
      "1849: [D loss: 0.406136, acc: 0.773438]  [G loss: 1.597960, acc: 0.296875]\n",
      "1850: [D loss: 0.503620, acc: 0.757812]  [G loss: 0.640021, acc: 0.656250]\n",
      "1851: [D loss: 0.271505, acc: 0.890625]  [G loss: 1.114136, acc: 0.375000]\n",
      "1852: [D loss: 0.403991, acc: 0.773438]  [G loss: 1.462478, acc: 0.171875]\n",
      "1853: [D loss: 0.406361, acc: 0.820312]  [G loss: 0.484393, acc: 0.734375]\n",
      "1854: [D loss: 0.284802, acc: 0.875000]  [G loss: 0.865029, acc: 0.546875]\n",
      "1855: [D loss: 0.474086, acc: 0.773438]  [G loss: 1.221901, acc: 0.250000]\n",
      "1856: [D loss: 0.391244, acc: 0.765625]  [G loss: 0.786471, acc: 0.453125]\n",
      "1857: [D loss: 0.431149, acc: 0.750000]  [G loss: 1.558383, acc: 0.093750]\n",
      "1858: [D loss: 0.424487, acc: 0.812500]  [G loss: 0.803189, acc: 0.468750]\n",
      "1859: [D loss: 0.324037, acc: 0.875000]  [G loss: 1.451950, acc: 0.046875]\n",
      "1860: [D loss: 0.344679, acc: 0.828125]  [G loss: 1.154420, acc: 0.203125]\n",
      "1861: [D loss: 0.391683, acc: 0.820312]  [G loss: 1.138849, acc: 0.156250]\n",
      "1862: [D loss: 0.367780, acc: 0.804688]  [G loss: 0.899734, acc: 0.421875]\n",
      "1863: [D loss: 0.359510, acc: 0.812500]  [G loss: 1.153735, acc: 0.328125]\n",
      "1864: [D loss: 0.192675, acc: 0.914062]  [G loss: 0.594608, acc: 0.656250]\n",
      "1865: [D loss: 0.319446, acc: 0.812500]  [G loss: 1.858163, acc: 0.062500]\n",
      "1866: [D loss: 0.329774, acc: 0.835938]  [G loss: 0.498282, acc: 0.687500]\n",
      "1867: [D loss: 0.524815, acc: 0.804688]  [G loss: 1.967309, acc: 0.078125]\n",
      "1868: [D loss: 0.360997, acc: 0.812500]  [G loss: 0.837593, acc: 0.421875]\n",
      "1869: [D loss: 0.443466, acc: 0.773438]  [G loss: 0.927312, acc: 0.328125]\n",
      "1870: [D loss: 0.300499, acc: 0.835938]  [G loss: 0.498161, acc: 0.687500]\n",
      "1871: [D loss: 0.307554, acc: 0.851562]  [G loss: 1.030526, acc: 0.328125]\n",
      "1872: [D loss: 0.338029, acc: 0.820312]  [G loss: 0.819097, acc: 0.421875]\n",
      "1873: [D loss: 0.302605, acc: 0.859375]  [G loss: 0.670220, acc: 0.640625]\n",
      "1874: [D loss: 0.238000, acc: 0.898438]  [G loss: 0.277575, acc: 0.890625]\n",
      "1875: [D loss: 0.379701, acc: 0.820312]  [G loss: 1.007133, acc: 0.328125]\n",
      "1876: [D loss: 0.238703, acc: 0.882812]  [G loss: 0.359754, acc: 0.843750]\n",
      "1877: [D loss: 0.327335, acc: 0.820312]  [G loss: 1.028028, acc: 0.312500]\n",
      "1878: [D loss: 0.403985, acc: 0.796875]  [G loss: 0.715818, acc: 0.578125]\n",
      "1879: [D loss: 0.210423, acc: 0.945312]  [G loss: 0.779478, acc: 0.453125]\n",
      "1880: [D loss: 0.288765, acc: 0.898438]  [G loss: 0.819501, acc: 0.484375]\n",
      "1881: [D loss: 0.436113, acc: 0.804688]  [G loss: 1.351103, acc: 0.140625]\n",
      "1882: [D loss: 0.295279, acc: 0.859375]  [G loss: 0.196980, acc: 0.984375]\n",
      "1883: [D loss: 0.286757, acc: 0.875000]  [G loss: 1.300668, acc: 0.218750]\n",
      "1884: [D loss: 0.390832, acc: 0.820312]  [G loss: 0.385235, acc: 0.843750]\n",
      "1885: [D loss: 0.504734, acc: 0.710938]  [G loss: 2.881086, acc: 0.000000]\n",
      "1886: [D loss: 0.474283, acc: 0.789062]  [G loss: 0.838628, acc: 0.406250]\n",
      "1887: [D loss: 0.330327, acc: 0.835938]  [G loss: 1.236430, acc: 0.187500]\n",
      "1888: [D loss: 0.244411, acc: 0.867188]  [G loss: 0.572263, acc: 0.703125]\n",
      "1889: [D loss: 0.224279, acc: 0.921875]  [G loss: 1.014734, acc: 0.312500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1890: [D loss: 0.266701, acc: 0.882812]  [G loss: 1.117346, acc: 0.359375]\n",
      "1891: [D loss: 0.219992, acc: 0.859375]  [G loss: 0.285441, acc: 0.890625]\n",
      "1892: [D loss: 0.286070, acc: 0.859375]  [G loss: 0.887635, acc: 0.421875]\n",
      "1893: [D loss: 0.227909, acc: 0.890625]  [G loss: 0.434282, acc: 0.765625]\n",
      "1894: [D loss: 0.359856, acc: 0.804688]  [G loss: 1.802941, acc: 0.109375]\n",
      "1895: [D loss: 0.314232, acc: 0.835938]  [G loss: 0.418289, acc: 0.718750]\n",
      "1896: [D loss: 0.649223, acc: 0.718750]  [G loss: 2.018674, acc: 0.031250]\n",
      "1897: [D loss: 0.386976, acc: 0.828125]  [G loss: 1.260499, acc: 0.187500]\n",
      "1898: [D loss: 0.305647, acc: 0.890625]  [G loss: 1.628143, acc: 0.046875]\n",
      "1899: [D loss: 0.290838, acc: 0.867188]  [G loss: 1.227405, acc: 0.203125]\n",
      "1900: [D loss: 0.179778, acc: 0.937500]  [G loss: 1.158190, acc: 0.234375]\n",
      "1901: [D loss: 0.251194, acc: 0.882812]  [G loss: 0.873667, acc: 0.375000]\n",
      "1902: [D loss: 0.246365, acc: 0.882812]  [G loss: 1.063005, acc: 0.390625]\n",
      "1903: [D loss: 0.317550, acc: 0.859375]  [G loss: 1.074062, acc: 0.312500]\n",
      "1904: [D loss: 0.341142, acc: 0.867188]  [G loss: 1.250645, acc: 0.296875]\n",
      "1905: [D loss: 0.315517, acc: 0.820312]  [G loss: 1.318816, acc: 0.265625]\n",
      "1906: [D loss: 0.288849, acc: 0.843750]  [G loss: 0.360943, acc: 0.781250]\n",
      "1907: [D loss: 0.432893, acc: 0.789062]  [G loss: 2.288098, acc: 0.062500]\n",
      "1908: [D loss: 0.283028, acc: 0.851562]  [G loss: 0.622967, acc: 0.703125]\n",
      "1909: [D loss: 0.591752, acc: 0.742188]  [G loss: 2.512028, acc: 0.015625]\n",
      "1910: [D loss: 0.385348, acc: 0.781250]  [G loss: 0.702022, acc: 0.562500]\n",
      "1911: [D loss: 0.314924, acc: 0.875000]  [G loss: 1.517243, acc: 0.187500]\n",
      "1912: [D loss: 0.283354, acc: 0.906250]  [G loss: 1.115631, acc: 0.250000]\n",
      "1913: [D loss: 0.182505, acc: 0.929688]  [G loss: 1.189322, acc: 0.281250]\n",
      "1914: [D loss: 0.306362, acc: 0.898438]  [G loss: 0.952862, acc: 0.421875]\n",
      "1915: [D loss: 0.249140, acc: 0.882812]  [G loss: 0.894550, acc: 0.453125]\n",
      "1916: [D loss: 0.295061, acc: 0.859375]  [G loss: 0.974952, acc: 0.359375]\n",
      "1917: [D loss: 0.312137, acc: 0.835938]  [G loss: 1.208003, acc: 0.359375]\n",
      "1918: [D loss: 0.391805, acc: 0.828125]  [G loss: 0.458886, acc: 0.718750]\n",
      "1919: [D loss: 0.481596, acc: 0.812500]  [G loss: 2.323246, acc: 0.031250]\n",
      "1920: [D loss: 0.418714, acc: 0.804688]  [G loss: 0.840885, acc: 0.484375]\n",
      "1921: [D loss: 0.416550, acc: 0.796875]  [G loss: 1.400682, acc: 0.187500]\n",
      "1922: [D loss: 0.313932, acc: 0.820312]  [G loss: 0.699280, acc: 0.546875]\n",
      "1923: [D loss: 0.391202, acc: 0.820312]  [G loss: 1.351489, acc: 0.234375]\n",
      "1924: [D loss: 0.342708, acc: 0.828125]  [G loss: 0.719075, acc: 0.531250]\n",
      "1925: [D loss: 0.288093, acc: 0.882812]  [G loss: 1.170934, acc: 0.343750]\n",
      "1926: [D loss: 0.440450, acc: 0.820312]  [G loss: 1.031735, acc: 0.406250]\n",
      "1927: [D loss: 0.283367, acc: 0.882812]  [G loss: 1.269338, acc: 0.265625]\n",
      "1928: [D loss: 0.299901, acc: 0.867188]  [G loss: 1.463489, acc: 0.187500]\n",
      "1929: [D loss: 0.344264, acc: 0.851562]  [G loss: 0.895136, acc: 0.375000]\n",
      "1930: [D loss: 0.357859, acc: 0.804688]  [G loss: 1.616464, acc: 0.234375]\n",
      "1931: [D loss: 0.286101, acc: 0.859375]  [G loss: 0.640583, acc: 0.593750]\n",
      "1932: [D loss: 0.555909, acc: 0.750000]  [G loss: 1.552164, acc: 0.093750]\n",
      "1933: [D loss: 0.419089, acc: 0.773438]  [G loss: 0.810936, acc: 0.468750]\n",
      "1934: [D loss: 0.387483, acc: 0.828125]  [G loss: 1.647643, acc: 0.171875]\n",
      "1935: [D loss: 0.284653, acc: 0.882812]  [G loss: 0.883364, acc: 0.468750]\n",
      "1936: [D loss: 0.366290, acc: 0.812500]  [G loss: 1.276054, acc: 0.265625]\n",
      "1937: [D loss: 0.378477, acc: 0.812500]  [G loss: 0.756873, acc: 0.515625]\n",
      "1938: [D loss: 0.435247, acc: 0.820312]  [G loss: 1.028442, acc: 0.375000]\n",
      "1939: [D loss: 0.344663, acc: 0.828125]  [G loss: 0.902557, acc: 0.437500]\n",
      "1940: [D loss: 0.283681, acc: 0.851562]  [G loss: 0.346671, acc: 0.859375]\n",
      "1941: [D loss: 0.512960, acc: 0.742188]  [G loss: 2.484480, acc: 0.093750]\n",
      "1942: [D loss: 0.445708, acc: 0.765625]  [G loss: 0.818496, acc: 0.468750]\n",
      "1943: [D loss: 0.329181, acc: 0.851562]  [G loss: 1.358810, acc: 0.187500]\n",
      "1944: [D loss: 0.251491, acc: 0.867188]  [G loss: 0.664523, acc: 0.625000]\n",
      "1945: [D loss: 0.408201, acc: 0.796875]  [G loss: 1.348859, acc: 0.156250]\n",
      "1946: [D loss: 0.319623, acc: 0.843750]  [G loss: 0.682763, acc: 0.640625]\n",
      "1947: [D loss: 0.374803, acc: 0.812500]  [G loss: 1.430350, acc: 0.250000]\n",
      "1948: [D loss: 0.269085, acc: 0.882812]  [G loss: 1.181935, acc: 0.312500]\n",
      "1949: [D loss: 0.268956, acc: 0.867188]  [G loss: 0.489167, acc: 0.656250]\n",
      "1950: [D loss: 0.348904, acc: 0.812500]  [G loss: 0.992713, acc: 0.375000]\n",
      "1951: [D loss: 0.254515, acc: 0.906250]  [G loss: 0.757456, acc: 0.484375]\n",
      "1952: [D loss: 0.359760, acc: 0.851562]  [G loss: 1.483423, acc: 0.281250]\n",
      "1953: [D loss: 0.364556, acc: 0.859375]  [G loss: 0.458481, acc: 0.781250]\n",
      "1954: [D loss: 0.588039, acc: 0.710938]  [G loss: 1.736389, acc: 0.109375]\n",
      "1955: [D loss: 0.404084, acc: 0.828125]  [G loss: 0.752665, acc: 0.531250]\n",
      "1956: [D loss: 0.369745, acc: 0.835938]  [G loss: 1.751920, acc: 0.140625]\n",
      "1957: [D loss: 0.359950, acc: 0.851562]  [G loss: 0.727936, acc: 0.562500]\n",
      "1958: [D loss: 0.419710, acc: 0.781250]  [G loss: 1.511143, acc: 0.093750]\n",
      "1959: [D loss: 0.323387, acc: 0.835938]  [G loss: 0.934906, acc: 0.421875]\n",
      "1960: [D loss: 0.288731, acc: 0.867188]  [G loss: 1.333248, acc: 0.234375]\n",
      "1961: [D loss: 0.332754, acc: 0.867188]  [G loss: 0.933766, acc: 0.500000]\n",
      "1962: [D loss: 0.261743, acc: 0.890625]  [G loss: 0.729775, acc: 0.593750]\n",
      "1963: [D loss: 0.298415, acc: 0.898438]  [G loss: 0.824796, acc: 0.515625]\n",
      "1964: [D loss: 0.276016, acc: 0.890625]  [G loss: 0.762759, acc: 0.500000]\n",
      "1965: [D loss: 0.261372, acc: 0.882812]  [G loss: 1.409319, acc: 0.296875]\n",
      "1966: [D loss: 0.303738, acc: 0.859375]  [G loss: 0.437955, acc: 0.796875]\n",
      "1967: [D loss: 0.517581, acc: 0.726562]  [G loss: 1.842463, acc: 0.109375]\n",
      "1968: [D loss: 0.489291, acc: 0.789062]  [G loss: 0.732005, acc: 0.562500]\n",
      "1969: [D loss: 0.249203, acc: 0.882812]  [G loss: 0.926323, acc: 0.390625]\n",
      "1970: [D loss: 0.271642, acc: 0.890625]  [G loss: 0.744901, acc: 0.515625]\n",
      "1971: [D loss: 0.297951, acc: 0.875000]  [G loss: 1.156671, acc: 0.296875]\n",
      "1972: [D loss: 0.327585, acc: 0.875000]  [G loss: 0.851756, acc: 0.453125]\n",
      "1973: [D loss: 0.255517, acc: 0.882812]  [G loss: 1.399814, acc: 0.250000]\n",
      "1974: [D loss: 0.199098, acc: 0.953125]  [G loss: 0.747123, acc: 0.515625]\n",
      "1975: [D loss: 0.230679, acc: 0.882812]  [G loss: 1.345124, acc: 0.218750]\n",
      "1976: [D loss: 0.355983, acc: 0.859375]  [G loss: 0.783301, acc: 0.578125]\n",
      "1977: [D loss: 0.274772, acc: 0.914062]  [G loss: 2.054757, acc: 0.093750]\n",
      "1978: [D loss: 0.161042, acc: 0.929688]  [G loss: 1.613018, acc: 0.218750]\n",
      "1979: [D loss: 0.271135, acc: 0.859375]  [G loss: 0.621319, acc: 0.687500]\n",
      "1980: [D loss: 0.368003, acc: 0.835938]  [G loss: 1.422960, acc: 0.390625]\n",
      "1981: [D loss: 0.468477, acc: 0.765625]  [G loss: 1.970446, acc: 0.156250]\n",
      "1982: [D loss: 0.308418, acc: 0.867188]  [G loss: 1.019511, acc: 0.406250]\n",
      "1983: [D loss: 0.278107, acc: 0.851562]  [G loss: 0.851392, acc: 0.500000]\n",
      "1984: [D loss: 0.221728, acc: 0.921875]  [G loss: 1.260720, acc: 0.234375]\n",
      "1985: [D loss: 0.339021, acc: 0.828125]  [G loss: 1.260051, acc: 0.328125]\n",
      "1986: [D loss: 0.339694, acc: 0.820312]  [G loss: 0.964855, acc: 0.421875]\n",
      "1987: [D loss: 0.328071, acc: 0.843750]  [G loss: 1.023654, acc: 0.359375]\n",
      "1988: [D loss: 0.344403, acc: 0.828125]  [G loss: 0.977497, acc: 0.390625]\n",
      "1989: [D loss: 0.260675, acc: 0.898438]  [G loss: 1.473209, acc: 0.156250]\n",
      "1990: [D loss: 0.287964, acc: 0.882812]  [G loss: 1.272482, acc: 0.296875]\n",
      "1991: [D loss: 0.234524, acc: 0.898438]  [G loss: 1.632082, acc: 0.312500]\n",
      "1992: [D loss: 0.348311, acc: 0.828125]  [G loss: 0.684256, acc: 0.578125]\n",
      "1993: [D loss: 0.563550, acc: 0.765625]  [G loss: 2.359879, acc: 0.046875]\n",
      "1994: [D loss: 0.488644, acc: 0.804688]  [G loss: 1.073811, acc: 0.359375]\n",
      "1995: [D loss: 0.364827, acc: 0.804688]  [G loss: 1.608452, acc: 0.265625]\n",
      "1996: [D loss: 0.385722, acc: 0.812500]  [G loss: 0.919575, acc: 0.421875]\n",
      "1997: [D loss: 0.289005, acc: 0.882812]  [G loss: 1.271321, acc: 0.281250]\n",
      "1998: [D loss: 0.260691, acc: 0.890625]  [G loss: 1.026371, acc: 0.328125]\n",
      "1999: [D loss: 0.349638, acc: 0.851562]  [G loss: 0.972965, acc: 0.359375]\n",
      "2000: [D loss: 0.275429, acc: 0.875000]  [G loss: 0.505493, acc: 0.703125]\n",
      "2001: [D loss: 0.403148, acc: 0.750000]  [G loss: 2.469813, acc: 0.015625]\n",
      "2002: [D loss: 0.331056, acc: 0.851562]  [G loss: 1.121108, acc: 0.265625]\n",
      "2003: [D loss: 0.245257, acc: 0.890625]  [G loss: 1.479403, acc: 0.250000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004: [D loss: 0.194810, acc: 0.921875]  [G loss: 0.972883, acc: 0.359375]\n",
      "2005: [D loss: 0.226694, acc: 0.906250]  [G loss: 0.969099, acc: 0.328125]\n",
      "2006: [D loss: 0.270192, acc: 0.875000]  [G loss: 1.332972, acc: 0.312500]\n",
      "2007: [D loss: 0.227319, acc: 0.898438]  [G loss: 1.015739, acc: 0.406250]\n",
      "2008: [D loss: 0.363500, acc: 0.804688]  [G loss: 1.302844, acc: 0.125000]\n",
      "2009: [D loss: 0.246293, acc: 0.898438]  [G loss: 0.683500, acc: 0.515625]\n",
      "2010: [D loss: 0.304560, acc: 0.882812]  [G loss: 1.415012, acc: 0.281250]\n",
      "2011: [D loss: 0.262385, acc: 0.898438]  [G loss: 1.223898, acc: 0.296875]\n",
      "2012: [D loss: 0.240693, acc: 0.882812]  [G loss: 0.950712, acc: 0.468750]\n",
      "2013: [D loss: 0.324725, acc: 0.843750]  [G loss: 1.684333, acc: 0.234375]\n",
      "2014: [D loss: 0.371954, acc: 0.867188]  [G loss: 1.717565, acc: 0.234375]\n",
      "2015: [D loss: 0.499147, acc: 0.765625]  [G loss: 0.552804, acc: 0.703125]\n",
      "2016: [D loss: 0.540728, acc: 0.742188]  [G loss: 2.232434, acc: 0.109375]\n",
      "2017: [D loss: 0.524328, acc: 0.781250]  [G loss: 1.717819, acc: 0.109375]\n",
      "2018: [D loss: 0.350723, acc: 0.859375]  [G loss: 1.715164, acc: 0.093750]\n",
      "2019: [D loss: 0.397276, acc: 0.828125]  [G loss: 1.517211, acc: 0.140625]\n",
      "2020: [D loss: 0.267495, acc: 0.906250]  [G loss: 1.128747, acc: 0.312500]\n",
      "2021: [D loss: 0.388548, acc: 0.828125]  [G loss: 1.579038, acc: 0.156250]\n",
      "2022: [D loss: 0.386457, acc: 0.835938]  [G loss: 1.114581, acc: 0.328125]\n",
      "2023: [D loss: 0.270599, acc: 0.882812]  [G loss: 0.873261, acc: 0.406250]\n",
      "2024: [D loss: 0.464703, acc: 0.765625]  [G loss: 1.527796, acc: 0.234375]\n",
      "2025: [D loss: 0.303633, acc: 0.843750]  [G loss: 0.618463, acc: 0.625000]\n",
      "2026: [D loss: 0.256080, acc: 0.890625]  [G loss: 1.274891, acc: 0.296875]\n",
      "2027: [D loss: 0.393773, acc: 0.820312]  [G loss: 0.986902, acc: 0.500000]\n",
      "2028: [D loss: 0.387025, acc: 0.843750]  [G loss: 0.702269, acc: 0.578125]\n",
      "2029: [D loss: 0.417158, acc: 0.812500]  [G loss: 1.149630, acc: 0.281250]\n",
      "2030: [D loss: 0.324428, acc: 0.859375]  [G loss: 0.520142, acc: 0.687500]\n",
      "2031: [D loss: 0.278270, acc: 0.890625]  [G loss: 0.956389, acc: 0.390625]\n",
      "2032: [D loss: 0.215262, acc: 0.921875]  [G loss: 0.497513, acc: 0.750000]\n",
      "2033: [D loss: 0.431416, acc: 0.765625]  [G loss: 2.647712, acc: 0.015625]\n",
      "2034: [D loss: 0.438539, acc: 0.765625]  [G loss: 0.404566, acc: 0.812500]\n",
      "2035: [D loss: 0.533550, acc: 0.750000]  [G loss: 1.522912, acc: 0.203125]\n",
      "2036: [D loss: 0.359244, acc: 0.812500]  [G loss: 0.930527, acc: 0.437500]\n",
      "2037: [D loss: 0.298732, acc: 0.835938]  [G loss: 1.374992, acc: 0.203125]\n",
      "2038: [D loss: 0.208034, acc: 0.921875]  [G loss: 1.050260, acc: 0.312500]\n",
      "2039: [D loss: 0.242023, acc: 0.914062]  [G loss: 1.374327, acc: 0.234375]\n",
      "2040: [D loss: 0.277951, acc: 0.882812]  [G loss: 1.128480, acc: 0.281250]\n",
      "2041: [D loss: 0.311914, acc: 0.859375]  [G loss: 1.611423, acc: 0.156250]\n",
      "2042: [D loss: 0.375444, acc: 0.859375]  [G loss: 1.026969, acc: 0.453125]\n",
      "2043: [D loss: 0.356468, acc: 0.843750]  [G loss: 1.212574, acc: 0.218750]\n",
      "2044: [D loss: 0.268864, acc: 0.906250]  [G loss: 1.065027, acc: 0.328125]\n",
      "2045: [D loss: 0.265675, acc: 0.882812]  [G loss: 0.566779, acc: 0.718750]\n",
      "2046: [D loss: 0.354252, acc: 0.859375]  [G loss: 2.607289, acc: 0.046875]\n",
      "2047: [D loss: 0.415308, acc: 0.820312]  [G loss: 0.621587, acc: 0.640625]\n",
      "2048: [D loss: 0.450343, acc: 0.742188]  [G loss: 2.689151, acc: 0.000000]\n",
      "2049: [D loss: 0.314700, acc: 0.867188]  [G loss: 1.351754, acc: 0.156250]\n",
      "2050: [D loss: 0.275370, acc: 0.859375]  [G loss: 2.020686, acc: 0.062500]\n",
      "2051: [D loss: 0.254920, acc: 0.875000]  [G loss: 1.408676, acc: 0.203125]\n",
      "2052: [D loss: 0.404482, acc: 0.828125]  [G loss: 1.841319, acc: 0.031250]\n",
      "2053: [D loss: 0.304895, acc: 0.882812]  [G loss: 1.533920, acc: 0.062500]\n",
      "2054: [D loss: 0.326863, acc: 0.875000]  [G loss: 1.830939, acc: 0.046875]\n",
      "2055: [D loss: 0.245738, acc: 0.890625]  [G loss: 1.279269, acc: 0.156250]\n",
      "2056: [D loss: 0.295755, acc: 0.851562]  [G loss: 1.168320, acc: 0.171875]\n",
      "2057: [D loss: 0.368828, acc: 0.859375]  [G loss: 1.400510, acc: 0.218750]\n",
      "2058: [D loss: 0.265699, acc: 0.882812]  [G loss: 0.740770, acc: 0.562500]\n",
      "2059: [D loss: 0.419052, acc: 0.789062]  [G loss: 3.288659, acc: 0.031250]\n",
      "2060: [D loss: 0.384384, acc: 0.835938]  [G loss: 0.914363, acc: 0.578125]\n",
      "2061: [D loss: 0.611961, acc: 0.703125]  [G loss: 2.305882, acc: 0.046875]\n",
      "2062: [D loss: 0.390172, acc: 0.828125]  [G loss: 0.981770, acc: 0.406250]\n",
      "2063: [D loss: 0.573553, acc: 0.734375]  [G loss: 1.192408, acc: 0.218750]\n",
      "2064: [D loss: 0.323677, acc: 0.867188]  [G loss: 0.873383, acc: 0.421875]\n",
      "2065: [D loss: 0.343233, acc: 0.835938]  [G loss: 0.998330, acc: 0.375000]\n",
      "2066: [D loss: 0.432494, acc: 0.828125]  [G loss: 0.911311, acc: 0.406250]\n",
      "2067: [D loss: 0.401200, acc: 0.812500]  [G loss: 0.725427, acc: 0.593750]\n",
      "2068: [D loss: 0.354210, acc: 0.851562]  [G loss: 1.031164, acc: 0.312500]\n",
      "2069: [D loss: 0.319360, acc: 0.898438]  [G loss: 0.783121, acc: 0.562500]\n",
      "2070: [D loss: 0.373207, acc: 0.851562]  [G loss: 0.956620, acc: 0.453125]\n",
      "2071: [D loss: 0.424366, acc: 0.804688]  [G loss: 1.163038, acc: 0.296875]\n",
      "2072: [D loss: 0.348515, acc: 0.835938]  [G loss: 0.762205, acc: 0.531250]\n",
      "2073: [D loss: 0.382845, acc: 0.773438]  [G loss: 2.019495, acc: 0.140625]\n",
      "2074: [D loss: 0.388052, acc: 0.828125]  [G loss: 0.748254, acc: 0.578125]\n",
      "2075: [D loss: 0.456214, acc: 0.796875]  [G loss: 1.415001, acc: 0.265625]\n",
      "2076: [D loss: 0.349011, acc: 0.859375]  [G loss: 0.858602, acc: 0.484375]\n",
      "2077: [D loss: 0.287846, acc: 0.867188]  [G loss: 1.025442, acc: 0.296875]\n",
      "2078: [D loss: 0.411581, acc: 0.804688]  [G loss: 0.897344, acc: 0.453125]\n",
      "2079: [D loss: 0.364072, acc: 0.796875]  [G loss: 1.435507, acc: 0.218750]\n",
      "2080: [D loss: 0.314688, acc: 0.851562]  [G loss: 0.964202, acc: 0.421875]\n",
      "2081: [D loss: 0.278209, acc: 0.914062]  [G loss: 1.072593, acc: 0.406250]\n",
      "2082: [D loss: 0.398803, acc: 0.843750]  [G loss: 1.302819, acc: 0.234375]\n",
      "2083: [D loss: 0.296752, acc: 0.843750]  [G loss: 1.562880, acc: 0.234375]\n",
      "2084: [D loss: 0.277736, acc: 0.890625]  [G loss: 1.355334, acc: 0.218750]\n",
      "2085: [D loss: 0.322513, acc: 0.835938]  [G loss: 2.320384, acc: 0.046875]\n",
      "2086: [D loss: 0.419622, acc: 0.812500]  [G loss: 0.936516, acc: 0.468750]\n",
      "2087: [D loss: 0.379070, acc: 0.820312]  [G loss: 2.486446, acc: 0.046875]\n",
      "2088: [D loss: 0.449482, acc: 0.773438]  [G loss: 0.584732, acc: 0.625000]\n",
      "2089: [D loss: 0.481833, acc: 0.718750]  [G loss: 2.310546, acc: 0.031250]\n",
      "2090: [D loss: 0.397138, acc: 0.796875]  [G loss: 1.128312, acc: 0.312500]\n",
      "2091: [D loss: 0.400215, acc: 0.820312]  [G loss: 1.149677, acc: 0.218750]\n",
      "2092: [D loss: 0.369471, acc: 0.843750]  [G loss: 1.217239, acc: 0.421875]\n",
      "2093: [D loss: 0.339066, acc: 0.843750]  [G loss: 0.694898, acc: 0.593750]\n",
      "2094: [D loss: 0.411255, acc: 0.796875]  [G loss: 1.484284, acc: 0.218750]\n",
      "2095: [D loss: 0.277159, acc: 0.859375]  [G loss: 0.494915, acc: 0.718750]\n",
      "2096: [D loss: 0.387479, acc: 0.828125]  [G loss: 1.182327, acc: 0.343750]\n",
      "2097: [D loss: 0.366406, acc: 0.851562]  [G loss: 0.694663, acc: 0.656250]\n",
      "2098: [D loss: 0.341886, acc: 0.867188]  [G loss: 1.163035, acc: 0.390625]\n",
      "2099: [D loss: 0.548032, acc: 0.757812]  [G loss: 0.882678, acc: 0.531250]\n",
      "2100: [D loss: 0.328597, acc: 0.875000]  [G loss: 0.908214, acc: 0.390625]\n",
      "2101: [D loss: 0.386664, acc: 0.812500]  [G loss: 0.856514, acc: 0.500000]\n",
      "2102: [D loss: 0.271679, acc: 0.859375]  [G loss: 0.735324, acc: 0.562500]\n",
      "2103: [D loss: 0.317005, acc: 0.835938]  [G loss: 1.079825, acc: 0.359375]\n",
      "2104: [D loss: 0.317437, acc: 0.882812]  [G loss: 0.586500, acc: 0.671875]\n",
      "2105: [D loss: 0.379782, acc: 0.820312]  [G loss: 0.911886, acc: 0.390625]\n",
      "2106: [D loss: 0.395191, acc: 0.851562]  [G loss: 0.465852, acc: 0.765625]\n",
      "2107: [D loss: 0.400498, acc: 0.796875]  [G loss: 2.041165, acc: 0.046875]\n",
      "2108: [D loss: 0.467400, acc: 0.773438]  [G loss: 0.397475, acc: 0.828125]\n",
      "2109: [D loss: 0.452276, acc: 0.773438]  [G loss: 1.364403, acc: 0.171875]\n",
      "2110: [D loss: 0.371456, acc: 0.820312]  [G loss: 0.617709, acc: 0.625000]\n",
      "2111: [D loss: 0.320092, acc: 0.859375]  [G loss: 1.491566, acc: 0.125000]\n",
      "2112: [D loss: 0.315487, acc: 0.875000]  [G loss: 0.518678, acc: 0.703125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2113: [D loss: 0.348009, acc: 0.843750]  [G loss: 1.560971, acc: 0.265625]\n",
      "2114: [D loss: 0.388952, acc: 0.789062]  [G loss: 0.819452, acc: 0.453125]\n",
      "2115: [D loss: 0.355811, acc: 0.835938]  [G loss: 1.343122, acc: 0.265625]\n",
      "2116: [D loss: 0.385171, acc: 0.843750]  [G loss: 0.478949, acc: 0.734375]\n",
      "2117: [D loss: 0.464733, acc: 0.781250]  [G loss: 1.800141, acc: 0.156250]\n",
      "2118: [D loss: 0.327751, acc: 0.851562]  [G loss: 0.696478, acc: 0.640625]\n",
      "2119: [D loss: 0.423906, acc: 0.750000]  [G loss: 1.216564, acc: 0.343750]\n",
      "2120: [D loss: 0.443420, acc: 0.750000]  [G loss: 0.788892, acc: 0.484375]\n",
      "2121: [D loss: 0.322435, acc: 0.843750]  [G loss: 0.716889, acc: 0.500000]\n",
      "2122: [D loss: 0.275339, acc: 0.875000]  [G loss: 0.620595, acc: 0.656250]\n",
      "2123: [D loss: 0.442645, acc: 0.734375]  [G loss: 1.387887, acc: 0.156250]\n",
      "2124: [D loss: 0.353281, acc: 0.820312]  [G loss: 0.813853, acc: 0.468750]\n",
      "2125: [D loss: 0.365462, acc: 0.867188]  [G loss: 0.821233, acc: 0.453125]\n",
      "2126: [D loss: 0.394658, acc: 0.851562]  [G loss: 1.218619, acc: 0.312500]\n",
      "2127: [D loss: 0.285169, acc: 0.882812]  [G loss: 0.821959, acc: 0.468750]\n",
      "2128: [D loss: 0.409729, acc: 0.835938]  [G loss: 0.615277, acc: 0.593750]\n",
      "2129: [D loss: 0.357930, acc: 0.859375]  [G loss: 0.997015, acc: 0.437500]\n",
      "2130: [D loss: 0.425694, acc: 0.781250]  [G loss: 0.846162, acc: 0.468750]\n",
      "2131: [D loss: 0.583739, acc: 0.726562]  [G loss: 1.431341, acc: 0.265625]\n",
      "2132: [D loss: 0.444525, acc: 0.781250]  [G loss: 0.386571, acc: 0.750000]\n",
      "2133: [D loss: 0.533353, acc: 0.726562]  [G loss: 1.513299, acc: 0.156250]\n",
      "2134: [D loss: 0.370967, acc: 0.796875]  [G loss: 0.633498, acc: 0.625000]\n",
      "2135: [D loss: 0.330986, acc: 0.843750]  [G loss: 0.644693, acc: 0.609375]\n",
      "2136: [D loss: 0.328854, acc: 0.843750]  [G loss: 0.488302, acc: 0.718750]\n",
      "2137: [D loss: 0.306125, acc: 0.820312]  [G loss: 0.823874, acc: 0.500000]\n",
      "2138: [D loss: 0.308923, acc: 0.859375]  [G loss: 0.461377, acc: 0.734375]\n",
      "2139: [D loss: 0.459662, acc: 0.765625]  [G loss: 1.651801, acc: 0.171875]\n",
      "2140: [D loss: 0.438438, acc: 0.796875]  [G loss: 0.676310, acc: 0.593750]\n",
      "2141: [D loss: 0.389359, acc: 0.812500]  [G loss: 1.267518, acc: 0.234375]\n",
      "2142: [D loss: 0.423753, acc: 0.781250]  [G loss: 0.630444, acc: 0.609375]\n",
      "2143: [D loss: 0.520313, acc: 0.757812]  [G loss: 1.046693, acc: 0.343750]\n",
      "2144: [D loss: 0.327470, acc: 0.859375]  [G loss: 1.178405, acc: 0.203125]\n",
      "2145: [D loss: 0.422800, acc: 0.789062]  [G loss: 0.707814, acc: 0.515625]\n",
      "2146: [D loss: 0.347283, acc: 0.859375]  [G loss: 1.431461, acc: 0.140625]\n",
      "2147: [D loss: 0.380379, acc: 0.804688]  [G loss: 0.557643, acc: 0.656250]\n",
      "2148: [D loss: 0.440637, acc: 0.789062]  [G loss: 1.131062, acc: 0.296875]\n",
      "2149: [D loss: 0.410006, acc: 0.851562]  [G loss: 0.841726, acc: 0.296875]\n",
      "2150: [D loss: 0.434728, acc: 0.812500]  [G loss: 1.220171, acc: 0.265625]\n",
      "2151: [D loss: 0.411131, acc: 0.804688]  [G loss: 0.770018, acc: 0.546875]\n",
      "2152: [D loss: 0.346945, acc: 0.804688]  [G loss: 1.106389, acc: 0.250000]\n",
      "2153: [D loss: 0.354341, acc: 0.835938]  [G loss: 0.773275, acc: 0.578125]\n",
      "2154: [D loss: 0.296562, acc: 0.859375]  [G loss: 1.051498, acc: 0.375000]\n",
      "2155: [D loss: 0.256354, acc: 0.906250]  [G loss: 1.137921, acc: 0.343750]\n",
      "2156: [D loss: 0.418378, acc: 0.789062]  [G loss: 0.951474, acc: 0.593750]\n",
      "2157: [D loss: 0.270593, acc: 0.921875]  [G loss: 0.739078, acc: 0.546875]\n",
      "2158: [D loss: 0.343356, acc: 0.828125]  [G loss: 1.783720, acc: 0.125000]\n",
      "2159: [D loss: 0.426513, acc: 0.781250]  [G loss: 0.377633, acc: 0.828125]\n",
      "2160: [D loss: 0.561799, acc: 0.765625]  [G loss: 1.233449, acc: 0.265625]\n",
      "2161: [D loss: 0.437887, acc: 0.812500]  [G loss: 0.709503, acc: 0.593750]\n",
      "2162: [D loss: 0.424180, acc: 0.812500]  [G loss: 1.722461, acc: 0.093750]\n",
      "2163: [D loss: 0.475119, acc: 0.781250]  [G loss: 1.083322, acc: 0.328125]\n",
      "2164: [D loss: 0.376440, acc: 0.835938]  [G loss: 0.874657, acc: 0.437500]\n",
      "2165: [D loss: 0.305741, acc: 0.890625]  [G loss: 0.866317, acc: 0.484375]\n",
      "2166: [D loss: 0.281788, acc: 0.867188]  [G loss: 0.430762, acc: 0.781250]\n",
      "2167: [D loss: 0.380018, acc: 0.867188]  [G loss: 1.061883, acc: 0.359375]\n",
      "2168: [D loss: 0.424928, acc: 0.820312]  [G loss: 0.574609, acc: 0.640625]\n",
      "2169: [D loss: 0.290115, acc: 0.882812]  [G loss: 0.702830, acc: 0.562500]\n",
      "2170: [D loss: 0.307849, acc: 0.882812]  [G loss: 1.111168, acc: 0.343750]\n",
      "2171: [D loss: 0.349014, acc: 0.843750]  [G loss: 0.382978, acc: 0.781250]\n",
      "2172: [D loss: 0.448955, acc: 0.859375]  [G loss: 1.808746, acc: 0.125000]\n",
      "2173: [D loss: 0.361663, acc: 0.835938]  [G loss: 0.683339, acc: 0.593750]\n",
      "2174: [D loss: 0.282186, acc: 0.859375]  [G loss: 0.601480, acc: 0.609375]\n",
      "2175: [D loss: 0.262924, acc: 0.890625]  [G loss: 0.570001, acc: 0.687500]\n",
      "2176: [D loss: 0.279755, acc: 0.882812]  [G loss: 0.952585, acc: 0.375000]\n",
      "2177: [D loss: 0.371622, acc: 0.804688]  [G loss: 0.545534, acc: 0.703125]\n",
      "2178: [D loss: 0.191902, acc: 0.929688]  [G loss: 0.457656, acc: 0.765625]\n",
      "2179: [D loss: 0.467554, acc: 0.781250]  [G loss: 2.126131, acc: 0.109375]\n",
      "2180: [D loss: 0.292920, acc: 0.875000]  [G loss: 0.609163, acc: 0.640625]\n",
      "2181: [D loss: 0.259336, acc: 0.882812]  [G loss: 1.221915, acc: 0.281250]\n",
      "2182: [D loss: 0.283210, acc: 0.867188]  [G loss: 0.295168, acc: 0.828125]\n",
      "2183: [D loss: 0.471397, acc: 0.781250]  [G loss: 1.795666, acc: 0.125000]\n",
      "2184: [D loss: 0.415529, acc: 0.820312]  [G loss: 0.437813, acc: 0.796875]\n",
      "2185: [D loss: 0.305578, acc: 0.843750]  [G loss: 1.447478, acc: 0.203125]\n",
      "2186: [D loss: 0.190203, acc: 0.937500]  [G loss: 0.990647, acc: 0.359375]\n",
      "2187: [D loss: 0.345812, acc: 0.843750]  [G loss: 1.393523, acc: 0.296875]\n",
      "2188: [D loss: 0.259858, acc: 0.906250]  [G loss: 0.622954, acc: 0.656250]\n",
      "2189: [D loss: 0.261905, acc: 0.867188]  [G loss: 1.222888, acc: 0.437500]\n",
      "2190: [D loss: 0.272636, acc: 0.890625]  [G loss: 1.161401, acc: 0.312500]\n",
      "2191: [D loss: 0.170292, acc: 0.929688]  [G loss: 0.951733, acc: 0.484375]\n",
      "2192: [D loss: 0.221607, acc: 0.906250]  [G loss: 2.177958, acc: 0.093750]\n",
      "2193: [D loss: 0.257435, acc: 0.859375]  [G loss: 1.507747, acc: 0.281250]\n",
      "2194: [D loss: 0.319116, acc: 0.882812]  [G loss: 0.961506, acc: 0.484375]\n",
      "2195: [D loss: 0.257398, acc: 0.890625]  [G loss: 1.603840, acc: 0.171875]\n",
      "2196: [D loss: 0.252764, acc: 0.882812]  [G loss: 0.748262, acc: 0.656250]\n",
      "2197: [D loss: 0.222617, acc: 0.898438]  [G loss: 1.331985, acc: 0.421875]\n",
      "2198: [D loss: 0.339227, acc: 0.843750]  [G loss: 0.813589, acc: 0.593750]\n",
      "2199: [D loss: 0.487025, acc: 0.828125]  [G loss: 4.029937, acc: 0.031250]\n",
      "2200: [D loss: 0.400187, acc: 0.796875]  [G loss: 0.498814, acc: 0.750000]\n",
      "2201: [D loss: 0.493725, acc: 0.781250]  [G loss: 2.762780, acc: 0.031250]\n",
      "2202: [D loss: 0.271727, acc: 0.875000]  [G loss: 1.223405, acc: 0.281250]\n",
      "2203: [D loss: 0.196933, acc: 0.914062]  [G loss: 1.000294, acc: 0.453125]\n",
      "2204: [D loss: 0.331223, acc: 0.828125]  [G loss: 1.614828, acc: 0.281250]\n",
      "2205: [D loss: 0.330546, acc: 0.828125]  [G loss: 0.949741, acc: 0.500000]\n",
      "2206: [D loss: 0.179733, acc: 0.898438]  [G loss: 0.808356, acc: 0.500000]\n",
      "2207: [D loss: 0.282128, acc: 0.859375]  [G loss: 2.442607, acc: 0.031250]\n",
      "2208: [D loss: 0.287178, acc: 0.851562]  [G loss: 0.879885, acc: 0.515625]\n",
      "2209: [D loss: 0.346487, acc: 0.851562]  [G loss: 1.893393, acc: 0.062500]\n",
      "2210: [D loss: 0.228214, acc: 0.906250]  [G loss: 0.873345, acc: 0.500000]\n",
      "2211: [D loss: 0.231114, acc: 0.898438]  [G loss: 2.426935, acc: 0.000000]\n",
      "2212: [D loss: 0.252048, acc: 0.882812]  [G loss: 1.081788, acc: 0.531250]\n",
      "2213: [D loss: 0.430220, acc: 0.789062]  [G loss: 1.938709, acc: 0.109375]\n",
      "2214: [D loss: 0.314096, acc: 0.867188]  [G loss: 1.475231, acc: 0.046875]\n",
      "2215: [D loss: 0.212945, acc: 0.937500]  [G loss: 1.082934, acc: 0.375000]\n",
      "2216: [D loss: 0.234768, acc: 0.867188]  [G loss: 1.340801, acc: 0.265625]\n",
      "2217: [D loss: 0.230494, acc: 0.890625]  [G loss: 1.029032, acc: 0.375000]\n",
      "2218: [D loss: 0.284491, acc: 0.851562]  [G loss: 1.141918, acc: 0.328125]\n",
      "2219: [D loss: 0.284317, acc: 0.875000]  [G loss: 0.889511, acc: 0.515625]\n",
      "2220: [D loss: 0.224900, acc: 0.882812]  [G loss: 1.243160, acc: 0.250000]\n",
      "2221: [D loss: 0.296485, acc: 0.859375]  [G loss: 1.094777, acc: 0.328125]\n",
      "2222: [D loss: 0.299225, acc: 0.882812]  [G loss: 2.384604, acc: 0.062500]\n",
      "2223: [D loss: 0.405353, acc: 0.789062]  [G loss: 0.402185, acc: 0.796875]\n",
      "2224: [D loss: 0.639181, acc: 0.734375]  [G loss: 2.848870, acc: 0.000000]\n",
      "2225: [D loss: 0.400082, acc: 0.812500]  [G loss: 1.090358, acc: 0.296875]\n",
      "2226: [D loss: 0.450253, acc: 0.804688]  [G loss: 1.553261, acc: 0.125000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2227: [D loss: 0.316376, acc: 0.867188]  [G loss: 1.088446, acc: 0.218750]\n",
      "2228: [D loss: 0.281677, acc: 0.898438]  [G loss: 1.573759, acc: 0.125000]\n",
      "2229: [D loss: 0.245669, acc: 0.898438]  [G loss: 1.108176, acc: 0.296875]\n",
      "2230: [D loss: 0.293760, acc: 0.875000]  [G loss: 1.415675, acc: 0.328125]\n",
      "2231: [D loss: 0.256732, acc: 0.890625]  [G loss: 0.985121, acc: 0.390625]\n",
      "2232: [D loss: 0.365895, acc: 0.859375]  [G loss: 1.370335, acc: 0.218750]\n",
      "2233: [D loss: 0.195288, acc: 0.921875]  [G loss: 0.927488, acc: 0.484375]\n",
      "2234: [D loss: 0.382202, acc: 0.835938]  [G loss: 2.098498, acc: 0.078125]\n",
      "2235: [D loss: 0.445181, acc: 0.820312]  [G loss: 0.576674, acc: 0.656250]\n",
      "2236: [D loss: 0.414066, acc: 0.789062]  [G loss: 2.040434, acc: 0.093750]\n",
      "2237: [D loss: 0.454138, acc: 0.804688]  [G loss: 0.613144, acc: 0.687500]\n",
      "2238: [D loss: 0.437387, acc: 0.796875]  [G loss: 1.405615, acc: 0.281250]\n",
      "2239: [D loss: 0.280059, acc: 0.875000]  [G loss: 0.646191, acc: 0.671875]\n",
      "2240: [D loss: 0.340937, acc: 0.867188]  [G loss: 0.951312, acc: 0.484375]\n",
      "2241: [D loss: 0.264540, acc: 0.875000]  [G loss: 1.211231, acc: 0.390625]\n",
      "2242: [D loss: 0.297381, acc: 0.882812]  [G loss: 0.916838, acc: 0.453125]\n",
      "2243: [D loss: 0.296443, acc: 0.851562]  [G loss: 0.771245, acc: 0.484375]\n",
      "2244: [D loss: 0.350116, acc: 0.828125]  [G loss: 1.050636, acc: 0.406250]\n",
      "2245: [D loss: 0.224853, acc: 0.890625]  [G loss: 0.624467, acc: 0.609375]\n",
      "2246: [D loss: 0.311059, acc: 0.820312]  [G loss: 1.134619, acc: 0.421875]\n",
      "2247: [D loss: 0.336074, acc: 0.851562]  [G loss: 0.969006, acc: 0.406250]\n",
      "2248: [D loss: 0.195006, acc: 0.937500]  [G loss: 1.025521, acc: 0.406250]\n",
      "2249: [D loss: 0.313111, acc: 0.835938]  [G loss: 1.358328, acc: 0.281250]\n",
      "2250: [D loss: 0.230428, acc: 0.898438]  [G loss: 0.747139, acc: 0.546875]\n",
      "2251: [D loss: 0.289203, acc: 0.851562]  [G loss: 1.827870, acc: 0.203125]\n",
      "2252: [D loss: 0.461376, acc: 0.796875]  [G loss: 0.487721, acc: 0.703125]\n",
      "2253: [D loss: 0.467601, acc: 0.757812]  [G loss: 1.690128, acc: 0.203125]\n",
      "2254: [D loss: 0.297306, acc: 0.859375]  [G loss: 1.017193, acc: 0.406250]\n",
      "2255: [D loss: 0.240517, acc: 0.914062]  [G loss: 1.050161, acc: 0.437500]\n",
      "2256: [D loss: 0.214458, acc: 0.882812]  [G loss: 1.437242, acc: 0.265625]\n",
      "2257: [D loss: 0.190329, acc: 0.960938]  [G loss: 1.199481, acc: 0.375000]\n",
      "2258: [D loss: 0.160569, acc: 0.921875]  [G loss: 0.673000, acc: 0.640625]\n",
      "2259: [D loss: 0.363295, acc: 0.828125]  [G loss: 1.523323, acc: 0.312500]\n",
      "2260: [D loss: 0.260322, acc: 0.882812]  [G loss: 0.485874, acc: 0.703125]\n",
      "2261: [D loss: 0.758612, acc: 0.718750]  [G loss: 1.922993, acc: 0.140625]\n",
      "2262: [D loss: 0.498372, acc: 0.726562]  [G loss: 0.686335, acc: 0.593750]\n",
      "2263: [D loss: 0.328969, acc: 0.828125]  [G loss: 1.363098, acc: 0.265625]\n",
      "2264: [D loss: 0.230378, acc: 0.906250]  [G loss: 0.584868, acc: 0.687500]\n",
      "2265: [D loss: 0.363010, acc: 0.828125]  [G loss: 1.862346, acc: 0.171875]\n",
      "2266: [D loss: 0.249026, acc: 0.898438]  [G loss: 0.812177, acc: 0.500000]\n",
      "2267: [D loss: 0.350339, acc: 0.843750]  [G loss: 1.769650, acc: 0.218750]\n",
      "2268: [D loss: 0.194533, acc: 0.906250]  [G loss: 0.666831, acc: 0.593750]\n",
      "2269: [D loss: 0.276047, acc: 0.875000]  [G loss: 1.115597, acc: 0.390625]\n",
      "2270: [D loss: 0.288533, acc: 0.882812]  [G loss: 1.032505, acc: 0.375000]\n",
      "2271: [D loss: 0.269944, acc: 0.906250]  [G loss: 0.770714, acc: 0.546875]\n",
      "2272: [D loss: 0.269313, acc: 0.875000]  [G loss: 0.354371, acc: 0.765625]\n",
      "2273: [D loss: 0.284707, acc: 0.851562]  [G loss: 1.804819, acc: 0.234375]\n",
      "2274: [D loss: 0.266902, acc: 0.851562]  [G loss: 0.545160, acc: 0.718750]\n",
      "2275: [D loss: 0.305213, acc: 0.835938]  [G loss: 1.964689, acc: 0.093750]\n",
      "2276: [D loss: 0.398006, acc: 0.789062]  [G loss: 0.608328, acc: 0.671875]\n",
      "2277: [D loss: 0.288260, acc: 0.859375]  [G loss: 1.497541, acc: 0.218750]\n",
      "2278: [D loss: 0.275203, acc: 0.851562]  [G loss: 0.481317, acc: 0.765625]\n",
      "2279: [D loss: 0.346572, acc: 0.835938]  [G loss: 2.319410, acc: 0.046875]\n",
      "2280: [D loss: 0.378940, acc: 0.828125]  [G loss: 0.402767, acc: 0.750000]\n",
      "2281: [D loss: 0.331132, acc: 0.859375]  [G loss: 2.338958, acc: 0.125000]\n",
      "2282: [D loss: 0.330842, acc: 0.851562]  [G loss: 0.750629, acc: 0.593750]\n",
      "2283: [D loss: 0.181958, acc: 0.929688]  [G loss: 0.533811, acc: 0.703125]\n",
      "2284: [D loss: 0.283858, acc: 0.875000]  [G loss: 1.650111, acc: 0.156250]\n",
      "2285: [D loss: 0.353854, acc: 0.835938]  [G loss: 1.068578, acc: 0.453125]\n",
      "2286: [D loss: 0.258812, acc: 0.882812]  [G loss: 1.220369, acc: 0.296875]\n",
      "2287: [D loss: 0.210359, acc: 0.921875]  [G loss: 1.240236, acc: 0.250000]\n",
      "2288: [D loss: 0.292959, acc: 0.867188]  [G loss: 1.514552, acc: 0.187500]\n",
      "2289: [D loss: 0.138558, acc: 0.960938]  [G loss: 1.308964, acc: 0.296875]\n",
      "2290: [D loss: 0.341028, acc: 0.867188]  [G loss: 1.800921, acc: 0.171875]\n",
      "2291: [D loss: 0.347680, acc: 0.859375]  [G loss: 1.313223, acc: 0.312500]\n",
      "2292: [D loss: 0.218110, acc: 0.906250]  [G loss: 1.329615, acc: 0.296875]\n",
      "2293: [D loss: 0.269718, acc: 0.906250]  [G loss: 1.846476, acc: 0.187500]\n",
      "2294: [D loss: 0.186443, acc: 0.929688]  [G loss: 0.836257, acc: 0.468750]\n",
      "2295: [D loss: 0.146227, acc: 0.929688]  [G loss: 1.678976, acc: 0.187500]\n",
      "2296: [D loss: 0.163243, acc: 0.945312]  [G loss: 1.277266, acc: 0.312500]\n",
      "2297: [D loss: 0.312269, acc: 0.890625]  [G loss: 0.753841, acc: 0.562500]\n",
      "2298: [D loss: 0.219233, acc: 0.937500]  [G loss: 0.784631, acc: 0.546875]\n",
      "2299: [D loss: 0.274407, acc: 0.875000]  [G loss: 0.764325, acc: 0.562500]\n",
      "2300: [D loss: 0.190762, acc: 0.914062]  [G loss: 1.327027, acc: 0.250000]\n",
      "2301: [D loss: 0.361465, acc: 0.835938]  [G loss: 0.348825, acc: 0.859375]\n",
      "2302: [D loss: 0.352734, acc: 0.820312]  [G loss: 3.525288, acc: 0.046875]\n",
      "2303: [D loss: 0.344072, acc: 0.851562]  [G loss: 0.768557, acc: 0.640625]\n",
      "2304: [D loss: 0.317355, acc: 0.859375]  [G loss: 1.325625, acc: 0.250000]\n",
      "2305: [D loss: 0.203564, acc: 0.898438]  [G loss: 0.844015, acc: 0.484375]\n",
      "2306: [D loss: 0.161383, acc: 0.937500]  [G loss: 0.658343, acc: 0.625000]\n",
      "2307: [D loss: 0.178520, acc: 0.945312]  [G loss: 1.681602, acc: 0.203125]\n",
      "2308: [D loss: 0.259430, acc: 0.859375]  [G loss: 0.642628, acc: 0.671875]\n",
      "2309: [D loss: 0.383929, acc: 0.835938]  [G loss: 2.129238, acc: 0.218750]\n",
      "2310: [D loss: 0.425861, acc: 0.796875]  [G loss: 0.396485, acc: 0.781250]\n",
      "2311: [D loss: 0.372543, acc: 0.812500]  [G loss: 2.105074, acc: 0.031250]\n",
      "2312: [D loss: 0.272498, acc: 0.890625]  [G loss: 0.707983, acc: 0.609375]\n",
      "2313: [D loss: 0.281263, acc: 0.890625]  [G loss: 1.260150, acc: 0.281250]\n",
      "2314: [D loss: 0.242544, acc: 0.867188]  [G loss: 0.680062, acc: 0.609375]\n",
      "2315: [D loss: 0.310194, acc: 0.875000]  [G loss: 1.742075, acc: 0.234375]\n",
      "2316: [D loss: 0.250933, acc: 0.882812]  [G loss: 0.936722, acc: 0.437500]\n",
      "2317: [D loss: 0.190478, acc: 0.906250]  [G loss: 1.813926, acc: 0.281250]\n",
      "2318: [D loss: 0.330342, acc: 0.851562]  [G loss: 1.081974, acc: 0.406250]\n",
      "2319: [D loss: 0.364228, acc: 0.828125]  [G loss: 1.680700, acc: 0.250000]\n",
      "2320: [D loss: 0.365440, acc: 0.781250]  [G loss: 0.999336, acc: 0.515625]\n",
      "2321: [D loss: 0.255226, acc: 0.890625]  [G loss: 2.055936, acc: 0.093750]\n",
      "2322: [D loss: 0.264204, acc: 0.898438]  [G loss: 0.739902, acc: 0.546875]\n",
      "2323: [D loss: 0.374746, acc: 0.835938]  [G loss: 2.474803, acc: 0.140625]\n",
      "2324: [D loss: 0.421601, acc: 0.812500]  [G loss: 0.662155, acc: 0.562500]\n",
      "2325: [D loss: 0.504170, acc: 0.796875]  [G loss: 2.436288, acc: 0.031250]\n",
      "2326: [D loss: 0.341331, acc: 0.851562]  [G loss: 1.609473, acc: 0.234375]\n",
      "2327: [D loss: 0.292884, acc: 0.875000]  [G loss: 1.727786, acc: 0.265625]\n",
      "2328: [D loss: 0.229475, acc: 0.906250]  [G loss: 1.464215, acc: 0.218750]\n",
      "2329: [D loss: 0.255479, acc: 0.890625]  [G loss: 1.713911, acc: 0.203125]\n",
      "2330: [D loss: 0.287410, acc: 0.875000]  [G loss: 1.352312, acc: 0.359375]\n",
      "2331: [D loss: 0.278745, acc: 0.859375]  [G loss: 2.177154, acc: 0.093750]\n",
      "2332: [D loss: 0.249967, acc: 0.882812]  [G loss: 0.860916, acc: 0.515625]\n",
      "2333: [D loss: 0.319502, acc: 0.843750]  [G loss: 1.941503, acc: 0.093750]\n",
      "2334: [D loss: 0.310513, acc: 0.851562]  [G loss: 0.712631, acc: 0.562500]\n",
      "2335: [D loss: 0.407460, acc: 0.765625]  [G loss: 2.766641, acc: 0.000000]\n",
      "2336: [D loss: 0.254088, acc: 0.906250]  [G loss: 1.544281, acc: 0.218750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2337: [D loss: 0.281426, acc: 0.859375]  [G loss: 1.047081, acc: 0.375000]\n",
      "2338: [D loss: 0.275325, acc: 0.890625]  [G loss: 2.011240, acc: 0.093750]\n",
      "2339: [D loss: 0.266387, acc: 0.875000]  [G loss: 1.234022, acc: 0.250000]\n",
      "2340: [D loss: 0.253546, acc: 0.882812]  [G loss: 1.822559, acc: 0.156250]\n",
      "2341: [D loss: 0.288124, acc: 0.875000]  [G loss: 0.783753, acc: 0.515625]\n",
      "2342: [D loss: 0.485719, acc: 0.804688]  [G loss: 2.021690, acc: 0.062500]\n",
      "2343: [D loss: 0.403225, acc: 0.820312]  [G loss: 0.623024, acc: 0.640625]\n",
      "2344: [D loss: 0.346292, acc: 0.835938]  [G loss: 2.036626, acc: 0.046875]\n",
      "2345: [D loss: 0.229272, acc: 0.890625]  [G loss: 1.050478, acc: 0.390625]\n",
      "2346: [D loss: 0.352167, acc: 0.843750]  [G loss: 1.141594, acc: 0.390625]\n",
      "2347: [D loss: 0.299557, acc: 0.859375]  [G loss: 0.886529, acc: 0.453125]\n",
      "2348: [D loss: 0.234309, acc: 0.890625]  [G loss: 1.374203, acc: 0.343750]\n",
      "2349: [D loss: 0.273111, acc: 0.898438]  [G loss: 0.619045, acc: 0.609375]\n",
      "2350: [D loss: 0.371236, acc: 0.835938]  [G loss: 1.923968, acc: 0.203125]\n",
      "2351: [D loss: 0.549614, acc: 0.789062]  [G loss: 0.566355, acc: 0.718750]\n",
      "2352: [D loss: 0.414452, acc: 0.781250]  [G loss: 1.428629, acc: 0.218750]\n",
      "2353: [D loss: 0.377029, acc: 0.843750]  [G loss: 1.299227, acc: 0.218750]\n",
      "2354: [D loss: 0.253068, acc: 0.898438]  [G loss: 0.650261, acc: 0.593750]\n",
      "2355: [D loss: 0.315251, acc: 0.882812]  [G loss: 1.190761, acc: 0.203125]\n",
      "2356: [D loss: 0.256222, acc: 0.867188]  [G loss: 0.612001, acc: 0.656250]\n",
      "2357: [D loss: 0.273605, acc: 0.875000]  [G loss: 1.341993, acc: 0.156250]\n",
      "2358: [D loss: 0.346599, acc: 0.843750]  [G loss: 0.713726, acc: 0.531250]\n",
      "2359: [D loss: 0.340662, acc: 0.820312]  [G loss: 1.675771, acc: 0.156250]\n",
      "2360: [D loss: 0.345090, acc: 0.851562]  [G loss: 0.372517, acc: 0.843750]\n",
      "2361: [D loss: 0.477771, acc: 0.789062]  [G loss: 1.293567, acc: 0.343750]\n",
      "2362: [D loss: 0.356206, acc: 0.875000]  [G loss: 0.990329, acc: 0.390625]\n",
      "2363: [D loss: 0.262278, acc: 0.890625]  [G loss: 1.298829, acc: 0.265625]\n",
      "2364: [D loss: 0.283722, acc: 0.867188]  [G loss: 0.883027, acc: 0.468750]\n",
      "2365: [D loss: 0.263945, acc: 0.914062]  [G loss: 0.996042, acc: 0.468750]\n",
      "2366: [D loss: 0.224266, acc: 0.898438]  [G loss: 0.656362, acc: 0.593750]\n",
      "2367: [D loss: 0.175203, acc: 0.937500]  [G loss: 1.249146, acc: 0.343750]\n",
      "2368: [D loss: 0.185424, acc: 0.929688]  [G loss: 0.866385, acc: 0.515625]\n",
      "2369: [D loss: 0.276079, acc: 0.875000]  [G loss: 1.221813, acc: 0.359375]\n",
      "2370: [D loss: 0.231402, acc: 0.898438]  [G loss: 1.501243, acc: 0.281250]\n",
      "2371: [D loss: 0.205643, acc: 0.890625]  [G loss: 1.383017, acc: 0.312500]\n",
      "2372: [D loss: 0.295857, acc: 0.859375]  [G loss: 0.278727, acc: 0.859375]\n",
      "2373: [D loss: 0.452399, acc: 0.796875]  [G loss: 2.590621, acc: 0.109375]\n",
      "2374: [D loss: 0.385383, acc: 0.851562]  [G loss: 0.694999, acc: 0.593750]\n",
      "2375: [D loss: 0.254842, acc: 0.867188]  [G loss: 1.345183, acc: 0.296875]\n",
      "2376: [D loss: 0.232548, acc: 0.914062]  [G loss: 0.708859, acc: 0.656250]\n",
      "2377: [D loss: 0.196079, acc: 0.914062]  [G loss: 0.907792, acc: 0.484375]\n",
      "2378: [D loss: 0.212240, acc: 0.914062]  [G loss: 0.830750, acc: 0.578125]\n",
      "2379: [D loss: 0.219381, acc: 0.898438]  [G loss: 0.397517, acc: 0.812500]\n",
      "2380: [D loss: 0.316563, acc: 0.867188]  [G loss: 0.870236, acc: 0.515625]\n",
      "2381: [D loss: 0.248076, acc: 0.906250]  [G loss: 0.628649, acc: 0.640625]\n",
      "2382: [D loss: 0.142691, acc: 0.945312]  [G loss: 0.779617, acc: 0.609375]\n",
      "2383: [D loss: 0.189173, acc: 0.937500]  [G loss: 0.825155, acc: 0.593750]\n",
      "2384: [D loss: 0.192150, acc: 0.929688]  [G loss: 0.589958, acc: 0.687500]\n",
      "2385: [D loss: 0.232799, acc: 0.929688]  [G loss: 0.741780, acc: 0.593750]\n",
      "2386: [D loss: 0.336144, acc: 0.843750]  [G loss: 0.939675, acc: 0.531250]\n",
      "2387: [D loss: 0.302408, acc: 0.882812]  [G loss: 1.012969, acc: 0.515625]\n",
      "2388: [D loss: 0.240982, acc: 0.906250]  [G loss: 0.528368, acc: 0.781250]\n",
      "2389: [D loss: 0.464411, acc: 0.804688]  [G loss: 2.308249, acc: 0.140625]\n",
      "2390: [D loss: 0.438269, acc: 0.820312]  [G loss: 0.394321, acc: 0.781250]\n",
      "2391: [D loss: 0.609726, acc: 0.781250]  [G loss: 3.061027, acc: 0.015625]\n",
      "2392: [D loss: 0.357814, acc: 0.867188]  [G loss: 0.740474, acc: 0.593750]\n",
      "2393: [D loss: 0.362070, acc: 0.835938]  [G loss: 1.057179, acc: 0.468750]\n",
      "2394: [D loss: 0.206325, acc: 0.906250]  [G loss: 0.362565, acc: 0.812500]\n",
      "2395: [D loss: 0.157155, acc: 0.937500]  [G loss: 0.439153, acc: 0.796875]\n",
      "2396: [D loss: 0.178473, acc: 0.929688]  [G loss: 0.821968, acc: 0.531250]\n",
      "2397: [D loss: 0.372145, acc: 0.812500]  [G loss: 0.839758, acc: 0.484375]\n",
      "2398: [D loss: 0.404572, acc: 0.812500]  [G loss: 1.584407, acc: 0.234375]\n",
      "2399: [D loss: 0.266323, acc: 0.851562]  [G loss: 0.502229, acc: 0.750000]\n",
      "2400: [D loss: 0.371082, acc: 0.859375]  [G loss: 1.049728, acc: 0.406250]\n",
      "2401: [D loss: 0.319711, acc: 0.859375]  [G loss: 0.416398, acc: 0.718750]\n",
      "2402: [D loss: 0.282330, acc: 0.914062]  [G loss: 0.444435, acc: 0.765625]\n",
      "2403: [D loss: 0.242053, acc: 0.890625]  [G loss: 0.732842, acc: 0.593750]\n",
      "2404: [D loss: 0.389367, acc: 0.820312]  [G loss: 0.728957, acc: 0.609375]\n",
      "2405: [D loss: 0.283757, acc: 0.875000]  [G loss: 1.044398, acc: 0.437500]\n",
      "2406: [D loss: 0.396662, acc: 0.765625]  [G loss: 0.687937, acc: 0.656250]\n",
      "2407: [D loss: 0.278828, acc: 0.875000]  [G loss: 0.979348, acc: 0.406250]\n",
      "2408: [D loss: 0.333233, acc: 0.820312]  [G loss: 1.208190, acc: 0.375000]\n",
      "2409: [D loss: 0.264838, acc: 0.890625]  [G loss: 1.430972, acc: 0.296875]\n",
      "2410: [D loss: 0.258094, acc: 0.898438]  [G loss: 0.372116, acc: 0.796875]\n",
      "2411: [D loss: 0.246876, acc: 0.890625]  [G loss: 0.985152, acc: 0.562500]\n",
      "2412: [D loss: 0.285301, acc: 0.882812]  [G loss: 0.769994, acc: 0.593750]\n",
      "2413: [D loss: 0.245115, acc: 0.890625]  [G loss: 0.928054, acc: 0.500000]\n",
      "2414: [D loss: 0.337487, acc: 0.843750]  [G loss: 0.571511, acc: 0.656250]\n",
      "2415: [D loss: 0.273979, acc: 0.890625]  [G loss: 2.690770, acc: 0.062500]\n",
      "2416: [D loss: 0.367393, acc: 0.828125]  [G loss: 0.527254, acc: 0.703125]\n",
      "2417: [D loss: 0.521040, acc: 0.750000]  [G loss: 2.182476, acc: 0.125000]\n",
      "2418: [D loss: 0.370611, acc: 0.812500]  [G loss: 0.621334, acc: 0.578125]\n",
      "2419: [D loss: 0.259486, acc: 0.851562]  [G loss: 1.332695, acc: 0.218750]\n",
      "2420: [D loss: 0.270013, acc: 0.859375]  [G loss: 0.827431, acc: 0.484375]\n",
      "2421: [D loss: 0.271102, acc: 0.898438]  [G loss: 1.078459, acc: 0.359375]\n",
      "2422: [D loss: 0.246927, acc: 0.890625]  [G loss: 0.969021, acc: 0.421875]\n",
      "2423: [D loss: 0.208500, acc: 0.898438]  [G loss: 0.797301, acc: 0.546875]\n",
      "2424: [D loss: 0.259728, acc: 0.898438]  [G loss: 1.318130, acc: 0.203125]\n",
      "2425: [D loss: 0.304015, acc: 0.851562]  [G loss: 0.798449, acc: 0.515625]\n",
      "2426: [D loss: 0.302260, acc: 0.851562]  [G loss: 1.877523, acc: 0.062500]\n",
      "2427: [D loss: 0.317113, acc: 0.867188]  [G loss: 0.864178, acc: 0.453125]\n",
      "2428: [D loss: 0.464860, acc: 0.773438]  [G loss: 2.335620, acc: 0.015625]\n",
      "2429: [D loss: 0.421380, acc: 0.796875]  [G loss: 0.503551, acc: 0.750000]\n",
      "2430: [D loss: 0.450115, acc: 0.765625]  [G loss: 2.246454, acc: 0.093750]\n",
      "2431: [D loss: 0.436871, acc: 0.804688]  [G loss: 1.294755, acc: 0.265625]\n",
      "2432: [D loss: 0.263179, acc: 0.890625]  [G loss: 0.726397, acc: 0.546875]\n",
      "2433: [D loss: 0.276983, acc: 0.890625]  [G loss: 1.136039, acc: 0.250000]\n",
      "2434: [D loss: 0.311926, acc: 0.851562]  [G loss: 0.866558, acc: 0.453125]\n",
      "2435: [D loss: 0.266476, acc: 0.882812]  [G loss: 1.651217, acc: 0.093750]\n",
      "2436: [D loss: 0.424404, acc: 0.804688]  [G loss: 0.522233, acc: 0.703125]\n",
      "2437: [D loss: 0.409786, acc: 0.781250]  [G loss: 1.370485, acc: 0.218750]\n",
      "2438: [D loss: 0.315589, acc: 0.859375]  [G loss: 0.804957, acc: 0.468750]\n",
      "2439: [D loss: 0.229059, acc: 0.921875]  [G loss: 0.756556, acc: 0.531250]\n",
      "2440: [D loss: 0.302707, acc: 0.851562]  [G loss: 1.206290, acc: 0.281250]\n",
      "2441: [D loss: 0.278296, acc: 0.882812]  [G loss: 1.140783, acc: 0.468750]\n",
      "2442: [D loss: 0.190500, acc: 0.921875]  [G loss: 0.846814, acc: 0.500000]\n",
      "2443: [D loss: 0.409928, acc: 0.812500]  [G loss: 1.749485, acc: 0.312500]\n",
      "2444: [D loss: 0.383783, acc: 0.796875]  [G loss: 0.263301, acc: 0.875000]\n",
      "2445: [D loss: 0.768495, acc: 0.718750]  [G loss: 2.791918, acc: 0.000000]\n",
      "2446: [D loss: 0.488734, acc: 0.781250]  [G loss: 0.599675, acc: 0.671875]\n",
      "2447: [D loss: 0.459753, acc: 0.789062]  [G loss: 1.335719, acc: 0.390625]\n",
      "2448: [D loss: 0.445745, acc: 0.835938]  [G loss: 0.635839, acc: 0.593750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2449: [D loss: 0.351478, acc: 0.851562]  [G loss: 0.827358, acc: 0.453125]\n",
      "2450: [D loss: 0.451795, acc: 0.773438]  [G loss: 0.787829, acc: 0.546875]\n",
      "2451: [D loss: 0.318899, acc: 0.851562]  [G loss: 0.709665, acc: 0.625000]\n",
      "2452: [D loss: 0.420582, acc: 0.812500]  [G loss: 0.592488, acc: 0.656250]\n",
      "2453: [D loss: 0.400331, acc: 0.765625]  [G loss: 1.031092, acc: 0.312500]\n",
      "2454: [D loss: 0.327017, acc: 0.867188]  [G loss: 0.763945, acc: 0.546875]\n",
      "2455: [D loss: 0.294673, acc: 0.867188]  [G loss: 0.860084, acc: 0.578125]\n",
      "2456: [D loss: 0.289806, acc: 0.882812]  [G loss: 1.052786, acc: 0.390625]\n",
      "2457: [D loss: 0.369563, acc: 0.796875]  [G loss: 0.298888, acc: 0.890625]\n",
      "2458: [D loss: 0.265844, acc: 0.867188]  [G loss: 0.890090, acc: 0.468750]\n",
      "2459: [D loss: 0.305321, acc: 0.882812]  [G loss: 1.387976, acc: 0.187500]\n",
      "2460: [D loss: 0.319616, acc: 0.835938]  [G loss: 0.455229, acc: 0.750000]\n",
      "2461: [D loss: 0.337750, acc: 0.835938]  [G loss: 1.565785, acc: 0.156250]\n",
      "2462: [D loss: 0.284071, acc: 0.867188]  [G loss: 0.701548, acc: 0.562500]\n",
      "2463: [D loss: 0.390683, acc: 0.812500]  [G loss: 2.048799, acc: 0.062500]\n",
      "2464: [D loss: 0.487174, acc: 0.789062]  [G loss: 0.617299, acc: 0.656250]\n",
      "2465: [D loss: 0.359208, acc: 0.875000]  [G loss: 1.666253, acc: 0.140625]\n",
      "2466: [D loss: 0.228608, acc: 0.890625]  [G loss: 1.116666, acc: 0.265625]\n",
      "2467: [D loss: 0.346888, acc: 0.843750]  [G loss: 1.108515, acc: 0.296875]\n",
      "2468: [D loss: 0.326877, acc: 0.851562]  [G loss: 1.672095, acc: 0.171875]\n",
      "2469: [D loss: 0.296472, acc: 0.882812]  [G loss: 0.637494, acc: 0.640625]\n",
      "2470: [D loss: 0.355213, acc: 0.882812]  [G loss: 2.239159, acc: 0.031250]\n",
      "2471: [D loss: 0.314389, acc: 0.851562]  [G loss: 0.660231, acc: 0.531250]\n",
      "2472: [D loss: 0.329925, acc: 0.835938]  [G loss: 0.843463, acc: 0.531250]\n",
      "2473: [D loss: 0.244126, acc: 0.882812]  [G loss: 0.867715, acc: 0.500000]\n",
      "2474: [D loss: 0.367639, acc: 0.835938]  [G loss: 1.113171, acc: 0.359375]\n",
      "2475: [D loss: 0.256432, acc: 0.882812]  [G loss: 1.053756, acc: 0.421875]\n",
      "2476: [D loss: 0.349202, acc: 0.875000]  [G loss: 1.141585, acc: 0.359375]\n",
      "2477: [D loss: 0.292431, acc: 0.851562]  [G loss: 1.558378, acc: 0.281250]\n",
      "2478: [D loss: 0.277835, acc: 0.875000]  [G loss: 0.907139, acc: 0.515625]\n",
      "2479: [D loss: 0.217912, acc: 0.890625]  [G loss: 0.688538, acc: 0.671875]\n",
      "2480: [D loss: 0.234934, acc: 0.921875]  [G loss: 1.328179, acc: 0.390625]\n",
      "2481: [D loss: 0.255974, acc: 0.890625]  [G loss: 0.450049, acc: 0.734375]\n",
      "2482: [D loss: 0.261573, acc: 0.843750]  [G loss: 2.365529, acc: 0.125000]\n",
      "2483: [D loss: 0.313432, acc: 0.875000]  [G loss: 0.562837, acc: 0.671875]\n",
      "2484: [D loss: 0.442026, acc: 0.812500]  [G loss: 1.907140, acc: 0.125000]\n",
      "2485: [D loss: 0.401935, acc: 0.796875]  [G loss: 0.333521, acc: 0.812500]\n",
      "2486: [D loss: 0.443735, acc: 0.726562]  [G loss: 2.579758, acc: 0.093750]\n",
      "2487: [D loss: 0.308549, acc: 0.890625]  [G loss: 1.160555, acc: 0.406250]\n",
      "2488: [D loss: 0.343153, acc: 0.851562]  [G loss: 1.031090, acc: 0.328125]\n",
      "2489: [D loss: 0.212480, acc: 0.945312]  [G loss: 1.075572, acc: 0.468750]\n",
      "2490: [D loss: 0.330501, acc: 0.843750]  [G loss: 0.694592, acc: 0.609375]\n",
      "2491: [D loss: 0.280475, acc: 0.875000]  [G loss: 0.824554, acc: 0.515625]\n",
      "2492: [D loss: 0.276140, acc: 0.859375]  [G loss: 0.946592, acc: 0.484375]\n",
      "2493: [D loss: 0.223871, acc: 0.898438]  [G loss: 0.971676, acc: 0.484375]\n",
      "2494: [D loss: 0.200667, acc: 0.937500]  [G loss: 0.825986, acc: 0.656250]\n",
      "2495: [D loss: 0.279919, acc: 0.875000]  [G loss: 2.180287, acc: 0.078125]\n",
      "2496: [D loss: 0.255255, acc: 0.890625]  [G loss: 0.707272, acc: 0.625000]\n",
      "2497: [D loss: 0.251910, acc: 0.882812]  [G loss: 1.209355, acc: 0.390625]\n",
      "2498: [D loss: 0.265949, acc: 0.898438]  [G loss: 1.922135, acc: 0.171875]\n",
      "2499: [D loss: 0.272555, acc: 0.867188]  [G loss: 0.348195, acc: 0.828125]\n",
      "2500: [D loss: 0.469911, acc: 0.742188]  [G loss: 2.657810, acc: 0.031250]\n",
      "2501: [D loss: 0.544278, acc: 0.804688]  [G loss: 0.599748, acc: 0.593750]\n",
      "2502: [D loss: 0.532165, acc: 0.765625]  [G loss: 2.226468, acc: 0.093750]\n",
      "2503: [D loss: 0.271401, acc: 0.875000]  [G loss: 1.071028, acc: 0.421875]\n",
      "2504: [D loss: 0.348355, acc: 0.867188]  [G loss: 1.256883, acc: 0.343750]\n",
      "2505: [D loss: 0.276732, acc: 0.859375]  [G loss: 1.002386, acc: 0.421875]\n",
      "2506: [D loss: 0.318280, acc: 0.843750]  [G loss: 1.421396, acc: 0.375000]\n",
      "2507: [D loss: 0.322466, acc: 0.906250]  [G loss: 0.726187, acc: 0.531250]\n",
      "2508: [D loss: 0.401687, acc: 0.804688]  [G loss: 1.687199, acc: 0.312500]\n",
      "2509: [D loss: 0.254662, acc: 0.898438]  [G loss: 1.056539, acc: 0.531250]\n",
      "2510: [D loss: 0.300875, acc: 0.890625]  [G loss: 1.173643, acc: 0.390625]\n",
      "2511: [D loss: 0.267055, acc: 0.875000]  [G loss: 1.850779, acc: 0.218750]\n",
      "2512: [D loss: 0.390459, acc: 0.828125]  [G loss: 0.497790, acc: 0.718750]\n",
      "2513: [D loss: 0.683747, acc: 0.687500]  [G loss: 2.796155, acc: 0.093750]\n",
      "2514: [D loss: 0.486459, acc: 0.820312]  [G loss: 1.320489, acc: 0.343750]\n",
      "2515: [D loss: 0.339418, acc: 0.835938]  [G loss: 1.523106, acc: 0.234375]\n",
      "2516: [D loss: 0.325053, acc: 0.835938]  [G loss: 1.204142, acc: 0.359375]\n",
      "2517: [D loss: 0.301569, acc: 0.875000]  [G loss: 1.260627, acc: 0.265625]\n",
      "2518: [D loss: 0.378019, acc: 0.835938]  [G loss: 1.053268, acc: 0.406250]\n",
      "2519: [D loss: 0.192177, acc: 0.960938]  [G loss: 0.996522, acc: 0.484375]\n",
      "2520: [D loss: 0.302599, acc: 0.859375]  [G loss: 1.458644, acc: 0.296875]\n",
      "2521: [D loss: 0.279885, acc: 0.906250]  [G loss: 0.630707, acc: 0.703125]\n",
      "2522: [D loss: 0.282678, acc: 0.890625]  [G loss: 1.537209, acc: 0.234375]\n",
      "2523: [D loss: 0.198907, acc: 0.921875]  [G loss: 0.353951, acc: 0.828125]\n",
      "2524: [D loss: 0.344611, acc: 0.843750]  [G loss: 1.415387, acc: 0.343750]\n",
      "2525: [D loss: 0.405438, acc: 0.812500]  [G loss: 0.323398, acc: 0.875000]\n",
      "2526: [D loss: 0.244727, acc: 0.914062]  [G loss: 0.931026, acc: 0.500000]\n",
      "2527: [D loss: 0.304554, acc: 0.906250]  [G loss: 0.806301, acc: 0.562500]\n",
      "2528: [D loss: 0.242138, acc: 0.898438]  [G loss: 0.971051, acc: 0.531250]\n",
      "2529: [D loss: 0.375936, acc: 0.812500]  [G loss: 1.425072, acc: 0.375000]\n",
      "2530: [D loss: 0.309779, acc: 0.835938]  [G loss: 1.251634, acc: 0.500000]\n",
      "2531: [D loss: 0.347661, acc: 0.835938]  [G loss: 0.455857, acc: 0.718750]\n",
      "2532: [D loss: 0.519957, acc: 0.765625]  [G loss: 2.338961, acc: 0.062500]\n",
      "2533: [D loss: 0.365818, acc: 0.828125]  [G loss: 0.884780, acc: 0.515625]\n",
      "2534: [D loss: 0.406811, acc: 0.812500]  [G loss: 1.057244, acc: 0.453125]\n",
      "2535: [D loss: 0.281219, acc: 0.898438]  [G loss: 1.125850, acc: 0.406250]\n",
      "2536: [D loss: 0.332822, acc: 0.835938]  [G loss: 0.698211, acc: 0.609375]\n",
      "2537: [D loss: 0.239096, acc: 0.890625]  [G loss: 1.769813, acc: 0.078125]\n",
      "2538: [D loss: 0.300756, acc: 0.851562]  [G loss: 1.156737, acc: 0.312500]\n",
      "2539: [D loss: 0.272910, acc: 0.882812]  [G loss: 1.429605, acc: 0.250000]\n",
      "2540: [D loss: 0.221525, acc: 0.898438]  [G loss: 1.309757, acc: 0.375000]\n",
      "2541: [D loss: 0.234241, acc: 0.898438]  [G loss: 1.311149, acc: 0.281250]\n",
      "2542: [D loss: 0.211791, acc: 0.867188]  [G loss: 0.492211, acc: 0.750000]\n",
      "2543: [D loss: 0.160706, acc: 0.929688]  [G loss: 0.798396, acc: 0.593750]\n",
      "2544: [D loss: 0.271902, acc: 0.882812]  [G loss: 2.167999, acc: 0.281250]\n",
      "2545: [D loss: 0.419151, acc: 0.851562]  [G loss: 0.697306, acc: 0.640625]\n",
      "2546: [D loss: 0.341194, acc: 0.835938]  [G loss: 3.155395, acc: 0.000000]\n",
      "2547: [D loss: 0.370080, acc: 0.851562]  [G loss: 0.919078, acc: 0.484375]\n",
      "2548: [D loss: 0.248294, acc: 0.906250]  [G loss: 2.216720, acc: 0.046875]\n",
      "2549: [D loss: 0.251062, acc: 0.867188]  [G loss: 1.088873, acc: 0.437500]\n",
      "2550: [D loss: 0.309888, acc: 0.843750]  [G loss: 2.026826, acc: 0.156250]\n",
      "2551: [D loss: 0.259213, acc: 0.898438]  [G loss: 0.717056, acc: 0.593750]\n",
      "2552: [D loss: 0.354260, acc: 0.867188]  [G loss: 1.495048, acc: 0.406250]\n",
      "2553: [D loss: 0.392889, acc: 0.781250]  [G loss: 1.680030, acc: 0.187500]\n",
      "2554: [D loss: 0.277740, acc: 0.859375]  [G loss: 1.127405, acc: 0.437500]\n",
      "2555: [D loss: 0.209906, acc: 0.914062]  [G loss: 0.960421, acc: 0.531250]\n",
      "2556: [D loss: 0.351917, acc: 0.843750]  [G loss: 1.441005, acc: 0.281250]\n",
      "2557: [D loss: 0.310023, acc: 0.859375]  [G loss: 1.443172, acc: 0.343750]\n",
      "2558: [D loss: 0.278244, acc: 0.906250]  [G loss: 0.842105, acc: 0.546875]\n",
      "2559: [D loss: 0.333878, acc: 0.828125]  [G loss: 1.409578, acc: 0.406250]\n",
      "2560: [D loss: 0.453145, acc: 0.757812]  [G loss: 0.789429, acc: 0.578125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2561: [D loss: 0.331744, acc: 0.851562]  [G loss: 1.799893, acc: 0.187500]\n",
      "2562: [D loss: 0.305699, acc: 0.851562]  [G loss: 0.762013, acc: 0.578125]\n",
      "2563: [D loss: 0.260959, acc: 0.890625]  [G loss: 1.544773, acc: 0.187500]\n",
      "2564: [D loss: 0.280567, acc: 0.851562]  [G loss: 1.136670, acc: 0.515625]\n",
      "2565: [D loss: 0.302095, acc: 0.867188]  [G loss: 1.315529, acc: 0.250000]\n",
      "2566: [D loss: 0.292627, acc: 0.859375]  [G loss: 1.058949, acc: 0.500000]\n",
      "2567: [D loss: 0.259886, acc: 0.890625]  [G loss: 1.118218, acc: 0.343750]\n",
      "2568: [D loss: 0.231191, acc: 0.906250]  [G loss: 0.901667, acc: 0.484375]\n",
      "2569: [D loss: 0.296080, acc: 0.875000]  [G loss: 1.272406, acc: 0.437500]\n",
      "2570: [D loss: 0.269296, acc: 0.875000]  [G loss: 0.589262, acc: 0.687500]\n",
      "2571: [D loss: 0.495322, acc: 0.789062]  [G loss: 3.271682, acc: 0.000000]\n",
      "2572: [D loss: 0.645555, acc: 0.734375]  [G loss: 0.348033, acc: 0.843750]\n",
      "2573: [D loss: 0.751990, acc: 0.695312]  [G loss: 1.423499, acc: 0.343750]\n",
      "2574: [D loss: 0.415377, acc: 0.820312]  [G loss: 0.715220, acc: 0.609375]\n",
      "2575: [D loss: 0.271464, acc: 0.859375]  [G loss: 0.906739, acc: 0.500000]\n",
      "2576: [D loss: 0.357882, acc: 0.828125]  [G loss: 1.212546, acc: 0.250000]\n",
      "2577: [D loss: 0.317044, acc: 0.875000]  [G loss: 0.982915, acc: 0.406250]\n",
      "2578: [D loss: 0.310243, acc: 0.851562]  [G loss: 0.734347, acc: 0.593750]\n",
      "2579: [D loss: 0.347155, acc: 0.835938]  [G loss: 0.524090, acc: 0.703125]\n",
      "2580: [D loss: 0.254547, acc: 0.890625]  [G loss: 1.242932, acc: 0.281250]\n",
      "2581: [D loss: 0.292507, acc: 0.875000]  [G loss: 1.504213, acc: 0.265625]\n",
      "2582: [D loss: 0.381849, acc: 0.781250]  [G loss: 0.232993, acc: 0.890625]\n",
      "2583: [D loss: 0.389614, acc: 0.843750]  [G loss: 1.269623, acc: 0.281250]\n",
      "2584: [D loss: 0.410348, acc: 0.796875]  [G loss: 0.921105, acc: 0.468750]\n",
      "2585: [D loss: 0.290783, acc: 0.875000]  [G loss: 1.935805, acc: 0.093750]\n",
      "2586: [D loss: 0.298648, acc: 0.867188]  [G loss: 0.823258, acc: 0.500000]\n",
      "2587: [D loss: 0.336932, acc: 0.835938]  [G loss: 1.542692, acc: 0.203125]\n",
      "2588: [D loss: 0.308279, acc: 0.843750]  [G loss: 0.919963, acc: 0.484375]\n",
      "2589: [D loss: 0.319061, acc: 0.867188]  [G loss: 1.022809, acc: 0.406250]\n",
      "2590: [D loss: 0.284843, acc: 0.875000]  [G loss: 0.650239, acc: 0.593750]\n",
      "2591: [D loss: 0.363441, acc: 0.835938]  [G loss: 1.843405, acc: 0.171875]\n",
      "2592: [D loss: 0.342530, acc: 0.828125]  [G loss: 0.779962, acc: 0.546875]\n",
      "2593: [D loss: 0.323406, acc: 0.843750]  [G loss: 1.658488, acc: 0.140625]\n",
      "2594: [D loss: 0.217445, acc: 0.906250]  [G loss: 0.789888, acc: 0.546875]\n",
      "2595: [D loss: 0.292207, acc: 0.890625]  [G loss: 1.812014, acc: 0.156250]\n",
      "2596: [D loss: 0.302893, acc: 0.851562]  [G loss: 0.654947, acc: 0.625000]\n",
      "2597: [D loss: 0.311074, acc: 0.851562]  [G loss: 1.155503, acc: 0.375000]\n",
      "2598: [D loss: 0.347171, acc: 0.843750]  [G loss: 0.974140, acc: 0.484375]\n",
      "2599: [D loss: 0.288695, acc: 0.875000]  [G loss: 1.324173, acc: 0.187500]\n",
      "2600: [D loss: 0.314010, acc: 0.851562]  [G loss: 0.348475, acc: 0.843750]\n",
      "2601: [D loss: 0.394633, acc: 0.820312]  [G loss: 1.592903, acc: 0.265625]\n",
      "2602: [D loss: 0.382163, acc: 0.828125]  [G loss: 0.806299, acc: 0.500000]\n",
      "2603: [D loss: 0.406095, acc: 0.835938]  [G loss: 0.876061, acc: 0.500000]\n",
      "2604: [D loss: 0.235144, acc: 0.929688]  [G loss: 0.618464, acc: 0.625000]\n",
      "2605: [D loss: 0.321768, acc: 0.890625]  [G loss: 0.999580, acc: 0.421875]\n",
      "2606: [D loss: 0.326306, acc: 0.835938]  [G loss: 0.987186, acc: 0.406250]\n",
      "2607: [D loss: 0.312411, acc: 0.843750]  [G loss: 0.676231, acc: 0.609375]\n",
      "2608: [D loss: 0.338379, acc: 0.828125]  [G loss: 1.014545, acc: 0.437500]\n",
      "2609: [D loss: 0.244290, acc: 0.898438]  [G loss: 0.682857, acc: 0.593750]\n",
      "2610: [D loss: 0.423907, acc: 0.789062]  [G loss: 2.546743, acc: 0.062500]\n",
      "2611: [D loss: 0.467024, acc: 0.789062]  [G loss: 0.317278, acc: 0.843750]\n",
      "2612: [D loss: 0.653956, acc: 0.726562]  [G loss: 2.157637, acc: 0.109375]\n",
      "2613: [D loss: 0.425484, acc: 0.804688]  [G loss: 0.920082, acc: 0.453125]\n",
      "2614: [D loss: 0.286287, acc: 0.875000]  [G loss: 1.382281, acc: 0.156250]\n",
      "2615: [D loss: 0.264804, acc: 0.921875]  [G loss: 0.786717, acc: 0.500000]\n",
      "2616: [D loss: 0.312819, acc: 0.875000]  [G loss: 1.085493, acc: 0.343750]\n",
      "2617: [D loss: 0.269390, acc: 0.914062]  [G loss: 1.162688, acc: 0.359375]\n",
      "2618: [D loss: 0.258054, acc: 0.875000]  [G loss: 1.129423, acc: 0.359375]\n",
      "2619: [D loss: 0.272095, acc: 0.875000]  [G loss: 1.174997, acc: 0.312500]\n",
      "2620: [D loss: 0.322743, acc: 0.875000]  [G loss: 1.032754, acc: 0.406250]\n",
      "2621: [D loss: 0.408236, acc: 0.875000]  [G loss: 1.312594, acc: 0.375000]\n",
      "2622: [D loss: 0.236905, acc: 0.898438]  [G loss: 0.492332, acc: 0.734375]\n",
      "2623: [D loss: 0.465578, acc: 0.781250]  [G loss: 3.673946, acc: 0.000000]\n",
      "2624: [D loss: 0.498655, acc: 0.765625]  [G loss: 1.205367, acc: 0.375000]\n",
      "2625: [D loss: 0.325665, acc: 0.835938]  [G loss: 1.815605, acc: 0.125000]\n",
      "2626: [D loss: 0.312502, acc: 0.875000]  [G loss: 1.185143, acc: 0.312500]\n",
      "2627: [D loss: 0.318780, acc: 0.882812]  [G loss: 1.148649, acc: 0.312500]\n",
      "2628: [D loss: 0.252234, acc: 0.882812]  [G loss: 0.873695, acc: 0.500000]\n",
      "2629: [D loss: 0.342339, acc: 0.820312]  [G loss: 1.962956, acc: 0.125000]\n",
      "2630: [D loss: 0.395059, acc: 0.820312]  [G loss: 0.801811, acc: 0.531250]\n",
      "2631: [D loss: 0.393630, acc: 0.773438]  [G loss: 1.405756, acc: 0.265625]\n",
      "2632: [D loss: 0.469280, acc: 0.742188]  [G loss: 0.848283, acc: 0.531250]\n",
      "2633: [D loss: 0.309214, acc: 0.851562]  [G loss: 1.237230, acc: 0.296875]\n",
      "2634: [D loss: 0.238175, acc: 0.906250]  [G loss: 1.124103, acc: 0.359375]\n",
      "2635: [D loss: 0.327355, acc: 0.835938]  [G loss: 1.314128, acc: 0.343750]\n",
      "2636: [D loss: 0.415098, acc: 0.820312]  [G loss: 0.865989, acc: 0.515625]\n",
      "2637: [D loss: 0.240509, acc: 0.921875]  [G loss: 0.432237, acc: 0.859375]\n",
      "2638: [D loss: 0.347216, acc: 0.835938]  [G loss: 1.676742, acc: 0.156250]\n",
      "2639: [D loss: 0.419511, acc: 0.812500]  [G loss: 0.469552, acc: 0.718750]\n",
      "2640: [D loss: 0.454119, acc: 0.812500]  [G loss: 1.450289, acc: 0.390625]\n",
      "2641: [D loss: 0.413125, acc: 0.812500]  [G loss: 0.773610, acc: 0.500000]\n",
      "2642: [D loss: 0.407600, acc: 0.796875]  [G loss: 1.105309, acc: 0.343750]\n",
      "2643: [D loss: 0.334263, acc: 0.851562]  [G loss: 0.588556, acc: 0.656250]\n",
      "2644: [D loss: 0.350332, acc: 0.804688]  [G loss: 1.109709, acc: 0.312500]\n",
      "2645: [D loss: 0.353377, acc: 0.828125]  [G loss: 0.624340, acc: 0.609375]\n",
      "2646: [D loss: 0.329469, acc: 0.851562]  [G loss: 1.112356, acc: 0.343750]\n",
      "2647: [D loss: 0.377017, acc: 0.796875]  [G loss: 0.731017, acc: 0.593750]\n",
      "2648: [D loss: 0.366494, acc: 0.812500]  [G loss: 0.922577, acc: 0.406250]\n",
      "2649: [D loss: 0.366414, acc: 0.828125]  [G loss: 0.955076, acc: 0.453125]\n",
      "2650: [D loss: 0.324179, acc: 0.859375]  [G loss: 0.510348, acc: 0.687500]\n",
      "2651: [D loss: 0.341659, acc: 0.867188]  [G loss: 1.926779, acc: 0.140625]\n",
      "2652: [D loss: 0.425880, acc: 0.812500]  [G loss: 0.460324, acc: 0.765625]\n",
      "2653: [D loss: 0.295144, acc: 0.867188]  [G loss: 1.583547, acc: 0.234375]\n",
      "2654: [D loss: 0.351809, acc: 0.875000]  [G loss: 0.614896, acc: 0.656250]\n",
      "2655: [D loss: 0.300465, acc: 0.859375]  [G loss: 1.387568, acc: 0.234375]\n",
      "2656: [D loss: 0.253406, acc: 0.890625]  [G loss: 1.176839, acc: 0.359375]\n",
      "2657: [D loss: 0.283792, acc: 0.890625]  [G loss: 1.226764, acc: 0.312500]\n",
      "2658: [D loss: 0.245253, acc: 0.921875]  [G loss: 0.661542, acc: 0.562500]\n",
      "2659: [D loss: 0.240797, acc: 0.914062]  [G loss: 1.477126, acc: 0.250000]\n",
      "2660: [D loss: 0.277145, acc: 0.828125]  [G loss: 0.810642, acc: 0.546875]\n",
      "2661: [D loss: 0.435512, acc: 0.781250]  [G loss: 2.896804, acc: 0.031250]\n",
      "2662: [D loss: 0.338404, acc: 0.843750]  [G loss: 0.565994, acc: 0.718750]\n",
      "2663: [D loss: 0.495152, acc: 0.757812]  [G loss: 2.135889, acc: 0.109375]\n",
      "2664: [D loss: 0.290689, acc: 0.890625]  [G loss: 1.023063, acc: 0.390625]\n",
      "2665: [D loss: 0.307207, acc: 0.867188]  [G loss: 1.622405, acc: 0.171875]\n",
      "2666: [D loss: 0.296012, acc: 0.875000]  [G loss: 0.820797, acc: 0.531250]\n",
      "2667: [D loss: 0.274067, acc: 0.875000]  [G loss: 0.875876, acc: 0.500000]\n",
      "2668: [D loss: 0.329680, acc: 0.859375]  [G loss: 0.752307, acc: 0.609375]\n",
      "2669: [D loss: 0.271935, acc: 0.875000]  [G loss: 0.991281, acc: 0.484375]\n",
      "2670: [D loss: 0.179602, acc: 0.929688]  [G loss: 0.664531, acc: 0.703125]\n",
      "2671: [D loss: 0.198546, acc: 0.937500]  [G loss: 0.651636, acc: 0.640625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2672: [D loss: 0.303513, acc: 0.867188]  [G loss: 0.494631, acc: 0.796875]\n",
      "2673: [D loss: 0.339877, acc: 0.851562]  [G loss: 1.005543, acc: 0.484375]\n",
      "2674: [D loss: 0.255669, acc: 0.867188]  [G loss: 0.524883, acc: 0.703125]\n",
      "2675: [D loss: 0.315461, acc: 0.828125]  [G loss: 1.031510, acc: 0.453125]\n",
      "2676: [D loss: 0.332891, acc: 0.859375]  [G loss: 0.618813, acc: 0.671875]\n",
      "2677: [D loss: 0.273163, acc: 0.867188]  [G loss: 1.968487, acc: 0.125000]\n",
      "2678: [D loss: 0.413050, acc: 0.796875]  [G loss: 0.196810, acc: 0.906250]\n",
      "2679: [D loss: 0.516561, acc: 0.773438]  [G loss: 2.029505, acc: 0.171875]\n",
      "2680: [D loss: 0.371413, acc: 0.820312]  [G loss: 1.162864, acc: 0.359375]\n",
      "2681: [D loss: 0.286243, acc: 0.851562]  [G loss: 0.766792, acc: 0.546875]\n",
      "2682: [D loss: 0.254350, acc: 0.898438]  [G loss: 1.314267, acc: 0.296875]\n",
      "2683: [D loss: 0.244795, acc: 0.914062]  [G loss: 0.713419, acc: 0.671875]\n",
      "2684: [D loss: 0.178472, acc: 0.937500]  [G loss: 0.628732, acc: 0.640625]\n",
      "2685: [D loss: 0.150364, acc: 0.937500]  [G loss: 0.339432, acc: 0.875000]\n",
      "2686: [D loss: 0.513766, acc: 0.781250]  [G loss: 2.244539, acc: 0.062500]\n",
      "2687: [D loss: 0.631592, acc: 0.742188]  [G loss: 0.491393, acc: 0.671875]\n",
      "2688: [D loss: 0.440424, acc: 0.789062]  [G loss: 1.576844, acc: 0.156250]\n",
      "2689: [D loss: 0.248477, acc: 0.875000]  [G loss: 0.727023, acc: 0.531250]\n",
      "2690: [D loss: 0.319005, acc: 0.859375]  [G loss: 0.772475, acc: 0.531250]\n",
      "2691: [D loss: 0.233770, acc: 0.914062]  [G loss: 0.510529, acc: 0.687500]\n",
      "2692: [D loss: 0.210464, acc: 0.906250]  [G loss: 0.386221, acc: 0.812500]\n",
      "2693: [D loss: 0.269146, acc: 0.867188]  [G loss: 0.248454, acc: 0.921875]\n",
      "2694: [D loss: 0.190133, acc: 0.945312]  [G loss: 0.447092, acc: 0.734375]\n",
      "2695: [D loss: 0.165617, acc: 0.937500]  [G loss: 0.512435, acc: 0.765625]\n",
      "2696: [D loss: 0.139570, acc: 0.937500]  [G loss: 0.205598, acc: 0.921875]\n",
      "2697: [D loss: 0.240015, acc: 0.898438]  [G loss: 0.516905, acc: 0.671875]\n",
      "2698: [D loss: 0.215838, acc: 0.914062]  [G loss: 0.515623, acc: 0.718750]\n",
      "2699: [D loss: 0.321481, acc: 0.828125]  [G loss: 0.945803, acc: 0.562500]\n",
      "2700: [D loss: 0.228497, acc: 0.898438]  [G loss: 0.407352, acc: 0.781250]\n",
      "2701: [D loss: 0.370110, acc: 0.828125]  [G loss: 2.154685, acc: 0.000000]\n",
      "2702: [D loss: 0.439186, acc: 0.828125]  [G loss: 0.212993, acc: 0.921875]\n",
      "2703: [D loss: 0.906126, acc: 0.718750]  [G loss: 2.186789, acc: 0.046875]\n",
      "2704: [D loss: 0.421609, acc: 0.804688]  [G loss: 0.781231, acc: 0.531250]\n",
      "2705: [D loss: 0.414063, acc: 0.796875]  [G loss: 0.626902, acc: 0.609375]\n",
      "2706: [D loss: 0.253339, acc: 0.898438]  [G loss: 0.836089, acc: 0.453125]\n",
      "2707: [D loss: 0.305713, acc: 0.890625]  [G loss: 1.255677, acc: 0.187500]\n",
      "2708: [D loss: 0.401409, acc: 0.804688]  [G loss: 1.109241, acc: 0.250000]\n",
      "2709: [D loss: 0.297429, acc: 0.867188]  [G loss: 1.100000, acc: 0.234375]\n",
      "2710: [D loss: 0.236450, acc: 0.929688]  [G loss: 1.320278, acc: 0.187500]\n",
      "2711: [D loss: 0.382616, acc: 0.835938]  [G loss: 1.160517, acc: 0.250000]\n",
      "2712: [D loss: 0.301680, acc: 0.851562]  [G loss: 1.561102, acc: 0.156250]\n",
      "2713: [D loss: 0.266140, acc: 0.882812]  [G loss: 1.014597, acc: 0.375000]\n",
      "2714: [D loss: 0.332960, acc: 0.820312]  [G loss: 2.020905, acc: 0.234375]\n",
      "2715: [D loss: 0.436154, acc: 0.773438]  [G loss: 0.590660, acc: 0.671875]\n",
      "2716: [D loss: 0.417211, acc: 0.804688]  [G loss: 1.658075, acc: 0.250000]\n",
      "2717: [D loss: 0.398328, acc: 0.796875]  [G loss: 0.368972, acc: 0.781250]\n",
      "2718: [D loss: 0.342305, acc: 0.820312]  [G loss: 1.179092, acc: 0.281250]\n",
      "2719: [D loss: 0.240023, acc: 0.906250]  [G loss: 0.944086, acc: 0.453125]\n",
      "2720: [D loss: 0.353341, acc: 0.875000]  [G loss: 0.683261, acc: 0.562500]\n",
      "2721: [D loss: 0.339199, acc: 0.851562]  [G loss: 1.142271, acc: 0.343750]\n",
      "2722: [D loss: 0.329615, acc: 0.851562]  [G loss: 0.693168, acc: 0.609375]\n",
      "2723: [D loss: 0.224791, acc: 0.898438]  [G loss: 0.634482, acc: 0.609375]\n",
      "2724: [D loss: 0.256917, acc: 0.875000]  [G loss: 1.123081, acc: 0.343750]\n",
      "2725: [D loss: 0.305388, acc: 0.835938]  [G loss: 0.551396, acc: 0.718750]\n",
      "2726: [D loss: 0.295380, acc: 0.867188]  [G loss: 1.034694, acc: 0.421875]\n",
      "2727: [D loss: 0.257910, acc: 0.890625]  [G loss: 0.999948, acc: 0.406250]\n",
      "2728: [D loss: 0.256050, acc: 0.882812]  [G loss: 1.273193, acc: 0.296875]\n",
      "2729: [D loss: 0.266690, acc: 0.851562]  [G loss: 1.336570, acc: 0.343750]\n",
      "2730: [D loss: 0.252866, acc: 0.898438]  [G loss: 0.751807, acc: 0.593750]\n",
      "2731: [D loss: 0.349713, acc: 0.843750]  [G loss: 1.878517, acc: 0.140625]\n",
      "2732: [D loss: 0.418809, acc: 0.804688]  [G loss: 0.280794, acc: 0.890625]\n",
      "2733: [D loss: 0.471383, acc: 0.750000]  [G loss: 2.749080, acc: 0.015625]\n",
      "2734: [D loss: 0.348633, acc: 0.804688]  [G loss: 0.454491, acc: 0.750000]\n",
      "2735: [D loss: 0.240084, acc: 0.898438]  [G loss: 1.147991, acc: 0.359375]\n",
      "2736: [D loss: 0.260490, acc: 0.898438]  [G loss: 1.026576, acc: 0.343750]\n",
      "2737: [D loss: 0.212078, acc: 0.914062]  [G loss: 0.669015, acc: 0.593750]\n",
      "2738: [D loss: 0.297934, acc: 0.867188]  [G loss: 1.504831, acc: 0.203125]\n",
      "2739: [D loss: 0.289172, acc: 0.890625]  [G loss: 0.576330, acc: 0.671875]\n",
      "2740: [D loss: 0.332800, acc: 0.859375]  [G loss: 2.041244, acc: 0.140625]\n",
      "2741: [D loss: 0.334751, acc: 0.906250]  [G loss: 0.656842, acc: 0.609375]\n",
      "2742: [D loss: 0.311754, acc: 0.859375]  [G loss: 1.951545, acc: 0.125000]\n",
      "2743: [D loss: 0.295972, acc: 0.851562]  [G loss: 0.640091, acc: 0.640625]\n",
      "2744: [D loss: 0.369525, acc: 0.812500]  [G loss: 1.840839, acc: 0.156250]\n",
      "2745: [D loss: 0.191178, acc: 0.937500]  [G loss: 0.925401, acc: 0.515625]\n",
      "2746: [D loss: 0.213260, acc: 0.921875]  [G loss: 1.273166, acc: 0.406250]\n",
      "2747: [D loss: 0.207430, acc: 0.914062]  [G loss: 1.103342, acc: 0.500000]\n",
      "2748: [D loss: 0.195895, acc: 0.921875]  [G loss: 0.670547, acc: 0.593750]\n",
      "2749: [D loss: 0.252366, acc: 0.898438]  [G loss: 1.616580, acc: 0.375000]\n",
      "2750: [D loss: 0.216344, acc: 0.929688]  [G loss: 0.966006, acc: 0.562500]\n",
      "2751: [D loss: 0.168321, acc: 0.937500]  [G loss: 0.495491, acc: 0.703125]\n",
      "2752: [D loss: 0.397202, acc: 0.789062]  [G loss: 2.724597, acc: 0.250000]\n",
      "2753: [D loss: 0.361728, acc: 0.859375]  [G loss: 0.348728, acc: 0.828125]\n",
      "2754: [D loss: 0.451719, acc: 0.781250]  [G loss: 1.114968, acc: 0.500000]\n",
      "2755: [D loss: 0.292820, acc: 0.851562]  [G loss: 0.653408, acc: 0.625000]\n",
      "2756: [D loss: 0.327240, acc: 0.835938]  [G loss: 0.732357, acc: 0.578125]\n",
      "2757: [D loss: 0.242296, acc: 0.921875]  [G loss: 1.389654, acc: 0.453125]\n",
      "2758: [D loss: 0.301577, acc: 0.882812]  [G loss: 0.283614, acc: 0.859375]\n",
      "2759: [D loss: 0.180141, acc: 0.945312]  [G loss: 0.488033, acc: 0.671875]\n",
      "2760: [D loss: 0.223340, acc: 0.898438]  [G loss: 0.404816, acc: 0.796875]\n",
      "2761: [D loss: 0.341495, acc: 0.828125]  [G loss: 1.406629, acc: 0.312500]\n",
      "2762: [D loss: 0.254695, acc: 0.867188]  [G loss: 0.382595, acc: 0.828125]\n",
      "2763: [D loss: 0.290017, acc: 0.851562]  [G loss: 0.659084, acc: 0.640625]\n",
      "2764: [D loss: 0.292678, acc: 0.882812]  [G loss: 1.880952, acc: 0.156250]\n",
      "2765: [D loss: 0.214451, acc: 0.914062]  [G loss: 0.277132, acc: 0.906250]\n",
      "2766: [D loss: 0.452746, acc: 0.750000]  [G loss: 2.285285, acc: 0.046875]\n",
      "2767: [D loss: 0.463832, acc: 0.820312]  [G loss: 0.291722, acc: 0.859375]\n",
      "2768: [D loss: 0.432388, acc: 0.796875]  [G loss: 1.664742, acc: 0.203125]\n",
      "2769: [D loss: 0.268364, acc: 0.890625]  [G loss: 0.693131, acc: 0.515625]\n",
      "2770: [D loss: 0.366791, acc: 0.843750]  [G loss: 0.881643, acc: 0.500000]\n",
      "2771: [D loss: 0.296423, acc: 0.882812]  [G loss: 0.389390, acc: 0.781250]\n",
      "2772: [D loss: 0.310133, acc: 0.867188]  [G loss: 2.381777, acc: 0.140625]\n",
      "2773: [D loss: 0.331607, acc: 0.835938]  [G loss: 0.710816, acc: 0.687500]\n",
      "2774: [D loss: 0.190685, acc: 0.929688]  [G loss: 0.521012, acc: 0.703125]\n",
      "2775: [D loss: 0.291311, acc: 0.851562]  [G loss: 0.855725, acc: 0.515625]\n",
      "2776: [D loss: 0.336756, acc: 0.851562]  [G loss: 0.884990, acc: 0.515625]\n",
      "2777: [D loss: 0.269136, acc: 0.898438]  [G loss: 0.699040, acc: 0.593750]\n",
      "2778: [D loss: 0.335366, acc: 0.859375]  [G loss: 0.498200, acc: 0.750000]\n",
      "2779: [D loss: 0.275661, acc: 0.882812]  [G loss: 1.485570, acc: 0.312500]\n",
      "2780: [D loss: 0.306147, acc: 0.882812]  [G loss: 0.517748, acc: 0.718750]\n",
      "2781: [D loss: 0.483364, acc: 0.757812]  [G loss: 2.475488, acc: 0.140625]\n",
      "2782: [D loss: 0.502532, acc: 0.828125]  [G loss: 0.497236, acc: 0.781250]\n",
      "2783: [D loss: 0.394489, acc: 0.843750]  [G loss: 1.390102, acc: 0.312500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2784: [D loss: 0.323521, acc: 0.859375]  [G loss: 0.755865, acc: 0.562500]\n",
      "2785: [D loss: 0.372985, acc: 0.804688]  [G loss: 0.996347, acc: 0.484375]\n",
      "2786: [D loss: 0.294376, acc: 0.882812]  [G loss: 1.100091, acc: 0.375000]\n",
      "2787: [D loss: 0.352345, acc: 0.796875]  [G loss: 0.869714, acc: 0.546875]\n",
      "2788: [D loss: 0.259830, acc: 0.914062]  [G loss: 0.722012, acc: 0.546875]\n",
      "2789: [D loss: 0.366474, acc: 0.835938]  [G loss: 0.941072, acc: 0.500000]\n",
      "2790: [D loss: 0.322111, acc: 0.820312]  [G loss: 0.527675, acc: 0.687500]\n",
      "2791: [D loss: 0.310064, acc: 0.851562]  [G loss: 1.644014, acc: 0.234375]\n",
      "2792: [D loss: 0.281127, acc: 0.859375]  [G loss: 0.403210, acc: 0.781250]\n",
      "2793: [D loss: 0.364029, acc: 0.828125]  [G loss: 1.110253, acc: 0.453125]\n",
      "2794: [D loss: 0.238164, acc: 0.898438]  [G loss: 0.425045, acc: 0.765625]\n",
      "2795: [D loss: 0.326723, acc: 0.843750]  [G loss: 1.108270, acc: 0.343750]\n",
      "2796: [D loss: 0.311940, acc: 0.867188]  [G loss: 1.037648, acc: 0.437500]\n",
      "2797: [D loss: 0.366414, acc: 0.828125]  [G loss: 0.728074, acc: 0.625000]\n",
      "2798: [D loss: 0.261443, acc: 0.921875]  [G loss: 1.385425, acc: 0.250000]\n",
      "2799: [D loss: 0.276557, acc: 0.882812]  [G loss: 0.607256, acc: 0.625000]\n",
      "2800: [D loss: 0.307727, acc: 0.898438]  [G loss: 1.768326, acc: 0.093750]\n",
      "2801: [D loss: 0.244377, acc: 0.929688]  [G loss: 0.834583, acc: 0.453125]\n",
      "2802: [D loss: 0.215084, acc: 0.890625]  [G loss: 1.520639, acc: 0.203125]\n",
      "2803: [D loss: 0.259350, acc: 0.875000]  [G loss: 0.572576, acc: 0.671875]\n",
      "2804: [D loss: 0.328785, acc: 0.851562]  [G loss: 1.620710, acc: 0.328125]\n",
      "2805: [D loss: 0.224831, acc: 0.898438]  [G loss: 0.950777, acc: 0.515625]\n",
      "2806: [D loss: 0.223625, acc: 0.906250]  [G loss: 1.299665, acc: 0.375000]\n",
      "2807: [D loss: 0.241431, acc: 0.906250]  [G loss: 0.518502, acc: 0.718750]\n",
      "2808: [D loss: 0.270323, acc: 0.859375]  [G loss: 2.605894, acc: 0.203125]\n",
      "2809: [D loss: 0.211870, acc: 0.929688]  [G loss: 1.569061, acc: 0.265625]\n",
      "2810: [D loss: 0.326213, acc: 0.851562]  [G loss: 1.512784, acc: 0.187500]\n",
      "2811: [D loss: 0.315978, acc: 0.859375]  [G loss: 1.114871, acc: 0.343750]\n",
      "2812: [D loss: 0.244221, acc: 0.890625]  [G loss: 1.576988, acc: 0.187500]\n",
      "2813: [D loss: 0.190909, acc: 0.921875]  [G loss: 0.870261, acc: 0.578125]\n",
      "2814: [D loss: 0.299426, acc: 0.851562]  [G loss: 2.295867, acc: 0.109375]\n",
      "2815: [D loss: 0.222116, acc: 0.898438]  [G loss: 0.548380, acc: 0.687500]\n",
      "2816: [D loss: 0.427342, acc: 0.804688]  [G loss: 2.560750, acc: 0.156250]\n",
      "2817: [D loss: 0.409234, acc: 0.859375]  [G loss: 0.543716, acc: 0.734375]\n",
      "2818: [D loss: 0.299758, acc: 0.835938]  [G loss: 1.433362, acc: 0.359375]\n",
      "2819: [D loss: 0.258680, acc: 0.867188]  [G loss: 1.705227, acc: 0.296875]\n",
      "2820: [D loss: 0.273623, acc: 0.859375]  [G loss: 1.317524, acc: 0.437500]\n",
      "2821: [D loss: 0.240767, acc: 0.890625]  [G loss: 1.039718, acc: 0.453125]\n",
      "2822: [D loss: 0.234021, acc: 0.890625]  [G loss: 1.350751, acc: 0.359375]\n",
      "2823: [D loss: 0.294594, acc: 0.835938]  [G loss: 1.151841, acc: 0.421875]\n",
      "2824: [D loss: 0.175682, acc: 0.945312]  [G loss: 0.919959, acc: 0.437500]\n",
      "2825: [D loss: 0.221437, acc: 0.906250]  [G loss: 0.771121, acc: 0.578125]\n",
      "2826: [D loss: 0.243574, acc: 0.898438]  [G loss: 1.121212, acc: 0.421875]\n",
      "2827: [D loss: 0.337103, acc: 0.875000]  [G loss: 1.694013, acc: 0.265625]\n",
      "2828: [D loss: 0.233962, acc: 0.882812]  [G loss: 0.441861, acc: 0.781250]\n",
      "2829: [D loss: 0.261185, acc: 0.867188]  [G loss: 2.092905, acc: 0.093750]\n",
      "2830: [D loss: 0.164376, acc: 0.929688]  [G loss: 0.756427, acc: 0.609375]\n",
      "2831: [D loss: 0.134264, acc: 0.937500]  [G loss: 0.657369, acc: 0.625000]\n",
      "2832: [D loss: 0.310159, acc: 0.875000]  [G loss: 2.357536, acc: 0.171875]\n",
      "2833: [D loss: 0.240321, acc: 0.882812]  [G loss: 0.420277, acc: 0.781250]\n",
      "2834: [D loss: 0.228945, acc: 0.906250]  [G loss: 1.649180, acc: 0.250000]\n",
      "2835: [D loss: 0.363127, acc: 0.859375]  [G loss: 0.795123, acc: 0.593750]\n",
      "2836: [D loss: 0.286925, acc: 0.859375]  [G loss: 1.659868, acc: 0.312500]\n",
      "2837: [D loss: 0.311654, acc: 0.843750]  [G loss: 0.510666, acc: 0.687500]\n",
      "2838: [D loss: 0.312830, acc: 0.835938]  [G loss: 1.926359, acc: 0.203125]\n",
      "2839: [D loss: 0.252218, acc: 0.867188]  [G loss: 0.660436, acc: 0.671875]\n",
      "2840: [D loss: 0.235322, acc: 0.906250]  [G loss: 1.366444, acc: 0.250000]\n",
      "2841: [D loss: 0.241241, acc: 0.875000]  [G loss: 0.526945, acc: 0.687500]\n",
      "2842: [D loss: 0.287512, acc: 0.875000]  [G loss: 1.389490, acc: 0.343750]\n",
      "2843: [D loss: 0.245954, acc: 0.898438]  [G loss: 0.870700, acc: 0.515625]\n",
      "2844: [D loss: 0.240710, acc: 0.906250]  [G loss: 0.509764, acc: 0.765625]\n",
      "2845: [D loss: 0.248954, acc: 0.859375]  [G loss: 1.424067, acc: 0.312500]\n",
      "2846: [D loss: 0.329436, acc: 0.859375]  [G loss: 0.489414, acc: 0.671875]\n",
      "2847: [D loss: 0.391951, acc: 0.765625]  [G loss: 2.576740, acc: 0.109375]\n",
      "2848: [D loss: 0.376687, acc: 0.875000]  [G loss: 0.572713, acc: 0.687500]\n",
      "2849: [D loss: 0.286620, acc: 0.875000]  [G loss: 1.772908, acc: 0.203125]\n",
      "2850: [D loss: 0.246261, acc: 0.890625]  [G loss: 0.647209, acc: 0.640625]\n",
      "2851: [D loss: 0.201102, acc: 0.937500]  [G loss: 1.270088, acc: 0.359375]\n",
      "2852: [D loss: 0.253792, acc: 0.906250]  [G loss: 0.560080, acc: 0.656250]\n",
      "2853: [D loss: 0.247005, acc: 0.914062]  [G loss: 1.159199, acc: 0.515625]\n",
      "2854: [D loss: 0.264434, acc: 0.890625]  [G loss: 0.508411, acc: 0.734375]\n",
      "2855: [D loss: 0.231615, acc: 0.914062]  [G loss: 1.445168, acc: 0.421875]\n",
      "2856: [D loss: 0.189990, acc: 0.929688]  [G loss: 1.123516, acc: 0.500000]\n",
      "2857: [D loss: 0.211873, acc: 0.906250]  [G loss: 0.517438, acc: 0.734375]\n",
      "2858: [D loss: 0.373844, acc: 0.867188]  [G loss: 1.586813, acc: 0.375000]\n",
      "2859: [D loss: 0.225707, acc: 0.890625]  [G loss: 0.721229, acc: 0.640625]\n",
      "2860: [D loss: 0.213519, acc: 0.906250]  [G loss: 1.217582, acc: 0.390625]\n",
      "2861: [D loss: 0.293036, acc: 0.875000]  [G loss: 0.378996, acc: 0.828125]\n",
      "2862: [D loss: 0.199313, acc: 0.921875]  [G loss: 1.216433, acc: 0.515625]\n",
      "2863: [D loss: 0.304552, acc: 0.875000]  [G loss: 1.329459, acc: 0.359375]\n",
      "2864: [D loss: 0.137154, acc: 0.945312]  [G loss: 0.537707, acc: 0.750000]\n",
      "2865: [D loss: 0.393088, acc: 0.851562]  [G loss: 2.997640, acc: 0.109375]\n",
      "2866: [D loss: 0.284590, acc: 0.843750]  [G loss: 0.484213, acc: 0.718750]\n",
      "2867: [D loss: 0.468913, acc: 0.781250]  [G loss: 3.571056, acc: 0.031250]\n",
      "2868: [D loss: 0.270824, acc: 0.867188]  [G loss: 0.948645, acc: 0.531250]\n",
      "2869: [D loss: 0.221708, acc: 0.875000]  [G loss: 1.081812, acc: 0.437500]\n",
      "2870: [D loss: 0.208458, acc: 0.921875]  [G loss: 0.738989, acc: 0.609375]\n",
      "2871: [D loss: 0.260639, acc: 0.867188]  [G loss: 1.388289, acc: 0.312500]\n",
      "2872: [D loss: 0.146645, acc: 0.921875]  [G loss: 0.475782, acc: 0.734375]\n",
      "2873: [D loss: 0.305065, acc: 0.843750]  [G loss: 1.392796, acc: 0.390625]\n",
      "2874: [D loss: 0.225552, acc: 0.890625]  [G loss: 0.643430, acc: 0.640625]\n",
      "2875: [D loss: 0.354024, acc: 0.835938]  [G loss: 2.824626, acc: 0.031250]\n",
      "2876: [D loss: 0.169838, acc: 0.945312]  [G loss: 1.281992, acc: 0.265625]\n",
      "2877: [D loss: 0.244991, acc: 0.929688]  [G loss: 1.072224, acc: 0.437500]\n",
      "2878: [D loss: 0.121729, acc: 0.960938]  [G loss: 0.767266, acc: 0.656250]\n",
      "2879: [D loss: 0.352183, acc: 0.843750]  [G loss: 2.024221, acc: 0.203125]\n",
      "2880: [D loss: 0.247143, acc: 0.882812]  [G loss: 0.555835, acc: 0.734375]\n",
      "2881: [D loss: 0.375694, acc: 0.820312]  [G loss: 2.514251, acc: 0.109375]\n",
      "2882: [D loss: 0.351245, acc: 0.851562]  [G loss: 0.891577, acc: 0.578125]\n",
      "2883: [D loss: 0.336720, acc: 0.867188]  [G loss: 3.371337, acc: 0.000000]\n",
      "2884: [D loss: 0.339199, acc: 0.859375]  [G loss: 1.121161, acc: 0.437500]\n",
      "2885: [D loss: 0.222024, acc: 0.906250]  [G loss: 1.246836, acc: 0.343750]\n",
      "2886: [D loss: 0.198810, acc: 0.906250]  [G loss: 1.173323, acc: 0.343750]\n",
      "2887: [D loss: 0.134099, acc: 0.945312]  [G loss: 0.760946, acc: 0.593750]\n",
      "2888: [D loss: 0.385165, acc: 0.828125]  [G loss: 1.419122, acc: 0.453125]\n",
      "2889: [D loss: 0.392451, acc: 0.812500]  [G loss: 1.487376, acc: 0.281250]\n",
      "2890: [D loss: 0.217977, acc: 0.921875]  [G loss: 0.788695, acc: 0.593750]\n",
      "2891: [D loss: 0.245510, acc: 0.906250]  [G loss: 1.258105, acc: 0.375000]\n",
      "2892: [D loss: 0.288903, acc: 0.890625]  [G loss: 0.652428, acc: 0.671875]\n",
      "2893: [D loss: 0.129654, acc: 0.976562]  [G loss: 0.881576, acc: 0.593750]\n",
      "2894: [D loss: 0.262102, acc: 0.898438]  [G loss: 0.966741, acc: 0.468750]\n",
      "2895: [D loss: 0.220499, acc: 0.906250]  [G loss: 1.384887, acc: 0.390625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2896: [D loss: 0.184045, acc: 0.914062]  [G loss: 1.512444, acc: 0.218750]\n",
      "2897: [D loss: 0.248853, acc: 0.882812]  [G loss: 0.624978, acc: 0.718750]\n",
      "2898: [D loss: 0.306505, acc: 0.867188]  [G loss: 2.894498, acc: 0.031250]\n",
      "2899: [D loss: 0.282337, acc: 0.890625]  [G loss: 1.044167, acc: 0.359375]\n",
      "2900: [D loss: 0.207366, acc: 0.937500]  [G loss: 1.364145, acc: 0.281250]\n",
      "2901: [D loss: 0.271539, acc: 0.898438]  [G loss: 1.514406, acc: 0.375000]\n",
      "2902: [D loss: 0.175803, acc: 0.929688]  [G loss: 1.109182, acc: 0.437500]\n",
      "2903: [D loss: 0.178962, acc: 0.929688]  [G loss: 1.370847, acc: 0.312500]\n",
      "2904: [D loss: 0.151824, acc: 0.937500]  [G loss: 0.829585, acc: 0.562500]\n",
      "2905: [D loss: 0.209655, acc: 0.914062]  [G loss: 1.412622, acc: 0.437500]\n",
      "2906: [D loss: 0.294093, acc: 0.906250]  [G loss: 0.903917, acc: 0.515625]\n",
      "2907: [D loss: 0.204278, acc: 0.898438]  [G loss: 0.781121, acc: 0.609375]\n",
      "2908: [D loss: 0.270733, acc: 0.906250]  [G loss: 2.177328, acc: 0.203125]\n",
      "2909: [D loss: 0.449483, acc: 0.812500]  [G loss: 0.130426, acc: 0.937500]\n",
      "2910: [D loss: 0.935884, acc: 0.640625]  [G loss: 3.850842, acc: 0.000000]\n",
      "2911: [D loss: 0.448009, acc: 0.828125]  [G loss: 1.046847, acc: 0.406250]\n",
      "2912: [D loss: 0.366417, acc: 0.843750]  [G loss: 1.678268, acc: 0.203125]\n",
      "2913: [D loss: 0.342685, acc: 0.828125]  [G loss: 0.581593, acc: 0.703125]\n",
      "2914: [D loss: 0.274511, acc: 0.867188]  [G loss: 1.228113, acc: 0.250000]\n",
      "2915: [D loss: 0.163514, acc: 0.945312]  [G loss: 0.928814, acc: 0.421875]\n",
      "2916: [D loss: 0.293363, acc: 0.890625]  [G loss: 1.714454, acc: 0.203125]\n",
      "2917: [D loss: 0.287797, acc: 0.851562]  [G loss: 0.764493, acc: 0.593750]\n",
      "2918: [D loss: 0.246959, acc: 0.890625]  [G loss: 1.068668, acc: 0.421875]\n",
      "2919: [D loss: 0.184912, acc: 0.914062]  [G loss: 0.766439, acc: 0.625000]\n",
      "2920: [D loss: 0.366669, acc: 0.835938]  [G loss: 0.982366, acc: 0.500000]\n",
      "2921: [D loss: 0.209488, acc: 0.929688]  [G loss: 0.293388, acc: 0.890625]\n",
      "2922: [D loss: 0.269793, acc: 0.898438]  [G loss: 1.087934, acc: 0.468750]\n",
      "2923: [D loss: 0.235516, acc: 0.898438]  [G loss: 1.351486, acc: 0.296875]\n",
      "2924: [D loss: 0.241823, acc: 0.906250]  [G loss: 0.497639, acc: 0.796875]\n",
      "2925: [D loss: 0.463554, acc: 0.804688]  [G loss: 2.192923, acc: 0.093750]\n",
      "2926: [D loss: 0.353453, acc: 0.796875]  [G loss: 0.262528, acc: 0.875000]\n",
      "2927: [D loss: 0.456631, acc: 0.812500]  [G loss: 2.000454, acc: 0.093750]\n",
      "2928: [D loss: 0.448463, acc: 0.789062]  [G loss: 0.357720, acc: 0.812500]\n",
      "2929: [D loss: 0.284879, acc: 0.875000]  [G loss: 0.738132, acc: 0.593750]\n",
      "2930: [D loss: 0.211033, acc: 0.945312]  [G loss: 0.840418, acc: 0.578125]\n",
      "2931: [D loss: 0.281055, acc: 0.882812]  [G loss: 0.439046, acc: 0.750000]\n",
      "2932: [D loss: 0.210401, acc: 0.914062]  [G loss: 0.889408, acc: 0.500000]\n",
      "2933: [D loss: 0.271489, acc: 0.882812]  [G loss: 0.935722, acc: 0.500000]\n",
      "2934: [D loss: 0.350308, acc: 0.828125]  [G loss: 1.391643, acc: 0.312500]\n",
      "2935: [D loss: 0.291325, acc: 0.867188]  [G loss: 0.424688, acc: 0.734375]\n",
      "2936: [D loss: 0.425867, acc: 0.781250]  [G loss: 2.623622, acc: 0.078125]\n",
      "2937: [D loss: 0.322012, acc: 0.867188]  [G loss: 0.712007, acc: 0.625000]\n",
      "2938: [D loss: 0.292985, acc: 0.882812]  [G loss: 1.004681, acc: 0.375000]\n",
      "2939: [D loss: 0.249872, acc: 0.898438]  [G loss: 0.601609, acc: 0.593750]\n",
      "2940: [D loss: 0.251022, acc: 0.906250]  [G loss: 1.060820, acc: 0.437500]\n",
      "2941: [D loss: 0.234893, acc: 0.882812]  [G loss: 0.660499, acc: 0.640625]\n",
      "2942: [D loss: 0.270901, acc: 0.859375]  [G loss: 0.541269, acc: 0.718750]\n",
      "2943: [D loss: 0.255009, acc: 0.882812]  [G loss: 0.791715, acc: 0.593750]\n",
      "2944: [D loss: 0.233806, acc: 0.929688]  [G loss: 0.780378, acc: 0.687500]\n",
      "2945: [D loss: 0.303670, acc: 0.867188]  [G loss: 1.318851, acc: 0.453125]\n",
      "2946: [D loss: 0.367486, acc: 0.859375]  [G loss: 0.704525, acc: 0.562500]\n",
      "2947: [D loss: 0.252914, acc: 0.906250]  [G loss: 0.879933, acc: 0.500000]\n",
      "2948: [D loss: 0.375542, acc: 0.828125]  [G loss: 1.783459, acc: 0.281250]\n",
      "2949: [D loss: 0.302221, acc: 0.851562]  [G loss: 0.629266, acc: 0.640625]\n",
      "2950: [D loss: 0.323761, acc: 0.867188]  [G loss: 1.630789, acc: 0.265625]\n",
      "2951: [D loss: 0.330183, acc: 0.843750]  [G loss: 0.204487, acc: 0.968750]\n",
      "2952: [D loss: 0.492388, acc: 0.757812]  [G loss: 3.133388, acc: 0.000000]\n",
      "2953: [D loss: 0.347694, acc: 0.828125]  [G loss: 0.694335, acc: 0.640625]\n",
      "2954: [D loss: 0.466535, acc: 0.820312]  [G loss: 2.464446, acc: 0.000000]\n",
      "2955: [D loss: 0.307706, acc: 0.851562]  [G loss: 0.693636, acc: 0.578125]\n",
      "2956: [D loss: 0.261852, acc: 0.898438]  [G loss: 1.089810, acc: 0.296875]\n",
      "2957: [D loss: 0.332775, acc: 0.859375]  [G loss: 0.708003, acc: 0.531250]\n",
      "2958: [D loss: 0.276584, acc: 0.867188]  [G loss: 1.502529, acc: 0.187500]\n",
      "2959: [D loss: 0.308011, acc: 0.859375]  [G loss: 0.598071, acc: 0.687500]\n",
      "2960: [D loss: 0.226116, acc: 0.890625]  [G loss: 1.116166, acc: 0.500000]\n",
      "2961: [D loss: 0.316597, acc: 0.851562]  [G loss: 0.868184, acc: 0.515625]\n",
      "2962: [D loss: 0.085781, acc: 0.976562]  [G loss: 0.713512, acc: 0.578125]\n",
      "2963: [D loss: 0.402607, acc: 0.835938]  [G loss: 2.327552, acc: 0.062500]\n",
      "2964: [D loss: 0.407403, acc: 0.835938]  [G loss: 0.974794, acc: 0.562500]\n",
      "2965: [D loss: 0.319354, acc: 0.820312]  [G loss: 0.872035, acc: 0.578125]\n",
      "2966: [D loss: 0.275401, acc: 0.875000]  [G loss: 1.189162, acc: 0.390625]\n",
      "2967: [D loss: 0.231829, acc: 0.921875]  [G loss: 1.536815, acc: 0.218750]\n",
      "2968: [D loss: 0.291318, acc: 0.843750]  [G loss: 0.344304, acc: 0.843750]\n",
      "2969: [D loss: 0.304132, acc: 0.890625]  [G loss: 1.990712, acc: 0.234375]\n",
      "2970: [D loss: 0.362126, acc: 0.851562]  [G loss: 1.216811, acc: 0.375000]\n",
      "2971: [D loss: 0.166591, acc: 0.937500]  [G loss: 0.516880, acc: 0.703125]\n",
      "2972: [D loss: 0.310177, acc: 0.882812]  [G loss: 1.448012, acc: 0.250000]\n",
      "2973: [D loss: 0.313671, acc: 0.851562]  [G loss: 1.518769, acc: 0.156250]\n",
      "2974: [D loss: 0.319598, acc: 0.843750]  [G loss: 0.731870, acc: 0.593750]\n",
      "2975: [D loss: 0.278798, acc: 0.914062]  [G loss: 1.781462, acc: 0.234375]\n",
      "2976: [D loss: 0.264911, acc: 0.898438]  [G loss: 0.988097, acc: 0.406250]\n",
      "2977: [D loss: 0.289251, acc: 0.859375]  [G loss: 1.618914, acc: 0.218750]\n",
      "2978: [D loss: 0.285338, acc: 0.890625]  [G loss: 0.656582, acc: 0.640625]\n",
      "2979: [D loss: 0.420054, acc: 0.796875]  [G loss: 1.439409, acc: 0.187500]\n",
      "2980: [D loss: 0.481770, acc: 0.843750]  [G loss: 1.844852, acc: 0.265625]\n",
      "2981: [D loss: 0.285193, acc: 0.898438]  [G loss: 1.069225, acc: 0.453125]\n",
      "2982: [D loss: 0.264538, acc: 0.921875]  [G loss: 1.092652, acc: 0.359375]\n",
      "2983: [D loss: 0.359072, acc: 0.812500]  [G loss: 1.239489, acc: 0.343750]\n",
      "2984: [D loss: 0.226119, acc: 0.914062]  [G loss: 1.289105, acc: 0.328125]\n",
      "2985: [D loss: 0.287715, acc: 0.859375]  [G loss: 0.421275, acc: 0.781250]\n",
      "2986: [D loss: 0.458988, acc: 0.757812]  [G loss: 2.124945, acc: 0.218750]\n",
      "2987: [D loss: 0.422543, acc: 0.835938]  [G loss: 0.526077, acc: 0.750000]\n",
      "2988: [D loss: 0.510057, acc: 0.757812]  [G loss: 1.787494, acc: 0.156250]\n",
      "2989: [D loss: 0.369994, acc: 0.828125]  [G loss: 0.853321, acc: 0.453125]\n",
      "2990: [D loss: 0.219203, acc: 0.898438]  [G loss: 1.081890, acc: 0.375000]\n",
      "2991: [D loss: 0.272183, acc: 0.882812]  [G loss: 1.685823, acc: 0.125000]\n",
      "2992: [D loss: 0.269874, acc: 0.882812]  [G loss: 0.640645, acc: 0.656250]\n",
      "2993: [D loss: 0.263291, acc: 0.882812]  [G loss: 0.907489, acc: 0.562500]\n",
      "2994: [D loss: 0.386883, acc: 0.796875]  [G loss: 1.632694, acc: 0.281250]\n",
      "2995: [D loss: 0.378357, acc: 0.804688]  [G loss: 0.608911, acc: 0.718750]\n",
      "2996: [D loss: 0.433624, acc: 0.804688]  [G loss: 1.555104, acc: 0.218750]\n",
      "2997: [D loss: 0.357909, acc: 0.851562]  [G loss: 0.875282, acc: 0.515625]\n",
      "2998: [D loss: 0.324180, acc: 0.835938]  [G loss: 1.320337, acc: 0.328125]\n",
      "2999: [D loss: 0.204970, acc: 0.906250]  [G loss: 0.535207, acc: 0.703125]\n",
      "3000: [D loss: 0.350468, acc: 0.843750]  [G loss: 1.450264, acc: 0.421875]\n",
      "3001: [D loss: 0.314025, acc: 0.851562]  [G loss: 0.506532, acc: 0.718750]\n",
      "3002: [D loss: 0.255794, acc: 0.867188]  [G loss: 0.698433, acc: 0.593750]\n",
      "3003: [D loss: 0.154450, acc: 0.953125]  [G loss: 0.325089, acc: 0.859375]\n",
      "3004: [D loss: 0.412006, acc: 0.804688]  [G loss: 1.651630, acc: 0.234375]\n",
      "3005: [D loss: 0.351597, acc: 0.828125]  [G loss: 0.407491, acc: 0.781250]\n",
      "3006: [D loss: 0.357941, acc: 0.796875]  [G loss: 1.643478, acc: 0.281250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3007: [D loss: 0.335771, acc: 0.843750]  [G loss: 0.545968, acc: 0.703125]\n",
      "3008: [D loss: 0.271971, acc: 0.882812]  [G loss: 1.097103, acc: 0.468750]\n",
      "3009: [D loss: 0.248768, acc: 0.875000]  [G loss: 0.434891, acc: 0.750000]\n",
      "3010: [D loss: 0.451964, acc: 0.765625]  [G loss: 1.574624, acc: 0.421875]\n",
      "3011: [D loss: 0.332344, acc: 0.828125]  [G loss: 0.364789, acc: 0.796875]\n",
      "3012: [D loss: 0.284246, acc: 0.867188]  [G loss: 0.807674, acc: 0.625000]\n",
      "3013: [D loss: 0.301883, acc: 0.875000]  [G loss: 0.505328, acc: 0.703125]\n",
      "3014: [D loss: 0.260502, acc: 0.890625]  [G loss: 0.670918, acc: 0.609375]\n",
      "3015: [D loss: 0.253462, acc: 0.898438]  [G loss: 0.465382, acc: 0.718750]\n",
      "3016: [D loss: 0.213423, acc: 0.906250]  [G loss: 0.536001, acc: 0.671875]\n",
      "3017: [D loss: 0.336924, acc: 0.820312]  [G loss: 1.339409, acc: 0.421875]\n",
      "3018: [D loss: 0.303892, acc: 0.843750]  [G loss: 0.296084, acc: 0.906250]\n",
      "3019: [D loss: 0.197410, acc: 0.945312]  [G loss: 0.847324, acc: 0.531250]\n",
      "3020: [D loss: 0.198975, acc: 0.921875]  [G loss: 0.503200, acc: 0.734375]\n",
      "3021: [D loss: 0.244539, acc: 0.898438]  [G loss: 0.675227, acc: 0.625000]\n",
      "3022: [D loss: 0.208702, acc: 0.929688]  [G loss: 0.224326, acc: 0.906250]\n",
      "3023: [D loss: 0.257312, acc: 0.882812]  [G loss: 1.924399, acc: 0.109375]\n",
      "3024: [D loss: 0.340318, acc: 0.843750]  [G loss: 0.633819, acc: 0.671875]\n",
      "3025: [D loss: 0.259318, acc: 0.890625]  [G loss: 1.130437, acc: 0.296875]\n",
      "3026: [D loss: 0.230044, acc: 0.898438]  [G loss: 0.575196, acc: 0.656250]\n",
      "3027: [D loss: 0.362352, acc: 0.835938]  [G loss: 2.587164, acc: 0.000000]\n",
      "3028: [D loss: 0.355265, acc: 0.882812]  [G loss: 0.588672, acc: 0.671875]\n",
      "3029: [D loss: 0.296372, acc: 0.859375]  [G loss: 1.905009, acc: 0.125000]\n",
      "3030: [D loss: 0.308739, acc: 0.843750]  [G loss: 1.531661, acc: 0.109375]\n",
      "3031: [D loss: 0.176448, acc: 0.953125]  [G loss: 1.491052, acc: 0.218750]\n",
      "3032: [D loss: 0.154114, acc: 0.929688]  [G loss: 1.243973, acc: 0.375000]\n",
      "3033: [D loss: 0.252966, acc: 0.898438]  [G loss: 1.752712, acc: 0.125000]\n",
      "3034: [D loss: 0.276236, acc: 0.867188]  [G loss: 1.332955, acc: 0.328125]\n",
      "3035: [D loss: 0.301176, acc: 0.890625]  [G loss: 2.717467, acc: 0.031250]\n",
      "3036: [D loss: 0.385424, acc: 0.812500]  [G loss: 0.768496, acc: 0.546875]\n",
      "3037: [D loss: 0.380240, acc: 0.812500]  [G loss: 1.684162, acc: 0.187500]\n",
      "3038: [D loss: 0.277699, acc: 0.875000]  [G loss: 1.154116, acc: 0.390625]\n",
      "3039: [D loss: 0.204939, acc: 0.906250]  [G loss: 1.037041, acc: 0.390625]\n",
      "3040: [D loss: 0.315941, acc: 0.851562]  [G loss: 1.816118, acc: 0.234375]\n",
      "3041: [D loss: 0.269331, acc: 0.890625]  [G loss: 1.214189, acc: 0.453125]\n",
      "3042: [D loss: 0.454627, acc: 0.796875]  [G loss: 0.820974, acc: 0.546875]\n",
      "3043: [D loss: 0.383612, acc: 0.828125]  [G loss: 1.624597, acc: 0.203125]\n",
      "3044: [D loss: 0.385596, acc: 0.804688]  [G loss: 0.737364, acc: 0.562500]\n",
      "3045: [D loss: 0.352517, acc: 0.820312]  [G loss: 1.545233, acc: 0.296875]\n",
      "3046: [D loss: 0.276816, acc: 0.867188]  [G loss: 0.690655, acc: 0.531250]\n",
      "3047: [D loss: 0.364851, acc: 0.835938]  [G loss: 1.487801, acc: 0.406250]\n",
      "3048: [D loss: 0.258233, acc: 0.875000]  [G loss: 0.824103, acc: 0.546875]\n",
      "3049: [D loss: 0.334901, acc: 0.867188]  [G loss: 2.151685, acc: 0.093750]\n",
      "3050: [D loss: 0.281005, acc: 0.882812]  [G loss: 0.679642, acc: 0.546875]\n",
      "3051: [D loss: 0.245449, acc: 0.890625]  [G loss: 1.012320, acc: 0.390625]\n",
      "3052: [D loss: 0.366239, acc: 0.820312]  [G loss: 0.798046, acc: 0.609375]\n",
      "3053: [D loss: 0.228757, acc: 0.890625]  [G loss: 1.391908, acc: 0.390625]\n",
      "3054: [D loss: 0.184966, acc: 0.914062]  [G loss: 0.390163, acc: 0.796875]\n",
      "3055: [D loss: 0.332417, acc: 0.828125]  [G loss: 2.359162, acc: 0.171875]\n",
      "3056: [D loss: 0.351740, acc: 0.867188]  [G loss: 0.560576, acc: 0.671875]\n",
      "3057: [D loss: 0.532771, acc: 0.742188]  [G loss: 2.322301, acc: 0.328125]\n",
      "3058: [D loss: 0.474769, acc: 0.820312]  [G loss: 0.354659, acc: 0.828125]\n",
      "3059: [D loss: 0.515257, acc: 0.781250]  [G loss: 1.486071, acc: 0.390625]\n",
      "3060: [D loss: 0.345612, acc: 0.812500]  [G loss: 0.566909, acc: 0.750000]\n",
      "3061: [D loss: 0.254447, acc: 0.890625]  [G loss: 1.075967, acc: 0.421875]\n",
      "3062: [D loss: 0.248998, acc: 0.890625]  [G loss: 0.914283, acc: 0.531250]\n",
      "3063: [D loss: 0.269542, acc: 0.867188]  [G loss: 0.844684, acc: 0.625000]\n",
      "3064: [D loss: 0.276227, acc: 0.875000]  [G loss: 0.547372, acc: 0.687500]\n",
      "3065: [D loss: 0.267103, acc: 0.890625]  [G loss: 0.583930, acc: 0.765625]\n",
      "3066: [D loss: 0.244552, acc: 0.882812]  [G loss: 0.856023, acc: 0.609375]\n",
      "3067: [D loss: 0.267313, acc: 0.898438]  [G loss: 0.939947, acc: 0.531250]\n",
      "3068: [D loss: 0.292415, acc: 0.843750]  [G loss: 0.485534, acc: 0.750000]\n",
      "3069: [D loss: 0.234474, acc: 0.921875]  [G loss: 1.481458, acc: 0.312500]\n",
      "3070: [D loss: 0.300107, acc: 0.867188]  [G loss: 0.388680, acc: 0.796875]\n",
      "3071: [D loss: 0.242881, acc: 0.898438]  [G loss: 1.452133, acc: 0.312500]\n",
      "3072: [D loss: 0.249275, acc: 0.906250]  [G loss: 0.614273, acc: 0.687500]\n",
      "3073: [D loss: 0.301009, acc: 0.843750]  [G loss: 0.891244, acc: 0.500000]\n",
      "3074: [D loss: 0.272671, acc: 0.890625]  [G loss: 1.361418, acc: 0.437500]\n",
      "3075: [D loss: 0.298978, acc: 0.875000]  [G loss: 0.395834, acc: 0.796875]\n",
      "3076: [D loss: 0.415872, acc: 0.820312]  [G loss: 2.054669, acc: 0.187500]\n",
      "3077: [D loss: 0.330918, acc: 0.828125]  [G loss: 0.660342, acc: 0.640625]\n",
      "3078: [D loss: 0.264805, acc: 0.898438]  [G loss: 1.915201, acc: 0.078125]\n",
      "3079: [D loss: 0.269748, acc: 0.890625]  [G loss: 1.631795, acc: 0.171875]\n",
      "3080: [D loss: 0.238742, acc: 0.921875]  [G loss: 1.588399, acc: 0.156250]\n",
      "3081: [D loss: 0.271308, acc: 0.859375]  [G loss: 0.842749, acc: 0.453125]\n",
      "3082: [D loss: 0.124882, acc: 0.976562]  [G loss: 0.820435, acc: 0.531250]\n",
      "3083: [D loss: 0.494266, acc: 0.781250]  [G loss: 2.518991, acc: 0.125000]\n",
      "3084: [D loss: 0.438834, acc: 0.812500]  [G loss: 0.461348, acc: 0.750000]\n",
      "3085: [D loss: 0.295732, acc: 0.867188]  [G loss: 2.345077, acc: 0.031250]\n",
      "3086: [D loss: 0.302040, acc: 0.851562]  [G loss: 0.975038, acc: 0.421875]\n",
      "3087: [D loss: 0.197048, acc: 0.914062]  [G loss: 1.331601, acc: 0.296875]\n",
      "3088: [D loss: 0.205457, acc: 0.929688]  [G loss: 1.455010, acc: 0.234375]\n",
      "3089: [D loss: 0.235539, acc: 0.882812]  [G loss: 0.798865, acc: 0.593750]\n",
      "3090: [D loss: 0.287197, acc: 0.882812]  [G loss: 1.326417, acc: 0.328125]\n",
      "3091: [D loss: 0.250455, acc: 0.898438]  [G loss: 0.941860, acc: 0.500000]\n",
      "3092: [D loss: 0.242452, acc: 0.882812]  [G loss: 0.720060, acc: 0.578125]\n",
      "3093: [D loss: 0.259688, acc: 0.898438]  [G loss: 2.062207, acc: 0.093750]\n",
      "3094: [D loss: 0.270262, acc: 0.882812]  [G loss: 1.520578, acc: 0.312500]\n",
      "3095: [D loss: 0.261451, acc: 0.906250]  [G loss: 1.248137, acc: 0.281250]\n",
      "3096: [D loss: 0.286280, acc: 0.851562]  [G loss: 1.363000, acc: 0.531250]\n",
      "3097: [D loss: 0.339784, acc: 0.851562]  [G loss: 0.964347, acc: 0.437500]\n",
      "3098: [D loss: 0.210240, acc: 0.921875]  [G loss: 0.822948, acc: 0.546875]\n",
      "3099: [D loss: 0.232374, acc: 0.906250]  [G loss: 1.336080, acc: 0.453125]\n",
      "3100: [D loss: 0.322665, acc: 0.859375]  [G loss: 0.640445, acc: 0.656250]\n",
      "3101: [D loss: 0.237997, acc: 0.898438]  [G loss: 2.625520, acc: 0.078125]\n",
      "3102: [D loss: 0.186149, acc: 0.929688]  [G loss: 1.317498, acc: 0.328125]\n",
      "3103: [D loss: 0.148416, acc: 0.937500]  [G loss: 0.873910, acc: 0.609375]\n",
      "3104: [D loss: 0.357549, acc: 0.812500]  [G loss: 2.672051, acc: 0.046875]\n",
      "3105: [D loss: 0.412336, acc: 0.781250]  [G loss: 1.141273, acc: 0.343750]\n",
      "3106: [D loss: 0.323265, acc: 0.851562]  [G loss: 2.034999, acc: 0.218750]\n",
      "3107: [D loss: 0.432268, acc: 0.828125]  [G loss: 0.528954, acc: 0.687500]\n",
      "3108: [D loss: 0.443059, acc: 0.843750]  [G loss: 1.863332, acc: 0.093750]\n",
      "3109: [D loss: 0.187477, acc: 0.929688]  [G loss: 1.040673, acc: 0.406250]\n",
      "3110: [D loss: 0.240003, acc: 0.906250]  [G loss: 0.764655, acc: 0.562500]\n",
      "3111: [D loss: 0.213725, acc: 0.921875]  [G loss: 1.026612, acc: 0.421875]\n",
      "3112: [D loss: 0.272067, acc: 0.882812]  [G loss: 0.707680, acc: 0.578125]\n",
      "3113: [D loss: 0.288322, acc: 0.867188]  [G loss: 0.942750, acc: 0.500000]\n",
      "3114: [D loss: 0.297114, acc: 0.867188]  [G loss: 0.735920, acc: 0.671875]\n",
      "3115: [D loss: 0.313922, acc: 0.835938]  [G loss: 0.288962, acc: 0.843750]\n",
      "3116: [D loss: 0.297881, acc: 0.875000]  [G loss: 1.092612, acc: 0.484375]\n",
      "3117: [D loss: 0.286516, acc: 0.867188]  [G loss: 0.341587, acc: 0.843750]\n",
      "3118: [D loss: 0.586825, acc: 0.773438]  [G loss: 1.578815, acc: 0.359375]\n",
      "3119: [D loss: 0.437725, acc: 0.789062]  [G loss: 0.257863, acc: 0.859375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3120: [D loss: 0.492200, acc: 0.796875]  [G loss: 1.773507, acc: 0.187500]\n",
      "3121: [D loss: 0.415404, acc: 0.867188]  [G loss: 0.991815, acc: 0.421875]\n",
      "3122: [D loss: 0.309105, acc: 0.882812]  [G loss: 0.515142, acc: 0.718750]\n",
      "3123: [D loss: 0.369952, acc: 0.820312]  [G loss: 0.743574, acc: 0.562500]\n",
      "3124: [D loss: 0.226658, acc: 0.906250]  [G loss: 0.573892, acc: 0.656250]\n",
      "3125: [D loss: 0.209384, acc: 0.929688]  [G loss: 0.421527, acc: 0.781250]\n",
      "3126: [D loss: 0.225452, acc: 0.906250]  [G loss: 0.766738, acc: 0.593750]\n",
      "3127: [D loss: 0.270819, acc: 0.898438]  [G loss: 0.659461, acc: 0.671875]\n",
      "3128: [D loss: 0.278055, acc: 0.875000]  [G loss: 0.831381, acc: 0.562500]\n",
      "3129: [D loss: 0.330778, acc: 0.851562]  [G loss: 0.477139, acc: 0.765625]\n",
      "3130: [D loss: 0.269222, acc: 0.867188]  [G loss: 1.112069, acc: 0.437500]\n",
      "3131: [D loss: 0.247307, acc: 0.898438]  [G loss: 0.473414, acc: 0.718750]\n",
      "3132: [D loss: 0.363120, acc: 0.828125]  [G loss: 1.455849, acc: 0.296875]\n",
      "3133: [D loss: 0.271025, acc: 0.875000]  [G loss: 0.319071, acc: 0.812500]\n",
      "3134: [D loss: 0.378604, acc: 0.781250]  [G loss: 1.796457, acc: 0.359375]\n",
      "3135: [D loss: 0.302288, acc: 0.875000]  [G loss: 0.429026, acc: 0.718750]\n",
      "3136: [D loss: 0.320194, acc: 0.828125]  [G loss: 1.283089, acc: 0.375000]\n",
      "3137: [D loss: 0.218397, acc: 0.921875]  [G loss: 0.659144, acc: 0.640625]\n",
      "3138: [D loss: 0.271402, acc: 0.875000]  [G loss: 1.062055, acc: 0.437500]\n",
      "3139: [D loss: 0.331551, acc: 0.835938]  [G loss: 0.474947, acc: 0.765625]\n",
      "3140: [D loss: 0.224052, acc: 0.921875]  [G loss: 1.074219, acc: 0.421875]\n",
      "3141: [D loss: 0.198769, acc: 0.898438]  [G loss: 0.704907, acc: 0.656250]\n",
      "3142: [D loss: 0.283123, acc: 0.859375]  [G loss: 2.251718, acc: 0.218750]\n",
      "3143: [D loss: 0.344859, acc: 0.843750]  [G loss: 1.108181, acc: 0.468750]\n",
      "3144: [D loss: 0.251168, acc: 0.914062]  [G loss: 1.839217, acc: 0.296875]\n",
      "3145: [D loss: 0.210261, acc: 0.898438]  [G loss: 0.640647, acc: 0.625000]\n",
      "3146: [D loss: 0.344409, acc: 0.851562]  [G loss: 2.805538, acc: 0.078125]\n",
      "3147: [D loss: 0.412170, acc: 0.796875]  [G loss: 0.304144, acc: 0.859375]\n",
      "3148: [D loss: 0.274862, acc: 0.882812]  [G loss: 1.822802, acc: 0.203125]\n",
      "3149: [D loss: 0.200986, acc: 0.921875]  [G loss: 0.805928, acc: 0.484375]\n",
      "3150: [D loss: 0.181834, acc: 0.921875]  [G loss: 1.805412, acc: 0.203125]\n",
      "3151: [D loss: 0.288963, acc: 0.843750]  [G loss: 1.108755, acc: 0.343750]\n",
      "3152: [D loss: 0.259458, acc: 0.898438]  [G loss: 1.571754, acc: 0.171875]\n",
      "3153: [D loss: 0.293423, acc: 0.898438]  [G loss: 1.416027, acc: 0.265625]\n",
      "3154: [D loss: 0.296852, acc: 0.851562]  [G loss: 3.797061, acc: 0.000000]\n",
      "3155: [D loss: 0.508496, acc: 0.773438]  [G loss: 0.864542, acc: 0.515625]\n",
      "3156: [D loss: 0.281852, acc: 0.867188]  [G loss: 2.131580, acc: 0.062500]\n",
      "3157: [D loss: 0.245408, acc: 0.914062]  [G loss: 1.406302, acc: 0.187500]\n",
      "3158: [D loss: 0.217315, acc: 0.914062]  [G loss: 2.663785, acc: 0.078125]\n",
      "3159: [D loss: 0.271170, acc: 0.890625]  [G loss: 1.920510, acc: 0.062500]\n",
      "3160: [D loss: 0.154994, acc: 0.945312]  [G loss: 1.696043, acc: 0.078125]\n",
      "3161: [D loss: 0.237916, acc: 0.898438]  [G loss: 1.050362, acc: 0.359375]\n",
      "3162: [D loss: 0.290037, acc: 0.890625]  [G loss: 2.474450, acc: 0.015625]\n",
      "3163: [D loss: 0.364610, acc: 0.828125]  [G loss: 0.474418, acc: 0.781250]\n",
      "3164: [D loss: 0.358708, acc: 0.812500]  [G loss: 2.014886, acc: 0.109375]\n",
      "3165: [D loss: 0.294947, acc: 0.875000]  [G loss: 1.200626, acc: 0.281250]\n",
      "3166: [D loss: 0.288092, acc: 0.859375]  [G loss: 1.641823, acc: 0.156250]\n",
      "3167: [D loss: 0.354623, acc: 0.851562]  [G loss: 0.479230, acc: 0.750000]\n",
      "3168: [D loss: 0.417471, acc: 0.828125]  [G loss: 2.718643, acc: 0.000000]\n",
      "3169: [D loss: 0.358665, acc: 0.835938]  [G loss: 0.955284, acc: 0.375000]\n",
      "3170: [D loss: 0.192196, acc: 0.921875]  [G loss: 0.614663, acc: 0.687500]\n",
      "3171: [D loss: 0.145723, acc: 0.953125]  [G loss: 0.786043, acc: 0.484375]\n",
      "3172: [D loss: 0.222024, acc: 0.929688]  [G loss: 1.239989, acc: 0.265625]\n",
      "3173: [D loss: 0.256155, acc: 0.882812]  [G loss: 0.670507, acc: 0.625000]\n",
      "3174: [D loss: 0.468292, acc: 0.781250]  [G loss: 2.211683, acc: 0.125000]\n",
      "3175: [D loss: 0.359668, acc: 0.851562]  [G loss: 0.605561, acc: 0.656250]\n",
      "3176: [D loss: 0.464548, acc: 0.835938]  [G loss: 1.904423, acc: 0.234375]\n",
      "3177: [D loss: 0.413642, acc: 0.812500]  [G loss: 0.898775, acc: 0.484375]\n",
      "3178: [D loss: 0.181802, acc: 0.898438]  [G loss: 0.758498, acc: 0.546875]\n",
      "3179: [D loss: 0.182191, acc: 0.953125]  [G loss: 0.708616, acc: 0.640625]\n",
      "3180: [D loss: 0.205818, acc: 0.921875]  [G loss: 0.588596, acc: 0.640625]\n",
      "3181: [D loss: 0.231544, acc: 0.914062]  [G loss: 0.781481, acc: 0.562500]\n",
      "3182: [D loss: 0.229784, acc: 0.890625]  [G loss: 1.047014, acc: 0.531250]\n",
      "3183: [D loss: 0.229747, acc: 0.914062]  [G loss: 1.037191, acc: 0.421875]\n",
      "3184: [D loss: 0.266758, acc: 0.843750]  [G loss: 1.260318, acc: 0.343750]\n",
      "3185: [D loss: 0.238998, acc: 0.890625]  [G loss: 0.499498, acc: 0.734375]\n",
      "3186: [D loss: 0.286091, acc: 0.859375]  [G loss: 0.886341, acc: 0.546875]\n",
      "3187: [D loss: 0.177056, acc: 0.914062]  [G loss: 0.593119, acc: 0.671875]\n",
      "3188: [D loss: 0.218076, acc: 0.898438]  [G loss: 0.567084, acc: 0.671875]\n",
      "3189: [D loss: 0.328345, acc: 0.851562]  [G loss: 0.870723, acc: 0.625000]\n",
      "3190: [D loss: 0.181834, acc: 0.906250]  [G loss: 0.259739, acc: 0.859375]\n",
      "3191: [D loss: 0.234699, acc: 0.882812]  [G loss: 0.700237, acc: 0.765625]\n",
      "3192: [D loss: 0.330479, acc: 0.859375]  [G loss: 2.552844, acc: 0.218750]\n",
      "3193: [D loss: 0.445662, acc: 0.789062]  [G loss: 0.246316, acc: 0.906250]\n",
      "3194: [D loss: 0.361301, acc: 0.835938]  [G loss: 0.818455, acc: 0.593750]\n",
      "3195: [D loss: 0.325035, acc: 0.867188]  [G loss: 0.546457, acc: 0.734375]\n",
      "3196: [D loss: 0.256058, acc: 0.875000]  [G loss: 0.299675, acc: 0.859375]\n",
      "3197: [D loss: 0.239403, acc: 0.898438]  [G loss: 0.887584, acc: 0.546875]\n",
      "3198: [D loss: 0.474570, acc: 0.796875]  [G loss: 0.992113, acc: 0.437500]\n",
      "3199: [D loss: 0.250092, acc: 0.890625]  [G loss: 0.379805, acc: 0.781250]\n",
      "3200: [D loss: 0.437197, acc: 0.789062]  [G loss: 1.713608, acc: 0.265625]\n",
      "3201: [D loss: 0.376035, acc: 0.843750]  [G loss: 0.711051, acc: 0.593750]\n",
      "3202: [D loss: 0.193244, acc: 0.906250]  [G loss: 0.620171, acc: 0.687500]\n",
      "3203: [D loss: 0.177416, acc: 0.945312]  [G loss: 0.665250, acc: 0.593750]\n",
      "3204: [D loss: 0.146255, acc: 0.953125]  [G loss: 0.553853, acc: 0.765625]\n",
      "3205: [D loss: 0.180705, acc: 0.914062]  [G loss: 0.385070, acc: 0.781250]\n",
      "3206: [D loss: 0.224838, acc: 0.882812]  [G loss: 1.092960, acc: 0.531250]\n",
      "3207: [D loss: 0.190745, acc: 0.914062]  [G loss: 0.390883, acc: 0.796875]\n",
      "3208: [D loss: 0.219657, acc: 0.921875]  [G loss: 1.110524, acc: 0.468750]\n",
      "3209: [D loss: 0.132377, acc: 0.945312]  [G loss: 0.289845, acc: 0.843750]\n",
      "3210: [D loss: 0.326895, acc: 0.882812]  [G loss: 1.704230, acc: 0.265625]\n",
      "3211: [D loss: 0.251575, acc: 0.875000]  [G loss: 0.660427, acc: 0.656250]\n",
      "3212: [D loss: 0.509465, acc: 0.789062]  [G loss: 2.175690, acc: 0.156250]\n",
      "3213: [D loss: 0.133399, acc: 0.968750]  [G loss: 1.147537, acc: 0.484375]\n",
      "3214: [D loss: 0.120644, acc: 0.953125]  [G loss: 0.753286, acc: 0.593750]\n",
      "3215: [D loss: 0.112322, acc: 0.945312]  [G loss: 1.053517, acc: 0.468750]\n",
      "3216: [D loss: 0.224075, acc: 0.906250]  [G loss: 0.336090, acc: 0.828125]\n",
      "3217: [D loss: 0.251105, acc: 0.898438]  [G loss: 1.444073, acc: 0.453125]\n",
      "3218: [D loss: 0.207851, acc: 0.898438]  [G loss: 0.669945, acc: 0.687500]\n",
      "3219: [D loss: 0.251291, acc: 0.898438]  [G loss: 0.863712, acc: 0.640625]\n",
      "3220: [D loss: 0.181680, acc: 0.937500]  [G loss: 0.508163, acc: 0.750000]\n",
      "3221: [D loss: 0.245926, acc: 0.859375]  [G loss: 1.132380, acc: 0.500000]\n",
      "3222: [D loss: 0.291721, acc: 0.867188]  [G loss: 0.867428, acc: 0.484375]\n",
      "3223: [D loss: 0.189474, acc: 0.906250]  [G loss: 0.862938, acc: 0.593750]\n",
      "3224: [D loss: 0.313899, acc: 0.882812]  [G loss: 1.416980, acc: 0.375000]\n",
      "3225: [D loss: 0.246150, acc: 0.906250]  [G loss: 0.585638, acc: 0.718750]\n",
      "3226: [D loss: 0.154875, acc: 0.937500]  [G loss: 0.498938, acc: 0.796875]\n",
      "3227: [D loss: 0.208601, acc: 0.914062]  [G loss: 0.669001, acc: 0.656250]\n",
      "3228: [D loss: 0.126958, acc: 0.960938]  [G loss: 0.886123, acc: 0.578125]\n",
      "3229: [D loss: 0.185388, acc: 0.945312]  [G loss: 0.918717, acc: 0.531250]\n",
      "3230: [D loss: 0.250977, acc: 0.875000]  [G loss: 0.968549, acc: 0.546875]\n",
      "3231: [D loss: 0.172036, acc: 0.937500]  [G loss: 0.893456, acc: 0.640625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232: [D loss: 0.225880, acc: 0.937500]  [G loss: 1.180112, acc: 0.546875]\n",
      "3233: [D loss: 0.175235, acc: 0.929688]  [G loss: 0.355976, acc: 0.828125]\n",
      "3234: [D loss: 0.267342, acc: 0.882812]  [G loss: 4.246399, acc: 0.031250]\n",
      "3235: [D loss: 0.622373, acc: 0.781250]  [G loss: 0.336315, acc: 0.812500]\n",
      "3236: [D loss: 0.507060, acc: 0.781250]  [G loss: 2.237120, acc: 0.250000]\n",
      "3237: [D loss: 0.230228, acc: 0.882812]  [G loss: 1.044249, acc: 0.468750]\n",
      "3238: [D loss: 0.198813, acc: 0.921875]  [G loss: 0.951624, acc: 0.562500]\n",
      "3239: [D loss: 0.140343, acc: 0.953125]  [G loss: 0.932342, acc: 0.546875]\n",
      "3240: [D loss: 0.175056, acc: 0.921875]  [G loss: 0.959071, acc: 0.593750]\n",
      "3241: [D loss: 0.167217, acc: 0.945312]  [G loss: 0.468392, acc: 0.750000]\n",
      "3242: [D loss: 0.210454, acc: 0.921875]  [G loss: 1.730089, acc: 0.437500]\n",
      "3243: [D loss: 0.291767, acc: 0.867188]  [G loss: 0.251331, acc: 0.875000]\n",
      "3244: [D loss: 0.251090, acc: 0.875000]  [G loss: 1.274315, acc: 0.375000]\n",
      "3245: [D loss: 0.228494, acc: 0.914062]  [G loss: 0.492719, acc: 0.718750]\n",
      "3246: [D loss: 0.181095, acc: 0.953125]  [G loss: 0.587971, acc: 0.734375]\n",
      "3247: [D loss: 0.180439, acc: 0.921875]  [G loss: 1.041067, acc: 0.421875]\n",
      "3248: [D loss: 0.228067, acc: 0.906250]  [G loss: 1.238202, acc: 0.484375]\n",
      "3249: [D loss: 0.148404, acc: 0.960938]  [G loss: 0.827987, acc: 0.562500]\n",
      "3250: [D loss: 0.155126, acc: 0.945312]  [G loss: 0.750765, acc: 0.578125]\n",
      "3251: [D loss: 0.154844, acc: 0.937500]  [G loss: 0.500569, acc: 0.796875]\n",
      "3252: [D loss: 0.398765, acc: 0.843750]  [G loss: 2.039914, acc: 0.265625]\n",
      "3253: [D loss: 0.467863, acc: 0.835938]  [G loss: 0.259389, acc: 0.859375]\n",
      "3254: [D loss: 0.275710, acc: 0.875000]  [G loss: 1.723009, acc: 0.265625]\n",
      "3255: [D loss: 0.314030, acc: 0.867188]  [G loss: 1.214588, acc: 0.390625]\n",
      "3256: [D loss: 0.218644, acc: 0.906250]  [G loss: 0.788608, acc: 0.515625]\n",
      "3257: [D loss: 0.212157, acc: 0.898438]  [G loss: 1.120995, acc: 0.531250]\n",
      "3258: [D loss: 0.154056, acc: 0.929688]  [G loss: 0.503613, acc: 0.750000]\n",
      "3259: [D loss: 0.161381, acc: 0.937500]  [G loss: 0.614930, acc: 0.718750]\n",
      "3260: [D loss: 0.178131, acc: 0.929688]  [G loss: 1.024232, acc: 0.484375]\n",
      "3261: [D loss: 0.298300, acc: 0.843750]  [G loss: 0.365121, acc: 0.843750]\n",
      "3262: [D loss: 0.212421, acc: 0.906250]  [G loss: 1.393668, acc: 0.375000]\n",
      "3263: [D loss: 0.209008, acc: 0.906250]  [G loss: 0.859056, acc: 0.562500]\n",
      "3264: [D loss: 0.225989, acc: 0.914062]  [G loss: 0.804808, acc: 0.656250]\n",
      "3265: [D loss: 0.212906, acc: 0.937500]  [G loss: 0.766513, acc: 0.625000]\n",
      "3266: [D loss: 0.214471, acc: 0.890625]  [G loss: 1.041920, acc: 0.453125]\n",
      "3267: [D loss: 0.255503, acc: 0.890625]  [G loss: 0.580774, acc: 0.703125]\n",
      "3268: [D loss: 0.242720, acc: 0.921875]  [G loss: 1.786329, acc: 0.406250]\n",
      "3269: [D loss: 0.285977, acc: 0.890625]  [G loss: 0.355625, acc: 0.828125]\n",
      "3270: [D loss: 0.295272, acc: 0.914062]  [G loss: 2.514274, acc: 0.140625]\n",
      "3271: [D loss: 0.499161, acc: 0.812500]  [G loss: 0.536974, acc: 0.687500]\n",
      "3272: [D loss: 0.541949, acc: 0.835938]  [G loss: 1.340805, acc: 0.421875]\n",
      "3273: [D loss: 0.318492, acc: 0.867188]  [G loss: 0.605834, acc: 0.718750]\n",
      "3274: [D loss: 0.264592, acc: 0.906250]  [G loss: 1.150674, acc: 0.406250]\n",
      "3275: [D loss: 0.222275, acc: 0.914062]  [G loss: 0.740917, acc: 0.531250]\n",
      "3276: [D loss: 0.189079, acc: 0.906250]  [G loss: 1.011974, acc: 0.406250]\n",
      "3277: [D loss: 0.221812, acc: 0.890625]  [G loss: 1.199108, acc: 0.406250]\n",
      "3278: [D loss: 0.172613, acc: 0.914062]  [G loss: 0.508794, acc: 0.765625]\n",
      "3279: [D loss: 0.362547, acc: 0.843750]  [G loss: 1.687368, acc: 0.265625]\n",
      "3280: [D loss: 0.296604, acc: 0.867188]  [G loss: 0.576503, acc: 0.640625]\n",
      "3281: [D loss: 0.188221, acc: 0.937500]  [G loss: 1.088997, acc: 0.437500]\n",
      "3282: [D loss: 0.183478, acc: 0.898438]  [G loss: 0.671159, acc: 0.687500]\n",
      "3283: [D loss: 0.177451, acc: 0.937500]  [G loss: 0.603168, acc: 0.687500]\n",
      "3284: [D loss: 0.173205, acc: 0.914062]  [G loss: 0.615512, acc: 0.750000]\n",
      "3285: [D loss: 0.306626, acc: 0.859375]  [G loss: 0.918845, acc: 0.546875]\n",
      "3286: [D loss: 0.246206, acc: 0.882812]  [G loss: 0.535799, acc: 0.703125]\n",
      "3287: [D loss: 0.192870, acc: 0.914062]  [G loss: 1.034816, acc: 0.500000]\n",
      "3288: [D loss: 0.271586, acc: 0.875000]  [G loss: 0.987706, acc: 0.562500]\n",
      "3289: [D loss: 0.339285, acc: 0.859375]  [G loss: 0.485541, acc: 0.750000]\n",
      "3290: [D loss: 0.208346, acc: 0.906250]  [G loss: 0.821963, acc: 0.656250]\n",
      "3291: [D loss: 0.226419, acc: 0.906250]  [G loss: 0.630382, acc: 0.687500]\n",
      "3292: [D loss: 0.299307, acc: 0.875000]  [G loss: 1.859652, acc: 0.312500]\n",
      "3293: [D loss: 0.473266, acc: 0.796875]  [G loss: 0.143092, acc: 0.953125]\n",
      "3294: [D loss: 0.615969, acc: 0.781250]  [G loss: 2.060525, acc: 0.250000]\n",
      "3295: [D loss: 0.394616, acc: 0.851562]  [G loss: 0.465044, acc: 0.750000]\n",
      "3296: [D loss: 0.347150, acc: 0.859375]  [G loss: 1.571787, acc: 0.250000]\n",
      "3297: [D loss: 0.203590, acc: 0.937500]  [G loss: 0.759950, acc: 0.593750]\n",
      "3298: [D loss: 0.272875, acc: 0.875000]  [G loss: 1.405668, acc: 0.281250]\n",
      "3299: [D loss: 0.274357, acc: 0.859375]  [G loss: 0.629010, acc: 0.593750]\n",
      "3300: [D loss: 0.209608, acc: 0.921875]  [G loss: 1.137971, acc: 0.390625]\n",
      "3301: [D loss: 0.227404, acc: 0.882812]  [G loss: 0.634575, acc: 0.625000]\n",
      "3302: [D loss: 0.203199, acc: 0.914062]  [G loss: 0.869371, acc: 0.562500]\n",
      "3303: [D loss: 0.223031, acc: 0.898438]  [G loss: 1.235400, acc: 0.406250]\n",
      "3304: [D loss: 0.221674, acc: 0.906250]  [G loss: 0.501536, acc: 0.718750]\n",
      "3305: [D loss: 0.186572, acc: 0.937500]  [G loss: 0.839623, acc: 0.515625]\n",
      "3306: [D loss: 0.232825, acc: 0.921875]  [G loss: 0.529195, acc: 0.656250]\n",
      "3307: [D loss: 0.278019, acc: 0.843750]  [G loss: 1.085460, acc: 0.437500]\n",
      "3308: [D loss: 0.163734, acc: 0.937500]  [G loss: 0.970496, acc: 0.515625]\n",
      "3309: [D loss: 0.194616, acc: 0.929688]  [G loss: 0.639587, acc: 0.625000]\n",
      "3310: [D loss: 0.290465, acc: 0.859375]  [G loss: 1.116308, acc: 0.468750]\n",
      "3311: [D loss: 0.312463, acc: 0.851562]  [G loss: 0.344819, acc: 0.875000]\n",
      "3312: [D loss: 0.413011, acc: 0.835938]  [G loss: 1.612648, acc: 0.312500]\n",
      "3313: [D loss: 0.354183, acc: 0.859375]  [G loss: 0.520846, acc: 0.781250]\n",
      "3314: [D loss: 0.260710, acc: 0.890625]  [G loss: 1.261505, acc: 0.390625]\n",
      "3315: [D loss: 0.178744, acc: 0.953125]  [G loss: 0.769783, acc: 0.562500]\n",
      "3316: [D loss: 0.124958, acc: 0.960938]  [G loss: 0.904082, acc: 0.593750]\n",
      "3317: [D loss: 0.311321, acc: 0.882812]  [G loss: 1.202049, acc: 0.500000]\n",
      "3318: [D loss: 0.365853, acc: 0.851562]  [G loss: 0.365366, acc: 0.781250]\n",
      "3319: [D loss: 0.303570, acc: 0.851562]  [G loss: 2.027724, acc: 0.218750]\n",
      "3320: [D loss: 0.286412, acc: 0.867188]  [G loss: 0.840626, acc: 0.578125]\n",
      "3321: [D loss: 0.227112, acc: 0.898438]  [G loss: 2.090694, acc: 0.265625]\n",
      "3322: [D loss: 0.329923, acc: 0.851562]  [G loss: 0.616788, acc: 0.703125]\n",
      "3323: [D loss: 0.163344, acc: 0.945312]  [G loss: 0.836986, acc: 0.468750]\n",
      "3324: [D loss: 0.183547, acc: 0.921875]  [G loss: 0.392392, acc: 0.796875]\n",
      "3325: [D loss: 0.220580, acc: 0.898438]  [G loss: 1.289382, acc: 0.453125]\n",
      "3326: [D loss: 0.280705, acc: 0.890625]  [G loss: 0.482640, acc: 0.765625]\n",
      "3327: [D loss: 0.107623, acc: 0.968750]  [G loss: 0.638293, acc: 0.625000]\n",
      "3328: [D loss: 0.217144, acc: 0.906250]  [G loss: 0.837664, acc: 0.625000]\n",
      "3329: [D loss: 0.181301, acc: 0.921875]  [G loss: 0.619160, acc: 0.687500]\n",
      "3330: [D loss: 0.229592, acc: 0.898438]  [G loss: 1.145055, acc: 0.562500]\n",
      "3331: [D loss: 0.265481, acc: 0.882812]  [G loss: 0.550377, acc: 0.718750]\n",
      "3332: [D loss: 0.198953, acc: 0.921875]  [G loss: 1.445912, acc: 0.500000]\n",
      "3333: [D loss: 0.253884, acc: 0.898438]  [G loss: 0.459977, acc: 0.734375]\n",
      "3334: [D loss: 0.282491, acc: 0.890625]  [G loss: 1.302010, acc: 0.500000]\n",
      "3335: [D loss: 0.246295, acc: 0.906250]  [G loss: 0.410155, acc: 0.828125]\n",
      "3336: [D loss: 0.292644, acc: 0.851562]  [G loss: 1.333650, acc: 0.531250]\n",
      "3337: [D loss: 0.462739, acc: 0.804688]  [G loss: 1.052636, acc: 0.484375]\n",
      "3338: [D loss: 0.203136, acc: 0.914062]  [G loss: 0.372355, acc: 0.843750]\n",
      "3339: [D loss: 0.230808, acc: 0.906250]  [G loss: 0.731546, acc: 0.609375]\n",
      "3340: [D loss: 0.162034, acc: 0.929688]  [G loss: 0.231908, acc: 0.906250]\n",
      "3341: [D loss: 0.160372, acc: 0.921875]  [G loss: 0.740930, acc: 0.718750]\n",
      "3342: [D loss: 0.182810, acc: 0.953125]  [G loss: 0.621941, acc: 0.703125]\n",
      "3343: [D loss: 0.084386, acc: 0.968750]  [G loss: 0.309795, acc: 0.875000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3344: [D loss: 0.185121, acc: 0.929688]  [G loss: 0.923853, acc: 0.515625]\n",
      "3345: [D loss: 0.251408, acc: 0.882812]  [G loss: 0.393316, acc: 0.812500]\n",
      "3346: [D loss: 0.260179, acc: 0.890625]  [G loss: 1.591883, acc: 0.312500]\n",
      "3347: [D loss: 0.261478, acc: 0.906250]  [G loss: 0.209775, acc: 0.906250]\n",
      "3348: [D loss: 0.278104, acc: 0.898438]  [G loss: 2.207683, acc: 0.328125]\n",
      "3349: [D loss: 0.182739, acc: 0.945312]  [G loss: 0.651541, acc: 0.625000]\n",
      "3350: [D loss: 0.199839, acc: 0.921875]  [G loss: 0.256431, acc: 0.843750]\n",
      "3351: [D loss: 0.299819, acc: 0.867188]  [G loss: 1.351374, acc: 0.453125]\n",
      "3352: [D loss: 0.352214, acc: 0.843750]  [G loss: 1.339660, acc: 0.406250]\n",
      "3353: [D loss: 0.240337, acc: 0.914062]  [G loss: 0.529672, acc: 0.703125]\n",
      "3354: [D loss: 0.293005, acc: 0.898438]  [G loss: 0.676457, acc: 0.578125]\n",
      "3355: [D loss: 0.306602, acc: 0.882812]  [G loss: 1.046912, acc: 0.453125]\n",
      "3356: [D loss: 0.253314, acc: 0.875000]  [G loss: 1.111500, acc: 0.453125]\n",
      "3357: [D loss: 0.171006, acc: 0.921875]  [G loss: 0.226426, acc: 0.921875]\n",
      "3358: [D loss: 0.246619, acc: 0.890625]  [G loss: 1.995008, acc: 0.343750]\n",
      "3359: [D loss: 0.251580, acc: 0.914062]  [G loss: 0.805358, acc: 0.656250]\n",
      "3360: [D loss: 0.272083, acc: 0.890625]  [G loss: 0.656159, acc: 0.640625]\n",
      "3361: [D loss: 0.166254, acc: 0.914062]  [G loss: 0.699856, acc: 0.593750]\n",
      "3362: [D loss: 0.208719, acc: 0.890625]  [G loss: 0.604046, acc: 0.625000]\n",
      "3363: [D loss: 0.187037, acc: 0.929688]  [G loss: 0.189217, acc: 0.875000]\n",
      "3364: [D loss: 0.190511, acc: 0.914062]  [G loss: 1.369480, acc: 0.359375]\n",
      "3365: [D loss: 0.226487, acc: 0.890625]  [G loss: 0.606818, acc: 0.656250]\n",
      "3366: [D loss: 0.197330, acc: 0.914062]  [G loss: 0.612136, acc: 0.671875]\n",
      "3367: [D loss: 0.154034, acc: 0.937500]  [G loss: 0.640829, acc: 0.703125]\n",
      "3368: [D loss: 0.242918, acc: 0.906250]  [G loss: 0.526456, acc: 0.718750]\n",
      "3369: [D loss: 0.206366, acc: 0.937500]  [G loss: 2.699636, acc: 0.125000]\n",
      "3370: [D loss: 0.356316, acc: 0.875000]  [G loss: 0.166956, acc: 0.968750]\n",
      "3371: [D loss: 0.193865, acc: 0.914062]  [G loss: 1.248581, acc: 0.390625]\n",
      "3372: [D loss: 0.274342, acc: 0.882812]  [G loss: 0.724324, acc: 0.656250]\n",
      "3373: [D loss: 0.206502, acc: 0.921875]  [G loss: 1.065731, acc: 0.500000]\n",
      "3374: [D loss: 0.177390, acc: 0.929688]  [G loss: 0.214934, acc: 0.921875]\n",
      "3375: [D loss: 0.155622, acc: 0.945312]  [G loss: 2.830910, acc: 0.234375]\n",
      "3376: [D loss: 0.202805, acc: 0.937500]  [G loss: 0.337384, acc: 0.875000]\n",
      "3377: [D loss: 0.175273, acc: 0.945312]  [G loss: 0.749035, acc: 0.546875]\n",
      "3378: [D loss: 0.052238, acc: 0.976562]  [G loss: 0.423443, acc: 0.781250]\n",
      "3379: [D loss: 0.179934, acc: 0.914062]  [G loss: 1.312991, acc: 0.437500]\n",
      "3380: [D loss: 0.204460, acc: 0.906250]  [G loss: 1.356910, acc: 0.343750]\n",
      "3381: [D loss: 0.175974, acc: 0.937500]  [G loss: 0.231039, acc: 0.890625]\n",
      "3382: [D loss: 0.196995, acc: 0.921875]  [G loss: 0.960080, acc: 0.546875]\n",
      "3383: [D loss: 0.204362, acc: 0.882812]  [G loss: 0.489700, acc: 0.796875]\n",
      "3384: [D loss: 0.246581, acc: 0.898438]  [G loss: 1.188443, acc: 0.406250]\n",
      "3385: [D loss: 0.212114, acc: 0.929688]  [G loss: 0.088438, acc: 0.937500]\n",
      "3386: [D loss: 0.258311, acc: 0.929688]  [G loss: 1.197875, acc: 0.375000]\n",
      "3387: [D loss: 0.234074, acc: 0.898438]  [G loss: 0.271206, acc: 0.859375]\n",
      "3388: [D loss: 0.151079, acc: 0.945312]  [G loss: 0.163193, acc: 0.937500]\n",
      "3389: [D loss: 0.136042, acc: 0.945312]  [G loss: 0.369057, acc: 0.828125]\n",
      "3390: [D loss: 0.217238, acc: 0.898438]  [G loss: 2.060017, acc: 0.187500]\n",
      "3391: [D loss: 0.419771, acc: 0.828125]  [G loss: 0.153733, acc: 0.937500]\n",
      "3392: [D loss: 0.297291, acc: 0.867188]  [G loss: 1.545493, acc: 0.171875]\n",
      "3393: [D loss: 0.162943, acc: 0.929688]  [G loss: 0.369725, acc: 0.859375]\n",
      "3394: [D loss: 0.132909, acc: 0.960938]  [G loss: 0.134278, acc: 0.953125]\n",
      "3395: [D loss: 0.256388, acc: 0.851562]  [G loss: 0.716349, acc: 0.671875]\n",
      "3396: [D loss: 0.233591, acc: 0.882812]  [G loss: 0.840164, acc: 0.515625]\n",
      "3397: [D loss: 0.257847, acc: 0.898438]  [G loss: 0.849398, acc: 0.625000]\n",
      "3398: [D loss: 0.123312, acc: 0.945312]  [G loss: 0.482202, acc: 0.687500]\n",
      "3399: [D loss: 0.308134, acc: 0.843750]  [G loss: 1.723822, acc: 0.328125]\n",
      "3400: [D loss: 0.349415, acc: 0.882812]  [G loss: 0.386955, acc: 0.796875]\n",
      "3401: [D loss: 0.391642, acc: 0.828125]  [G loss: 1.688718, acc: 0.265625]\n",
      "3402: [D loss: 0.217673, acc: 0.898438]  [G loss: 0.793676, acc: 0.578125]\n",
      "3403: [D loss: 0.307049, acc: 0.851562]  [G loss: 0.338318, acc: 0.859375]\n",
      "3404: [D loss: 0.322619, acc: 0.835938]  [G loss: 1.890088, acc: 0.156250]\n",
      "3405: [D loss: 0.190785, acc: 0.921875]  [G loss: 0.444111, acc: 0.812500]\n",
      "3406: [D loss: 0.170628, acc: 0.929688]  [G loss: 0.930236, acc: 0.500000]\n",
      "3407: [D loss: 0.204219, acc: 0.929688]  [G loss: 0.187316, acc: 0.921875]\n",
      "3408: [D loss: 0.549929, acc: 0.781250]  [G loss: 1.892051, acc: 0.250000]\n",
      "3409: [D loss: 0.346076, acc: 0.859375]  [G loss: 0.700910, acc: 0.640625]\n",
      "3410: [D loss: 0.327440, acc: 0.851562]  [G loss: 1.501296, acc: 0.156250]\n",
      "3411: [D loss: 0.221662, acc: 0.890625]  [G loss: 0.877444, acc: 0.437500]\n",
      "3412: [D loss: 0.270196, acc: 0.890625]  [G loss: 0.794251, acc: 0.531250]\n",
      "3413: [D loss: 0.180351, acc: 0.937500]  [G loss: 0.638467, acc: 0.671875]\n",
      "3414: [D loss: 0.240215, acc: 0.914062]  [G loss: 0.388967, acc: 0.812500]\n",
      "3415: [D loss: 0.165605, acc: 0.929688]  [G loss: 0.484989, acc: 0.750000]\n",
      "3416: [D loss: 0.131836, acc: 0.953125]  [G loss: 0.253785, acc: 0.906250]\n",
      "3417: [D loss: 0.141010, acc: 0.976562]  [G loss: 1.128276, acc: 0.421875]\n",
      "3418: [D loss: 0.210086, acc: 0.875000]  [G loss: 0.587529, acc: 0.718750]\n",
      "3419: [D loss: 0.087645, acc: 0.968750]  [G loss: 0.120032, acc: 0.968750]\n",
      "3420: [D loss: 0.143887, acc: 0.953125]  [G loss: 0.200660, acc: 0.953125]\n",
      "3421: [D loss: 0.085886, acc: 0.968750]  [G loss: 0.110752, acc: 0.937500]\n",
      "3422: [D loss: 0.401585, acc: 0.851562]  [G loss: 2.690341, acc: 0.109375]\n",
      "3423: [D loss: 0.522006, acc: 0.828125]  [G loss: 0.193929, acc: 0.921875]\n",
      "3424: [D loss: 0.592156, acc: 0.718750]  [G loss: 2.471148, acc: 0.125000]\n",
      "3425: [D loss: 0.275308, acc: 0.867188]  [G loss: 1.178998, acc: 0.359375]\n",
      "3426: [D loss: 0.147339, acc: 0.929688]  [G loss: 0.729839, acc: 0.593750]\n",
      "3427: [D loss: 0.230078, acc: 0.898438]  [G loss: 0.960587, acc: 0.453125]\n",
      "3428: [D loss: 0.229595, acc: 0.867188]  [G loss: 0.801372, acc: 0.546875]\n",
      "3429: [D loss: 0.220638, acc: 0.906250]  [G loss: 0.635788, acc: 0.656250]\n",
      "3430: [D loss: 0.222347, acc: 0.921875]  [G loss: 0.317522, acc: 0.875000]\n",
      "3431: [D loss: 0.245840, acc: 0.867188]  [G loss: 1.062970, acc: 0.531250]\n",
      "3432: [D loss: 0.170763, acc: 0.921875]  [G loss: 0.452995, acc: 0.765625]\n",
      "3433: [D loss: 0.183317, acc: 0.914062]  [G loss: 0.555462, acc: 0.703125]\n",
      "3434: [D loss: 0.199512, acc: 0.898438]  [G loss: 0.658687, acc: 0.609375]\n",
      "3435: [D loss: 0.167645, acc: 0.929688]  [G loss: 0.430050, acc: 0.765625]\n",
      "3436: [D loss: 0.229181, acc: 0.906250]  [G loss: 1.012361, acc: 0.531250]\n",
      "3437: [D loss: 0.208464, acc: 0.890625]  [G loss: 0.804433, acc: 0.687500]\n",
      "3438: [D loss: 0.183491, acc: 0.921875]  [G loss: 1.031286, acc: 0.484375]\n",
      "3439: [D loss: 0.208402, acc: 0.914062]  [G loss: 0.914962, acc: 0.468750]\n",
      "3440: [D loss: 0.218281, acc: 0.906250]  [G loss: 1.537015, acc: 0.312500]\n",
      "3441: [D loss: 0.197207, acc: 0.906250]  [G loss: 0.677148, acc: 0.578125]\n",
      "3442: [D loss: 0.183479, acc: 0.929688]  [G loss: 0.878104, acc: 0.562500]\n",
      "3443: [D loss: 0.207951, acc: 0.921875]  [G loss: 1.728241, acc: 0.156250]\n",
      "3444: [D loss: 0.155132, acc: 0.937500]  [G loss: 0.489388, acc: 0.750000]\n",
      "3445: [D loss: 0.199035, acc: 0.906250]  [G loss: 1.429778, acc: 0.359375]\n",
      "3446: [D loss: 0.226298, acc: 0.898438]  [G loss: 0.385896, acc: 0.781250]\n",
      "3447: [D loss: 0.202313, acc: 0.882812]  [G loss: 1.195977, acc: 0.437500]\n",
      "3448: [D loss: 0.253259, acc: 0.890625]  [G loss: 0.268790, acc: 0.796875]\n",
      "3449: [D loss: 0.264827, acc: 0.882812]  [G loss: 1.393892, acc: 0.296875]\n",
      "3450: [D loss: 0.331853, acc: 0.867188]  [G loss: 0.993267, acc: 0.531250]\n",
      "3451: [D loss: 0.348349, acc: 0.859375]  [G loss: 0.692934, acc: 0.656250]\n",
      "3452: [D loss: 0.197648, acc: 0.937500]  [G loss: 0.562963, acc: 0.687500]\n",
      "3453: [D loss: 0.134378, acc: 0.968750]  [G loss: 0.508677, acc: 0.703125]\n",
      "3454: [D loss: 0.178279, acc: 0.937500]  [G loss: 0.881105, acc: 0.578125]\n",
      "3455: [D loss: 0.180149, acc: 0.929688]  [G loss: 0.955030, acc: 0.437500]\n",
      "3456: [D loss: 0.183818, acc: 0.921875]  [G loss: 0.737001, acc: 0.546875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3457: [D loss: 0.138893, acc: 0.945312]  [G loss: 0.684198, acc: 0.609375]\n",
      "3458: [D loss: 0.175092, acc: 0.937500]  [G loss: 1.535200, acc: 0.250000]\n",
      "3459: [D loss: 0.141732, acc: 0.929688]  [G loss: 1.805126, acc: 0.250000]\n",
      "3460: [D loss: 0.203001, acc: 0.921875]  [G loss: 0.619300, acc: 0.718750]\n",
      "3461: [D loss: 0.680042, acc: 0.804688]  [G loss: 3.174192, acc: 0.125000]\n",
      "3462: [D loss: 0.461916, acc: 0.835938]  [G loss: 1.063909, acc: 0.500000]\n",
      "3463: [D loss: 0.258367, acc: 0.898438]  [G loss: 1.488761, acc: 0.218750]\n",
      "3464: [D loss: 0.167060, acc: 0.929688]  [G loss: 0.581542, acc: 0.671875]\n",
      "3465: [D loss: 0.120015, acc: 0.960938]  [G loss: 0.830440, acc: 0.500000]\n",
      "3466: [D loss: 0.117080, acc: 0.968750]  [G loss: 0.752949, acc: 0.593750]\n",
      "3467: [D loss: 0.127703, acc: 0.960938]  [G loss: 1.196972, acc: 0.328125]\n",
      "3468: [D loss: 0.130489, acc: 0.945312]  [G loss: 0.401569, acc: 0.796875]\n",
      "3469: [D loss: 0.078596, acc: 0.992188]  [G loss: 1.050230, acc: 0.562500]\n",
      "3470: [D loss: 0.083536, acc: 0.976562]  [G loss: 0.549991, acc: 0.765625]\n",
      "3471: [D loss: 0.136426, acc: 0.953125]  [G loss: 0.981865, acc: 0.546875]\n",
      "3472: [D loss: 0.167253, acc: 0.929688]  [G loss: 0.359867, acc: 0.796875]\n",
      "3473: [D loss: 0.109666, acc: 0.960938]  [G loss: 0.738435, acc: 0.593750]\n",
      "3474: [D loss: 0.076874, acc: 0.976562]  [G loss: 0.384571, acc: 0.734375]\n",
      "3475: [D loss: 0.068429, acc: 0.984375]  [G loss: 0.339682, acc: 0.875000]\n",
      "3476: [D loss: 0.197617, acc: 0.921875]  [G loss: 2.309527, acc: 0.171875]\n",
      "3477: [D loss: 0.256958, acc: 0.898438]  [G loss: 0.280428, acc: 0.890625]\n",
      "3478: [D loss: 0.665736, acc: 0.820312]  [G loss: 2.769084, acc: 0.109375]\n",
      "3479: [D loss: 0.207933, acc: 0.929688]  [G loss: 0.863185, acc: 0.500000]\n",
      "3480: [D loss: 0.129763, acc: 0.937500]  [G loss: 1.378710, acc: 0.312500]\n",
      "3481: [D loss: 0.068903, acc: 0.984375]  [G loss: 0.528880, acc: 0.734375]\n",
      "3482: [D loss: 0.170679, acc: 0.929688]  [G loss: 1.275764, acc: 0.375000]\n",
      "3483: [D loss: 0.154553, acc: 0.937500]  [G loss: 0.992611, acc: 0.578125]\n",
      "3484: [D loss: 0.138603, acc: 0.953125]  [G loss: 0.730056, acc: 0.687500]\n",
      "3485: [D loss: 0.078037, acc: 0.968750]  [G loss: 0.425665, acc: 0.765625]\n",
      "3486: [D loss: 0.114669, acc: 0.960938]  [G loss: 0.360887, acc: 0.796875]\n",
      "3487: [D loss: 0.237613, acc: 0.882812]  [G loss: 3.959124, acc: 0.062500]\n",
      "3488: [D loss: 0.273467, acc: 0.914062]  [G loss: 0.581944, acc: 0.656250]\n",
      "3489: [D loss: 0.185987, acc: 0.914062]  [G loss: 1.491163, acc: 0.296875]\n",
      "3490: [D loss: 0.257611, acc: 0.882812]  [G loss: 0.497200, acc: 0.781250]\n",
      "3491: [D loss: 0.194700, acc: 0.914062]  [G loss: 1.701378, acc: 0.296875]\n",
      "3492: [D loss: 0.167906, acc: 0.921875]  [G loss: 0.290603, acc: 0.875000]\n",
      "3493: [D loss: 0.169892, acc: 0.906250]  [G loss: 0.743113, acc: 0.578125]\n",
      "3494: [D loss: 0.198911, acc: 0.937500]  [G loss: 0.691591, acc: 0.625000]\n",
      "3495: [D loss: 0.149317, acc: 0.937500]  [G loss: 0.656047, acc: 0.703125]\n",
      "3496: [D loss: 0.105534, acc: 0.984375]  [G loss: 2.301653, acc: 0.218750]\n",
      "3497: [D loss: 0.266644, acc: 0.882812]  [G loss: 0.547948, acc: 0.671875]\n",
      "3498: [D loss: 0.097336, acc: 0.960938]  [G loss: 0.436901, acc: 0.750000]\n",
      "3499: [D loss: 0.251732, acc: 0.929688]  [G loss: 2.789494, acc: 0.093750]\n",
      "3500: [D loss: 0.273859, acc: 0.875000]  [G loss: 0.316892, acc: 0.937500]\n",
      "3501: [D loss: 0.369520, acc: 0.859375]  [G loss: 2.982554, acc: 0.218750]\n",
      "3502: [D loss: 0.204912, acc: 0.921875]  [G loss: 0.715042, acc: 0.609375]\n",
      "3503: [D loss: 0.174537, acc: 0.921875]  [G loss: 0.449053, acc: 0.734375]\n",
      "3504: [D loss: 0.083271, acc: 0.968750]  [G loss: 0.513767, acc: 0.765625]\n",
      "3505: [D loss: 0.161509, acc: 0.937500]  [G loss: 1.311924, acc: 0.546875]\n",
      "3506: [D loss: 0.161346, acc: 0.929688]  [G loss: 0.708427, acc: 0.656250]\n",
      "3507: [D loss: 0.163810, acc: 0.945312]  [G loss: 1.193517, acc: 0.609375]\n",
      "3508: [D loss: 0.096714, acc: 0.968750]  [G loss: 0.725122, acc: 0.718750]\n",
      "3509: [D loss: 0.213056, acc: 0.906250]  [G loss: 1.304867, acc: 0.390625]\n",
      "3510: [D loss: 0.198475, acc: 0.937500]  [G loss: 0.337558, acc: 0.859375]\n",
      "3511: [D loss: 0.389936, acc: 0.851562]  [G loss: 1.538302, acc: 0.296875]\n",
      "3512: [D loss: 0.192106, acc: 0.906250]  [G loss: 0.383349, acc: 0.812500]\n",
      "3513: [D loss: 0.219148, acc: 0.906250]  [G loss: 1.744513, acc: 0.359375]\n",
      "3514: [D loss: 0.182280, acc: 0.921875]  [G loss: 0.357303, acc: 0.796875]\n",
      "3515: [D loss: 0.192908, acc: 0.914062]  [G loss: 0.543531, acc: 0.703125]\n",
      "3516: [D loss: 0.195319, acc: 0.914062]  [G loss: 1.715815, acc: 0.375000]\n",
      "3517: [D loss: 0.179684, acc: 0.921875]  [G loss: 0.300674, acc: 0.843750]\n",
      "3518: [D loss: 0.267721, acc: 0.867188]  [G loss: 1.946279, acc: 0.453125]\n",
      "3519: [D loss: 0.186995, acc: 0.929688]  [G loss: 1.310353, acc: 0.375000]\n",
      "3520: [D loss: 0.215243, acc: 0.906250]  [G loss: 0.267463, acc: 0.890625]\n",
      "3521: [D loss: 0.200950, acc: 0.898438]  [G loss: 0.503304, acc: 0.734375]\n",
      "3522: [D loss: 0.158402, acc: 0.914062]  [G loss: 0.347590, acc: 0.796875]\n",
      "3523: [D loss: 0.225187, acc: 0.890625]  [G loss: 2.113890, acc: 0.250000]\n",
      "3524: [D loss: 0.288905, acc: 0.914062]  [G loss: 0.640077, acc: 0.671875]\n",
      "3525: [D loss: 0.239212, acc: 0.890625]  [G loss: 2.210823, acc: 0.203125]\n",
      "3526: [D loss: 0.221882, acc: 0.921875]  [G loss: 0.669079, acc: 0.671875]\n",
      "3527: [D loss: 0.276031, acc: 0.843750]  [G loss: 1.328424, acc: 0.406250]\n",
      "3528: [D loss: 0.191728, acc: 0.921875]  [G loss: 0.757757, acc: 0.640625]\n",
      "3529: [D loss: 0.131532, acc: 0.953125]  [G loss: 0.433443, acc: 0.765625]\n",
      "3530: [D loss: 0.301423, acc: 0.875000]  [G loss: 1.298116, acc: 0.468750]\n",
      "3531: [D loss: 0.289817, acc: 0.898438]  [G loss: 0.907649, acc: 0.562500]\n",
      "3532: [D loss: 0.367756, acc: 0.828125]  [G loss: 1.435707, acc: 0.421875]\n",
      "3533: [D loss: 0.315819, acc: 0.843750]  [G loss: 0.322405, acc: 0.812500]\n",
      "3534: [D loss: 0.272444, acc: 0.859375]  [G loss: 2.375068, acc: 0.218750]\n",
      "3535: [D loss: 0.261217, acc: 0.890625]  [G loss: 0.756174, acc: 0.609375]\n",
      "3536: [D loss: 0.265139, acc: 0.906250]  [G loss: 0.613369, acc: 0.718750]\n",
      "3537: [D loss: 0.298327, acc: 0.843750]  [G loss: 1.448197, acc: 0.328125]\n",
      "3538: [D loss: 0.243603, acc: 0.890625]  [G loss: 0.731376, acc: 0.609375]\n",
      "3539: [D loss: 0.170288, acc: 0.953125]  [G loss: 0.547985, acc: 0.687500]\n",
      "3540: [D loss: 0.140623, acc: 0.953125]  [G loss: 0.744032, acc: 0.671875]\n",
      "3541: [D loss: 0.160349, acc: 0.945312]  [G loss: 0.386487, acc: 0.765625]\n",
      "3542: [D loss: 0.285043, acc: 0.882812]  [G loss: 1.444047, acc: 0.343750]\n",
      "3543: [D loss: 0.261177, acc: 0.875000]  [G loss: 0.250672, acc: 0.890625]\n",
      "3544: [D loss: 0.233878, acc: 0.929688]  [G loss: 1.042827, acc: 0.593750]\n",
      "3545: [D loss: 0.330058, acc: 0.875000]  [G loss: 0.404126, acc: 0.796875]\n",
      "3546: [D loss: 0.321007, acc: 0.882812]  [G loss: 0.850029, acc: 0.546875]\n",
      "3547: [D loss: 0.228228, acc: 0.914062]  [G loss: 0.528355, acc: 0.734375]\n",
      "3548: [D loss: 0.179951, acc: 0.960938]  [G loss: 0.582373, acc: 0.687500]\n",
      "3549: [D loss: 0.195220, acc: 0.914062]  [G loss: 0.332349, acc: 0.828125]\n",
      "3550: [D loss: 0.175170, acc: 0.906250]  [G loss: 0.356666, acc: 0.843750]\n",
      "3551: [D loss: 0.110129, acc: 0.968750]  [G loss: 0.414836, acc: 0.828125]\n",
      "3552: [D loss: 0.213762, acc: 0.882812]  [G loss: 2.109784, acc: 0.328125]\n",
      "3553: [D loss: 0.300365, acc: 0.867188]  [G loss: 0.348911, acc: 0.812500]\n",
      "3554: [D loss: 0.387616, acc: 0.812500]  [G loss: 1.712477, acc: 0.359375]\n",
      "3555: [D loss: 0.259697, acc: 0.898438]  [G loss: 0.576573, acc: 0.734375]\n",
      "3556: [D loss: 0.147919, acc: 0.960938]  [G loss: 0.643390, acc: 0.609375]\n",
      "3557: [D loss: 0.153227, acc: 0.921875]  [G loss: 1.083028, acc: 0.531250]\n",
      "3558: [D loss: 0.277003, acc: 0.875000]  [G loss: 0.215859, acc: 0.875000]\n",
      "3559: [D loss: 0.189234, acc: 0.929688]  [G loss: 0.750266, acc: 0.531250]\n",
      "3560: [D loss: 0.191111, acc: 0.937500]  [G loss: 1.060720, acc: 0.375000]\n",
      "3561: [D loss: 0.177264, acc: 0.929688]  [G loss: 0.410802, acc: 0.765625]\n",
      "3562: [D loss: 0.172908, acc: 0.937500]  [G loss: 0.628889, acc: 0.687500]\n",
      "3563: [D loss: 0.166951, acc: 0.937500]  [G loss: 0.559327, acc: 0.703125]\n",
      "3564: [D loss: 0.176704, acc: 0.929688]  [G loss: 0.342895, acc: 0.812500]\n",
      "3565: [D loss: 0.231842, acc: 0.914062]  [G loss: 0.946242, acc: 0.578125]\n",
      "3566: [D loss: 0.180530, acc: 0.945312]  [G loss: 0.343744, acc: 0.765625]\n",
      "3567: [D loss: 0.242873, acc: 0.875000]  [G loss: 2.953999, acc: 0.109375]\n",
      "3568: [D loss: 0.289054, acc: 0.890625]  [G loss: 0.337293, acc: 0.859375]\n",
      "3569: [D loss: 0.258539, acc: 0.875000]  [G loss: 0.722950, acc: 0.562500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3570: [D loss: 0.156261, acc: 0.960938]  [G loss: 0.696172, acc: 0.640625]\n",
      "3571: [D loss: 0.089432, acc: 0.968750]  [G loss: 0.499973, acc: 0.765625]\n",
      "3572: [D loss: 0.152025, acc: 0.945312]  [G loss: 1.722854, acc: 0.250000]\n",
      "3573: [D loss: 0.154934, acc: 0.945312]  [G loss: 0.283383, acc: 0.906250]\n",
      "3574: [D loss: 0.105188, acc: 0.953125]  [G loss: 0.667555, acc: 0.593750]\n",
      "3575: [D loss: 0.204917, acc: 0.914062]  [G loss: 0.268225, acc: 0.921875]\n",
      "3576: [D loss: 0.134778, acc: 0.945312]  [G loss: 0.620812, acc: 0.656250]\n",
      "3577: [D loss: 0.097276, acc: 0.960938]  [G loss: 0.177562, acc: 0.921875]\n",
      "3578: [D loss: 0.125843, acc: 0.937500]  [G loss: 0.214291, acc: 0.937500]\n",
      "3579: [D loss: 0.122614, acc: 0.960938]  [G loss: 0.580467, acc: 0.671875]\n",
      "3580: [D loss: 0.110504, acc: 0.937500]  [G loss: 0.424304, acc: 0.796875]\n",
      "3581: [D loss: 0.082920, acc: 0.976562]  [G loss: 0.566344, acc: 0.718750]\n",
      "3582: [D loss: 0.091649, acc: 0.968750]  [G loss: 0.269114, acc: 0.859375]\n",
      "3583: [D loss: 0.443552, acc: 0.828125]  [G loss: 8.092165, acc: 0.000000]\n",
      "3584: [D loss: 0.443446, acc: 0.843750]  [G loss: 1.638505, acc: 0.546875]\n",
      "3585: [D loss: 0.361989, acc: 0.875000]  [G loss: 2.756207, acc: 0.078125]\n",
      "3586: [D loss: 0.398482, acc: 0.859375]  [G loss: 0.791691, acc: 0.484375]\n",
      "3587: [D loss: 0.270806, acc: 0.875000]  [G loss: 0.844378, acc: 0.500000]\n",
      "3588: [D loss: 0.318013, acc: 0.867188]  [G loss: 1.093044, acc: 0.390625]\n",
      "3589: [D loss: 0.171788, acc: 0.937500]  [G loss: 0.727979, acc: 0.546875]\n",
      "3590: [D loss: 0.166903, acc: 0.929688]  [G loss: 0.447421, acc: 0.765625]\n",
      "3591: [D loss: 0.208680, acc: 0.914062]  [G loss: 0.825241, acc: 0.515625]\n",
      "3592: [D loss: 0.183570, acc: 0.898438]  [G loss: 0.591952, acc: 0.703125]\n",
      "3593: [D loss: 0.248474, acc: 0.906250]  [G loss: 0.731952, acc: 0.578125]\n",
      "3594: [D loss: 0.131395, acc: 0.953125]  [G loss: 0.585296, acc: 0.625000]\n",
      "3595: [D loss: 0.185921, acc: 0.906250]  [G loss: 0.173653, acc: 0.921875]\n",
      "3596: [D loss: 0.146640, acc: 0.953125]  [G loss: 0.650090, acc: 0.609375]\n",
      "3597: [D loss: 0.186884, acc: 0.914062]  [G loss: 0.598323, acc: 0.609375]\n",
      "3598: [D loss: 0.132899, acc: 0.929688]  [G loss: 0.540091, acc: 0.718750]\n",
      "3599: [D loss: 0.214164, acc: 0.914062]  [G loss: 1.690504, acc: 0.265625]\n",
      "3600: [D loss: 0.292168, acc: 0.875000]  [G loss: 0.294506, acc: 0.843750]\n",
      "3601: [D loss: 0.240674, acc: 0.906250]  [G loss: 0.489659, acc: 0.750000]\n",
      "3602: [D loss: 0.207614, acc: 0.937500]  [G loss: 1.295295, acc: 0.437500]\n",
      "3603: [D loss: 0.202378, acc: 0.914062]  [G loss: 0.327865, acc: 0.812500]\n",
      "3604: [D loss: 0.318235, acc: 0.851562]  [G loss: 1.784397, acc: 0.312500]\n",
      "3605: [D loss: 0.325918, acc: 0.867188]  [G loss: 1.352893, acc: 0.250000]\n",
      "3606: [D loss: 0.206628, acc: 0.929688]  [G loss: 0.765688, acc: 0.625000]\n",
      "3607: [D loss: 0.174148, acc: 0.937500]  [G loss: 1.193973, acc: 0.421875]\n",
      "3608: [D loss: 0.195123, acc: 0.937500]  [G loss: 0.728933, acc: 0.593750]\n",
      "3609: [D loss: 0.177026, acc: 0.937500]  [G loss: 1.229399, acc: 0.421875]\n",
      "3610: [D loss: 0.220284, acc: 0.875000]  [G loss: 0.775141, acc: 0.609375]\n",
      "3611: [D loss: 0.245052, acc: 0.898438]  [G loss: 1.269076, acc: 0.484375]\n",
      "3612: [D loss: 0.324189, acc: 0.898438]  [G loss: 0.242535, acc: 0.890625]\n",
      "3613: [D loss: 0.391564, acc: 0.828125]  [G loss: 2.033351, acc: 0.171875]\n",
      "3614: [D loss: 0.236162, acc: 0.914062]  [G loss: 0.643552, acc: 0.609375]\n",
      "3615: [D loss: 0.226705, acc: 0.914062]  [G loss: 0.350969, acc: 0.859375]\n",
      "3616: [D loss: 0.302545, acc: 0.882812]  [G loss: 1.168564, acc: 0.453125]\n",
      "3617: [D loss: 0.408769, acc: 0.820312]  [G loss: 0.210330, acc: 0.906250]\n",
      "3618: [D loss: 0.309039, acc: 0.851562]  [G loss: 1.495565, acc: 0.343750]\n",
      "3619: [D loss: 0.311021, acc: 0.851562]  [G loss: 0.361184, acc: 0.796875]\n",
      "3620: [D loss: 0.194358, acc: 0.906250]  [G loss: 0.834129, acc: 0.562500]\n",
      "3621: [D loss: 0.158390, acc: 0.929688]  [G loss: 0.423742, acc: 0.781250]\n",
      "3622: [D loss: 0.163215, acc: 0.945312]  [G loss: 0.975008, acc: 0.468750]\n",
      "3623: [D loss: 0.185653, acc: 0.906250]  [G loss: 0.700038, acc: 0.593750]\n",
      "3624: [D loss: 0.179692, acc: 0.921875]  [G loss: 0.537938, acc: 0.703125]\n",
      "3625: [D loss: 0.165483, acc: 0.914062]  [G loss: 1.099726, acc: 0.625000]\n",
      "3626: [D loss: 0.187106, acc: 0.937500]  [G loss: 0.938721, acc: 0.515625]\n",
      "3627: [D loss: 0.145391, acc: 0.929688]  [G loss: 0.572929, acc: 0.718750]\n",
      "3628: [D loss: 0.207580, acc: 0.929688]  [G loss: 1.108407, acc: 0.500000]\n",
      "3629: [D loss: 0.101216, acc: 0.984375]  [G loss: 0.409669, acc: 0.828125]\n",
      "3630: [D loss: 0.123573, acc: 0.953125]  [G loss: 0.808788, acc: 0.625000]\n",
      "3631: [D loss: 0.119364, acc: 0.960938]  [G loss: 0.821441, acc: 0.671875]\n",
      "3632: [D loss: 0.186954, acc: 0.921875]  [G loss: 0.388238, acc: 0.812500]\n",
      "3633: [D loss: 0.199884, acc: 0.937500]  [G loss: 1.708168, acc: 0.343750]\n",
      "3634: [D loss: 0.244500, acc: 0.898438]  [G loss: 0.313713, acc: 0.812500]\n",
      "3635: [D loss: 0.264623, acc: 0.867188]  [G loss: 2.002285, acc: 0.234375]\n",
      "3636: [D loss: 0.271857, acc: 0.898438]  [G loss: 0.646470, acc: 0.687500]\n",
      "3637: [D loss: 0.159319, acc: 0.937500]  [G loss: 0.701676, acc: 0.562500]\n",
      "3638: [D loss: 0.159034, acc: 0.937500]  [G loss: 1.145799, acc: 0.421875]\n",
      "3639: [D loss: 0.231774, acc: 0.898438]  [G loss: 0.430099, acc: 0.750000]\n",
      "3640: [D loss: 0.220014, acc: 0.898438]  [G loss: 1.338991, acc: 0.359375]\n",
      "3641: [D loss: 0.111375, acc: 0.960938]  [G loss: 0.677117, acc: 0.656250]\n",
      "3642: [D loss: 0.168106, acc: 0.960938]  [G loss: 0.564866, acc: 0.687500]\n",
      "3643: [D loss: 0.097779, acc: 0.968750]  [G loss: 0.962761, acc: 0.546875]\n",
      "3644: [D loss: 0.181695, acc: 0.914062]  [G loss: 2.617018, acc: 0.156250]\n",
      "3645: [D loss: 0.277783, acc: 0.898438]  [G loss: 0.731325, acc: 0.640625]\n",
      "3646: [D loss: 0.234747, acc: 0.914062]  [G loss: 1.576436, acc: 0.328125]\n",
      "3647: [D loss: 0.166610, acc: 0.914062]  [G loss: 0.558633, acc: 0.718750]\n",
      "3648: [D loss: 0.189451, acc: 0.929688]  [G loss: 1.572852, acc: 0.343750]\n",
      "3649: [D loss: 0.121780, acc: 0.921875]  [G loss: 0.836096, acc: 0.593750]\n",
      "3650: [D loss: 0.178138, acc: 0.914062]  [G loss: 0.725498, acc: 0.687500]\n",
      "3651: [D loss: 0.115744, acc: 0.953125]  [G loss: 0.904632, acc: 0.578125]\n",
      "3652: [D loss: 0.147869, acc: 0.937500]  [G loss: 0.489745, acc: 0.781250]\n",
      "3653: [D loss: 0.369784, acc: 0.882812]  [G loss: 2.274089, acc: 0.093750]\n",
      "3654: [D loss: 0.431518, acc: 0.796875]  [G loss: 0.442186, acc: 0.781250]\n",
      "3655: [D loss: 0.238056, acc: 0.921875]  [G loss: 2.299970, acc: 0.296875]\n",
      "3656: [D loss: 0.151828, acc: 0.937500]  [G loss: 0.833534, acc: 0.609375]\n",
      "3657: [D loss: 0.096368, acc: 0.960938]  [G loss: 0.782796, acc: 0.687500]\n",
      "3658: [D loss: 0.136824, acc: 0.953125]  [G loss: 0.884331, acc: 0.562500]\n",
      "3659: [D loss: 0.165804, acc: 0.929688]  [G loss: 0.547013, acc: 0.718750]\n",
      "3660: [D loss: 0.126979, acc: 0.953125]  [G loss: 0.856086, acc: 0.515625]\n",
      "3661: [D loss: 0.135984, acc: 0.968750]  [G loss: 1.666406, acc: 0.328125]\n",
      "3662: [D loss: 0.239800, acc: 0.898438]  [G loss: 0.752127, acc: 0.531250]\n",
      "3663: [D loss: 0.348632, acc: 0.843750]  [G loss: 3.260442, acc: 0.062500]\n",
      "3664: [D loss: 0.327213, acc: 0.835938]  [G loss: 0.633388, acc: 0.656250]\n",
      "3665: [D loss: 0.234662, acc: 0.921875]  [G loss: 1.931507, acc: 0.328125]\n",
      "3666: [D loss: 0.320845, acc: 0.882812]  [G loss: 1.321340, acc: 0.375000]\n",
      "3667: [D loss: 0.159621, acc: 0.937500]  [G loss: 1.799060, acc: 0.140625]\n",
      "3668: [D loss: 0.235170, acc: 0.914062]  [G loss: 0.648012, acc: 0.687500]\n",
      "3669: [D loss: 0.249238, acc: 0.875000]  [G loss: 2.694588, acc: 0.156250]\n",
      "3670: [D loss: 0.170743, acc: 0.953125]  [G loss: 1.224176, acc: 0.343750]\n",
      "3671: [D loss: 0.309504, acc: 0.898438]  [G loss: 2.715541, acc: 0.171875]\n",
      "3672: [D loss: 0.419248, acc: 0.820312]  [G loss: 0.376840, acc: 0.796875]\n",
      "3673: [D loss: 0.489768, acc: 0.789062]  [G loss: 2.113337, acc: 0.203125]\n",
      "3674: [D loss: 0.172303, acc: 0.914062]  [G loss: 1.029498, acc: 0.468750]\n",
      "3675: [D loss: 0.204752, acc: 0.929688]  [G loss: 1.045039, acc: 0.500000]\n",
      "3676: [D loss: 0.182571, acc: 0.921875]  [G loss: 1.183221, acc: 0.421875]\n",
      "3677: [D loss: 0.161328, acc: 0.945312]  [G loss: 1.091476, acc: 0.437500]\n",
      "3678: [D loss: 0.192178, acc: 0.937500]  [G loss: 0.572247, acc: 0.671875]\n",
      "3679: [D loss: 0.246533, acc: 0.929688]  [G loss: 1.768194, acc: 0.203125]\n",
      "3680: [D loss: 0.244943, acc: 0.906250]  [G loss: 0.664896, acc: 0.687500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3681: [D loss: 0.224673, acc: 0.875000]  [G loss: 1.261881, acc: 0.390625]\n",
      "3682: [D loss: 0.124485, acc: 0.953125]  [G loss: 1.374039, acc: 0.328125]\n",
      "3683: [D loss: 0.230898, acc: 0.921875]  [G loss: 0.329491, acc: 0.796875]\n",
      "3684: [D loss: 0.260219, acc: 0.914062]  [G loss: 2.114803, acc: 0.250000]\n",
      "3685: [D loss: 0.312341, acc: 0.875000]  [G loss: 0.254400, acc: 0.875000]\n",
      "3686: [D loss: 0.385652, acc: 0.867188]  [G loss: 1.484702, acc: 0.359375]\n",
      "3687: [D loss: 0.172707, acc: 0.937500]  [G loss: 0.968487, acc: 0.562500]\n",
      "3688: [D loss: 0.269951, acc: 0.890625]  [G loss: 0.609892, acc: 0.671875]\n",
      "3689: [D loss: 0.232664, acc: 0.875000]  [G loss: 1.299094, acc: 0.531250]\n",
      "3690: [D loss: 0.237005, acc: 0.914062]  [G loss: 0.362323, acc: 0.843750]\n",
      "3691: [D loss: 0.185378, acc: 0.945312]  [G loss: 1.311746, acc: 0.375000]\n",
      "3692: [D loss: 0.281606, acc: 0.875000]  [G loss: 0.667181, acc: 0.703125]\n",
      "3693: [D loss: 0.157633, acc: 0.937500]  [G loss: 0.878922, acc: 0.609375]\n",
      "3694: [D loss: 0.140171, acc: 0.953125]  [G loss: 0.641558, acc: 0.593750]\n",
      "3695: [D loss: 0.196736, acc: 0.945312]  [G loss: 1.038509, acc: 0.484375]\n",
      "3696: [D loss: 0.122488, acc: 0.960938]  [G loss: 0.707208, acc: 0.609375]\n",
      "3697: [D loss: 0.206310, acc: 0.921875]  [G loss: 1.724045, acc: 0.296875]\n",
      "3698: [D loss: 0.366006, acc: 0.835938]  [G loss: 0.306681, acc: 0.828125]\n",
      "3699: [D loss: 0.306551, acc: 0.882812]  [G loss: 1.860418, acc: 0.312500]\n",
      "3700: [D loss: 0.314142, acc: 0.851562]  [G loss: 0.647496, acc: 0.656250]\n",
      "3701: [D loss: 0.300270, acc: 0.929688]  [G loss: 1.061218, acc: 0.546875]\n",
      "3702: [D loss: 0.144974, acc: 0.937500]  [G loss: 0.849671, acc: 0.562500]\n",
      "3703: [D loss: 0.092734, acc: 0.976562]  [G loss: 0.728823, acc: 0.625000]\n",
      "3704: [D loss: 0.199670, acc: 0.929688]  [G loss: 0.987157, acc: 0.500000]\n",
      "3705: [D loss: 0.217858, acc: 0.898438]  [G loss: 1.442349, acc: 0.390625]\n",
      "3706: [D loss: 0.202051, acc: 0.929688]  [G loss: 0.449412, acc: 0.734375]\n",
      "3707: [D loss: 0.204473, acc: 0.929688]  [G loss: 1.112125, acc: 0.484375]\n",
      "3708: [D loss: 0.246317, acc: 0.875000]  [G loss: 0.368448, acc: 0.765625]\n",
      "3709: [D loss: 0.226158, acc: 0.882812]  [G loss: 1.671048, acc: 0.281250]\n",
      "3710: [D loss: 0.313720, acc: 0.867188]  [G loss: 0.343342, acc: 0.812500]\n",
      "3711: [D loss: 0.213272, acc: 0.945312]  [G loss: 0.779765, acc: 0.625000]\n",
      "3712: [D loss: 0.139322, acc: 0.953125]  [G loss: 0.581735, acc: 0.734375]\n",
      "3713: [D loss: 0.307110, acc: 0.882812]  [G loss: 1.762652, acc: 0.328125]\n",
      "3714: [D loss: 0.613497, acc: 0.789062]  [G loss: 0.213435, acc: 0.875000]\n",
      "3715: [D loss: 0.484167, acc: 0.796875]  [G loss: 1.580401, acc: 0.437500]\n",
      "3716: [D loss: 0.267568, acc: 0.867188]  [G loss: 0.629868, acc: 0.671875]\n",
      "3717: [D loss: 0.225547, acc: 0.906250]  [G loss: 0.918488, acc: 0.593750]\n",
      "3718: [D loss: 0.232749, acc: 0.882812]  [G loss: 0.572803, acc: 0.687500]\n",
      "3719: [D loss: 0.204598, acc: 0.914062]  [G loss: 0.788858, acc: 0.578125]\n",
      "3720: [D loss: 0.215297, acc: 0.882812]  [G loss: 0.788396, acc: 0.609375]\n",
      "3721: [D loss: 0.186319, acc: 0.906250]  [G loss: 0.753126, acc: 0.593750]\n",
      "3722: [D loss: 0.230303, acc: 0.898438]  [G loss: 0.780736, acc: 0.578125]\n",
      "3723: [D loss: 0.209159, acc: 0.882812]  [G loss: 0.354215, acc: 0.828125]\n",
      "3724: [D loss: 0.335382, acc: 0.812500]  [G loss: 2.200932, acc: 0.343750]\n",
      "3725: [D loss: 0.363409, acc: 0.843750]  [G loss: 0.739968, acc: 0.687500]\n",
      "3726: [D loss: 0.339051, acc: 0.828125]  [G loss: 0.652357, acc: 0.687500]\n",
      "3727: [D loss: 0.236392, acc: 0.906250]  [G loss: 1.439739, acc: 0.265625]\n",
      "3728: [D loss: 0.306747, acc: 0.859375]  [G loss: 0.627231, acc: 0.640625]\n",
      "3729: [D loss: 0.253051, acc: 0.898438]  [G loss: 1.964142, acc: 0.125000]\n",
      "3730: [D loss: 0.217095, acc: 0.914062]  [G loss: 0.608609, acc: 0.625000]\n",
      "3731: [D loss: 0.255466, acc: 0.875000]  [G loss: 1.645720, acc: 0.281250]\n",
      "3732: [D loss: 0.358609, acc: 0.843750]  [G loss: 0.403862, acc: 0.750000]\n",
      "3733: [D loss: 0.221887, acc: 0.890625]  [G loss: 1.294439, acc: 0.359375]\n",
      "3734: [D loss: 0.254979, acc: 0.875000]  [G loss: 0.477089, acc: 0.796875]\n",
      "3735: [D loss: 0.326342, acc: 0.875000]  [G loss: 2.064808, acc: 0.218750]\n",
      "3736: [D loss: 0.267146, acc: 0.875000]  [G loss: 0.699604, acc: 0.625000]\n",
      "3737: [D loss: 0.260179, acc: 0.890625]  [G loss: 1.126680, acc: 0.500000]\n",
      "3738: [D loss: 0.236043, acc: 0.882812]  [G loss: 1.193765, acc: 0.375000]\n",
      "3739: [D loss: 0.220582, acc: 0.921875]  [G loss: 0.899871, acc: 0.515625]\n",
      "3740: [D loss: 0.168959, acc: 0.945312]  [G loss: 0.617564, acc: 0.656250]\n",
      "3741: [D loss: 0.238160, acc: 0.921875]  [G loss: 0.952000, acc: 0.515625]\n",
      "3742: [D loss: 0.287438, acc: 0.875000]  [G loss: 0.549770, acc: 0.718750]\n",
      "3743: [D loss: 0.195754, acc: 0.914062]  [G loss: 1.221189, acc: 0.484375]\n",
      "3744: [D loss: 0.185720, acc: 0.921875]  [G loss: 0.640819, acc: 0.671875]\n",
      "3745: [D loss: 0.192464, acc: 0.906250]  [G loss: 1.801861, acc: 0.328125]\n",
      "3746: [D loss: 0.536920, acc: 0.765625]  [G loss: 0.140663, acc: 0.968750]\n",
      "3747: [D loss: 0.643304, acc: 0.812500]  [G loss: 1.781740, acc: 0.265625]\n",
      "3748: [D loss: 0.375575, acc: 0.851562]  [G loss: 0.772589, acc: 0.562500]\n",
      "3749: [D loss: 0.268309, acc: 0.898438]  [G loss: 1.108755, acc: 0.453125]\n",
      "3750: [D loss: 0.277421, acc: 0.890625]  [G loss: 0.548684, acc: 0.703125]\n",
      "3751: [D loss: 0.121770, acc: 0.953125]  [G loss: 0.270923, acc: 0.890625]\n",
      "3752: [D loss: 0.428537, acc: 0.789062]  [G loss: 1.194713, acc: 0.390625]\n",
      "3753: [D loss: 0.269550, acc: 0.867188]  [G loss: 0.694770, acc: 0.562500]\n",
      "3754: [D loss: 0.191176, acc: 0.929688]  [G loss: 0.736829, acc: 0.546875]\n",
      "3755: [D loss: 0.201856, acc: 0.929688]  [G loss: 0.437022, acc: 0.703125]\n",
      "3756: [D loss: 0.199691, acc: 0.921875]  [G loss: 0.362337, acc: 0.796875]\n",
      "3757: [D loss: 0.203087, acc: 0.906250]  [G loss: 0.261623, acc: 0.953125]\n",
      "3758: [D loss: 0.115892, acc: 0.976562]  [G loss: 0.594319, acc: 0.671875]\n",
      "3759: [D loss: 0.223889, acc: 0.921875]  [G loss: 0.922912, acc: 0.437500]\n",
      "3760: [D loss: 0.231268, acc: 0.906250]  [G loss: 0.324256, acc: 0.890625]\n",
      "3761: [D loss: 0.283564, acc: 0.882812]  [G loss: 0.724740, acc: 0.578125]\n",
      "3762: [D loss: 0.224130, acc: 0.906250]  [G loss: 0.390600, acc: 0.765625]\n",
      "3763: [D loss: 0.325422, acc: 0.882812]  [G loss: 0.815432, acc: 0.546875]\n",
      "3764: [D loss: 0.192796, acc: 0.906250]  [G loss: 0.779102, acc: 0.484375]\n",
      "3765: [D loss: 0.227392, acc: 0.898438]  [G loss: 0.675709, acc: 0.625000]\n",
      "3766: [D loss: 0.147358, acc: 0.968750]  [G loss: 0.220303, acc: 0.921875]\n",
      "3767: [D loss: 0.239183, acc: 0.898438]  [G loss: 1.291272, acc: 0.437500]\n",
      "3768: [D loss: 0.140971, acc: 0.937500]  [G loss: 0.244300, acc: 0.890625]\n",
      "3769: [D loss: 0.144535, acc: 0.953125]  [G loss: 0.394362, acc: 0.796875]\n",
      "3770: [D loss: 0.143583, acc: 0.914062]  [G loss: 0.130259, acc: 0.968750]\n",
      "3771: [D loss: 0.176653, acc: 0.914062]  [G loss: 1.254600, acc: 0.406250]\n",
      "3772: [D loss: 0.082583, acc: 0.976562]  [G loss: 0.409140, acc: 0.765625]\n",
      "3773: [D loss: 0.290828, acc: 0.890625]  [G loss: 2.031203, acc: 0.171875]\n",
      "3774: [D loss: 0.195219, acc: 0.929688]  [G loss: 0.281145, acc: 0.906250]\n",
      "3775: [D loss: 0.136553, acc: 0.945312]  [G loss: 0.196595, acc: 0.921875]\n",
      "3776: [D loss: 0.102251, acc: 0.960938]  [G loss: 0.226967, acc: 0.921875]\n",
      "3777: [D loss: 0.139622, acc: 0.960938]  [G loss: 0.504575, acc: 0.718750]\n",
      "3778: [D loss: 0.133505, acc: 0.960938]  [G loss: 0.144917, acc: 0.953125]\n",
      "3779: [D loss: 0.259452, acc: 0.906250]  [G loss: 1.395379, acc: 0.359375]\n",
      "3780: [D loss: 0.138562, acc: 0.945312]  [G loss: 0.274680, acc: 0.875000]\n",
      "3781: [D loss: 0.282251, acc: 0.890625]  [G loss: 1.483308, acc: 0.343750]\n",
      "3782: [D loss: 0.272827, acc: 0.906250]  [G loss: 0.376618, acc: 0.765625]\n",
      "3783: [D loss: 0.128508, acc: 0.968750]  [G loss: 1.343068, acc: 0.375000]\n",
      "3784: [D loss: 0.134113, acc: 0.960938]  [G loss: 0.668117, acc: 0.656250]\n",
      "3785: [D loss: 0.163949, acc: 0.929688]  [G loss: 0.425465, acc: 0.828125]\n",
      "3786: [D loss: 0.095986, acc: 0.976562]  [G loss: 0.461728, acc: 0.796875]\n",
      "3787: [D loss: 0.156199, acc: 0.929688]  [G loss: 0.717682, acc: 0.593750]\n",
      "3788: [D loss: 0.224125, acc: 0.906250]  [G loss: 0.402951, acc: 0.781250]\n",
      "3789: [D loss: 0.296116, acc: 0.867188]  [G loss: 5.500139, acc: 0.093750]\n",
      "3790: [D loss: 0.640878, acc: 0.835938]  [G loss: 0.409374, acc: 0.796875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3791: [D loss: 0.501876, acc: 0.859375]  [G loss: 1.505426, acc: 0.406250]\n",
      "3792: [D loss: 0.223660, acc: 0.914062]  [G loss: 0.453562, acc: 0.812500]\n",
      "3793: [D loss: 0.137099, acc: 0.953125]  [G loss: 0.272592, acc: 0.890625]\n",
      "3794: [D loss: 0.151390, acc: 0.945312]  [G loss: 0.339932, acc: 0.812500]\n",
      "3795: [D loss: 0.374462, acc: 0.835938]  [G loss: 1.085903, acc: 0.484375]\n",
      "3796: [D loss: 0.372554, acc: 0.851562]  [G loss: 0.122482, acc: 0.953125]\n",
      "3797: [D loss: 0.320193, acc: 0.875000]  [G loss: 1.072317, acc: 0.484375]\n",
      "3798: [D loss: 0.249284, acc: 0.898438]  [G loss: 0.245375, acc: 0.828125]\n",
      "3799: [D loss: 0.266033, acc: 0.914062]  [G loss: 0.507927, acc: 0.718750]\n",
      "3800: [D loss: 0.160796, acc: 0.937500]  [G loss: 0.428582, acc: 0.765625]\n",
      "3801: [D loss: 0.158314, acc: 0.929688]  [G loss: 0.439417, acc: 0.765625]\n",
      "3802: [D loss: 0.114032, acc: 0.945312]  [G loss: 0.535091, acc: 0.750000]\n",
      "3803: [D loss: 0.198428, acc: 0.898438]  [G loss: 0.643302, acc: 0.687500]\n",
      "3804: [D loss: 0.239117, acc: 0.890625]  [G loss: 0.229640, acc: 0.875000]\n",
      "3805: [D loss: 0.247372, acc: 0.890625]  [G loss: 1.814771, acc: 0.359375]\n",
      "3806: [D loss: 0.200929, acc: 0.929688]  [G loss: 0.421691, acc: 0.781250]\n",
      "3807: [D loss: 0.228420, acc: 0.875000]  [G loss: 2.151896, acc: 0.453125]\n",
      "3808: [D loss: 0.147680, acc: 0.937500]  [G loss: 1.084884, acc: 0.593750]\n",
      "3809: [D loss: 0.177416, acc: 0.906250]  [G loss: 0.435398, acc: 0.812500]\n",
      "3810: [D loss: 0.072022, acc: 0.992188]  [G loss: 0.664920, acc: 0.671875]\n",
      "3811: [D loss: 0.174040, acc: 0.945312]  [G loss: 1.654573, acc: 0.421875]\n",
      "3812: [D loss: 0.257751, acc: 0.882812]  [G loss: 0.194842, acc: 0.937500]\n",
      "3813: [D loss: 0.280421, acc: 0.859375]  [G loss: 2.916641, acc: 0.218750]\n",
      "3814: [D loss: 0.308610, acc: 0.835938]  [G loss: 0.679208, acc: 0.750000]\n",
      "3815: [D loss: 0.196585, acc: 0.914062]  [G loss: 1.326823, acc: 0.484375]\n",
      "3816: [D loss: 0.127737, acc: 0.960938]  [G loss: 0.723820, acc: 0.609375]\n",
      "3817: [D loss: 0.159125, acc: 0.945312]  [G loss: 0.347098, acc: 0.828125]\n",
      "3818: [D loss: 0.092999, acc: 0.968750]  [G loss: 0.434440, acc: 0.734375]\n",
      "3819: [D loss: 0.187236, acc: 0.937500]  [G loss: 0.698216, acc: 0.671875]\n",
      "3820: [D loss: 0.166970, acc: 0.937500]  [G loss: 0.259987, acc: 0.859375]\n",
      "3821: [D loss: 0.259652, acc: 0.937500]  [G loss: 1.367957, acc: 0.453125]\n",
      "3822: [D loss: 0.244917, acc: 0.882812]  [G loss: 0.633481, acc: 0.703125]\n",
      "3823: [D loss: 0.247097, acc: 0.890625]  [G loss: 0.496678, acc: 0.765625]\n",
      "3824: [D loss: 0.183982, acc: 0.945312]  [G loss: 0.769413, acc: 0.609375]\n",
      "3825: [D loss: 0.207722, acc: 0.914062]  [G loss: 0.286647, acc: 0.859375]\n",
      "3826: [D loss: 0.197330, acc: 0.921875]  [G loss: 0.654402, acc: 0.687500]\n",
      "3827: [D loss: 0.196697, acc: 0.914062]  [G loss: 0.222138, acc: 0.890625]\n",
      "3828: [D loss: 0.070076, acc: 0.976562]  [G loss: 0.206916, acc: 0.906250]\n",
      "3829: [D loss: 0.150336, acc: 0.945312]  [G loss: 0.391485, acc: 0.812500]\n",
      "3830: [D loss: 0.139988, acc: 0.937500]  [G loss: 0.411691, acc: 0.781250]\n",
      "3831: [D loss: 0.171326, acc: 0.929688]  [G loss: 0.964339, acc: 0.640625]\n",
      "3832: [D loss: 0.167818, acc: 0.898438]  [G loss: 0.113857, acc: 0.953125]\n",
      "3833: [D loss: 0.370250, acc: 0.867188]  [G loss: 1.254496, acc: 0.578125]\n",
      "3834: [D loss: 0.397527, acc: 0.875000]  [G loss: 0.154546, acc: 0.953125]\n",
      "3835: [D loss: 0.207585, acc: 0.890625]  [G loss: 0.837652, acc: 0.593750]\n",
      "3836: [D loss: 0.178968, acc: 0.937500]  [G loss: 0.320307, acc: 0.843750]\n",
      "3837: [D loss: 0.256465, acc: 0.882812]  [G loss: 1.082851, acc: 0.453125]\n",
      "3838: [D loss: 0.140148, acc: 0.953125]  [G loss: 0.496201, acc: 0.687500]\n",
      "3839: [D loss: 0.107215, acc: 0.953125]  [G loss: 0.545223, acc: 0.703125]\n",
      "3840: [D loss: 0.180652, acc: 0.914062]  [G loss: 1.140339, acc: 0.437500]\n",
      "3841: [D loss: 0.198764, acc: 0.914062]  [G loss: 0.879145, acc: 0.687500]\n",
      "3842: [D loss: 0.258570, acc: 0.867188]  [G loss: 0.074907, acc: 0.984375]\n",
      "3843: [D loss: 0.330834, acc: 0.882812]  [G loss: 1.667446, acc: 0.343750]\n",
      "3844: [D loss: 0.092473, acc: 0.968750]  [G loss: 0.338299, acc: 0.796875]\n",
      "3845: [D loss: 0.202660, acc: 0.937500]  [G loss: 0.976150, acc: 0.578125]\n",
      "3846: [D loss: 0.231959, acc: 0.898438]  [G loss: 0.263687, acc: 0.843750]\n",
      "3847: [D loss: 0.177585, acc: 0.937500]  [G loss: 0.819123, acc: 0.656250]\n",
      "3848: [D loss: 0.123252, acc: 0.929688]  [G loss: 0.430422, acc: 0.828125]\n",
      "3849: [D loss: 0.148630, acc: 0.937500]  [G loss: 3.337576, acc: 0.046875]\n",
      "3850: [D loss: 0.296390, acc: 0.914062]  [G loss: 0.475788, acc: 0.750000]\n",
      "3851: [D loss: 0.230114, acc: 0.898438]  [G loss: 2.126242, acc: 0.234375]\n",
      "3852: [D loss: 0.306624, acc: 0.890625]  [G loss: 0.465780, acc: 0.812500]\n",
      "3853: [D loss: 0.183714, acc: 0.914062]  [G loss: 0.340927, acc: 0.890625]\n",
      "3854: [D loss: 0.152535, acc: 0.929688]  [G loss: 0.434803, acc: 0.750000]\n",
      "3855: [D loss: 0.147898, acc: 0.945312]  [G loss: 0.357224, acc: 0.828125]\n",
      "3856: [D loss: 0.100439, acc: 0.968750]  [G loss: 0.153600, acc: 0.937500]\n",
      "3857: [D loss: 0.154522, acc: 0.937500]  [G loss: 1.347302, acc: 0.593750]\n",
      "3858: [D loss: 0.169631, acc: 0.937500]  [G loss: 0.821425, acc: 0.718750]\n",
      "3859: [D loss: 0.124024, acc: 0.960938]  [G loss: 0.516505, acc: 0.781250]\n",
      "3860: [D loss: 0.142792, acc: 0.945312]  [G loss: 0.083053, acc: 0.968750]\n",
      "3861: [D loss: 0.185013, acc: 0.906250]  [G loss: 0.903200, acc: 0.515625]\n",
      "3862: [D loss: 0.114152, acc: 0.960938]  [G loss: 0.773104, acc: 0.609375]\n",
      "3863: [D loss: 0.146352, acc: 0.921875]  [G loss: 0.045477, acc: 0.984375]\n",
      "3864: [D loss: 0.063647, acc: 0.976562]  [G loss: 0.363068, acc: 0.828125]\n",
      "3865: [D loss: 0.065552, acc: 0.984375]  [G loss: 0.212348, acc: 0.921875]\n",
      "3866: [D loss: 0.134627, acc: 0.929688]  [G loss: 0.367459, acc: 0.906250]\n",
      "3867: [D loss: 0.217438, acc: 0.921875]  [G loss: 0.059490, acc: 0.968750]\n",
      "3868: [D loss: 0.345104, acc: 0.882812]  [G loss: 1.770637, acc: 0.187500]\n",
      "3869: [D loss: 0.159541, acc: 0.929688]  [G loss: 0.130717, acc: 0.953125]\n",
      "3870: [D loss: 0.162319, acc: 0.937500]  [G loss: 0.470771, acc: 0.812500]\n",
      "3871: [D loss: 0.116623, acc: 0.945312]  [G loss: 0.895319, acc: 0.625000]\n",
      "3872: [D loss: 0.244812, acc: 0.914062]  [G loss: 0.306252, acc: 0.875000]\n",
      "3873: [D loss: 0.366026, acc: 0.851562]  [G loss: 2.591920, acc: 0.156250]\n",
      "3874: [D loss: 0.239072, acc: 0.882812]  [G loss: 0.382392, acc: 0.859375]\n",
      "3875: [D loss: 0.284361, acc: 0.921875]  [G loss: 2.848780, acc: 0.093750]\n",
      "3876: [D loss: 0.204684, acc: 0.929688]  [G loss: 0.779658, acc: 0.625000]\n",
      "3877: [D loss: 0.177544, acc: 0.906250]  [G loss: 0.643771, acc: 0.703125]\n",
      "3878: [D loss: 0.139220, acc: 0.945312]  [G loss: 0.516385, acc: 0.812500]\n",
      "3879: [D loss: 0.258655, acc: 0.882812]  [G loss: 1.578014, acc: 0.484375]\n",
      "3880: [D loss: 0.232376, acc: 0.914062]  [G loss: 0.341618, acc: 0.812500]\n",
      "3881: [D loss: 0.233443, acc: 0.898438]  [G loss: 2.337033, acc: 0.171875]\n",
      "3882: [D loss: 0.302430, acc: 0.890625]  [G loss: 0.566863, acc: 0.703125]\n",
      "3883: [D loss: 0.346501, acc: 0.906250]  [G loss: 0.996987, acc: 0.578125]\n",
      "3884: [D loss: 0.166259, acc: 0.945312]  [G loss: 0.716941, acc: 0.640625]\n",
      "3885: [D loss: 0.158453, acc: 0.906250]  [G loss: 0.770085, acc: 0.609375]\n",
      "3886: [D loss: 0.128792, acc: 0.945312]  [G loss: 0.359704, acc: 0.859375]\n",
      "3887: [D loss: 0.252351, acc: 0.882812]  [G loss: 2.006004, acc: 0.265625]\n",
      "3888: [D loss: 0.364035, acc: 0.859375]  [G loss: 0.371328, acc: 0.812500]\n",
      "3889: [D loss: 0.215295, acc: 0.898438]  [G loss: 2.075469, acc: 0.250000]\n",
      "3890: [D loss: 0.195068, acc: 0.929688]  [G loss: 0.609498, acc: 0.671875]\n",
      "3891: [D loss: 0.121137, acc: 0.953125]  [G loss: 0.876831, acc: 0.640625]\n",
      "3892: [D loss: 0.089300, acc: 0.976562]  [G loss: 0.619962, acc: 0.687500]\n",
      "3893: [D loss: 0.201112, acc: 0.882812]  [G loss: 1.142324, acc: 0.484375]\n",
      "3894: [D loss: 0.154775, acc: 0.937500]  [G loss: 0.460921, acc: 0.796875]\n",
      "3895: [D loss: 0.331464, acc: 0.875000]  [G loss: 1.037714, acc: 0.562500]\n",
      "3896: [D loss: 0.226009, acc: 0.906250]  [G loss: 0.457844, acc: 0.796875]\n",
      "3897: [D loss: 0.234522, acc: 0.890625]  [G loss: 1.089376, acc: 0.453125]\n",
      "3898: [D loss: 0.125737, acc: 0.945312]  [G loss: 0.494508, acc: 0.718750]\n",
      "3899: [D loss: 0.228981, acc: 0.914062]  [G loss: 1.162831, acc: 0.500000]\n",
      "3900: [D loss: 0.165504, acc: 0.929688]  [G loss: 1.021663, acc: 0.453125]\n",
      "3901: [D loss: 0.190604, acc: 0.914062]  [G loss: 0.626008, acc: 0.640625]\n",
      "3902: [D loss: 0.353713, acc: 0.882812]  [G loss: 1.589966, acc: 0.453125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3903: [D loss: 0.404088, acc: 0.851562]  [G loss: 0.227987, acc: 0.890625]\n",
      "3904: [D loss: 0.266169, acc: 0.875000]  [G loss: 1.658517, acc: 0.312500]\n",
      "3905: [D loss: 0.223432, acc: 0.906250]  [G loss: 0.355068, acc: 0.781250]\n",
      "3906: [D loss: 0.129439, acc: 0.953125]  [G loss: 0.581304, acc: 0.703125]\n",
      "3907: [D loss: 0.145351, acc: 0.937500]  [G loss: 0.353620, acc: 0.843750]\n",
      "3908: [D loss: 0.179214, acc: 0.929688]  [G loss: 0.889194, acc: 0.671875]\n",
      "3909: [D loss: 0.151301, acc: 0.929688]  [G loss: 1.059886, acc: 0.562500]\n",
      "3910: [D loss: 0.285474, acc: 0.851562]  [G loss: 0.948242, acc: 0.593750]\n",
      "3911: [D loss: 0.129619, acc: 0.953125]  [G loss: 0.571412, acc: 0.703125]\n",
      "3912: [D loss: 0.169512, acc: 0.921875]  [G loss: 0.538338, acc: 0.750000]\n",
      "3913: [D loss: 0.142781, acc: 0.945312]  [G loss: 0.643816, acc: 0.687500]\n",
      "3914: [D loss: 0.244429, acc: 0.882812]  [G loss: 0.538459, acc: 0.734375]\n",
      "3915: [D loss: 0.189404, acc: 0.921875]  [G loss: 1.515925, acc: 0.375000]\n",
      "3916: [D loss: 0.186829, acc: 0.921875]  [G loss: 0.454395, acc: 0.781250]\n",
      "3917: [D loss: 0.276273, acc: 0.906250]  [G loss: 0.784659, acc: 0.625000]\n",
      "3918: [D loss: 0.211223, acc: 0.921875]  [G loss: 1.412158, acc: 0.343750]\n",
      "3919: [D loss: 0.147147, acc: 0.953125]  [G loss: 0.376907, acc: 0.765625]\n",
      "3920: [D loss: 0.301986, acc: 0.890625]  [G loss: 1.316648, acc: 0.343750]\n",
      "3921: [D loss: 0.315917, acc: 0.859375]  [G loss: 0.290559, acc: 0.875000]\n",
      "3922: [D loss: 0.272576, acc: 0.851562]  [G loss: 2.177523, acc: 0.171875]\n",
      "3923: [D loss: 0.237945, acc: 0.890625]  [G loss: 0.474451, acc: 0.687500]\n",
      "3924: [D loss: 0.174594, acc: 0.929688]  [G loss: 0.919977, acc: 0.593750]\n",
      "3925: [D loss: 0.146338, acc: 0.929688]  [G loss: 0.682309, acc: 0.718750]\n",
      "3926: [D loss: 0.210532, acc: 0.921875]  [G loss: 1.562647, acc: 0.406250]\n",
      "3927: [D loss: 0.222218, acc: 0.867188]  [G loss: 0.603838, acc: 0.671875]\n",
      "3928: [D loss: 0.179464, acc: 0.914062]  [G loss: 0.914331, acc: 0.562500]\n",
      "3929: [D loss: 0.180416, acc: 0.945312]  [G loss: 0.694538, acc: 0.625000]\n",
      "3930: [D loss: 0.247437, acc: 0.898438]  [G loss: 1.177047, acc: 0.562500]\n",
      "3931: [D loss: 0.228428, acc: 0.898438]  [G loss: 0.250978, acc: 0.875000]\n",
      "3932: [D loss: 0.194398, acc: 0.898438]  [G loss: 1.009642, acc: 0.531250]\n",
      "3933: [D loss: 0.210962, acc: 0.914062]  [G loss: 0.206849, acc: 0.921875]\n",
      "3934: [D loss: 0.233229, acc: 0.898438]  [G loss: 0.948636, acc: 0.562500]\n",
      "3935: [D loss: 0.208165, acc: 0.914062]  [G loss: 0.264970, acc: 0.843750]\n",
      "3936: [D loss: 0.255763, acc: 0.882812]  [G loss: 1.576092, acc: 0.406250]\n",
      "3937: [D loss: 0.236713, acc: 0.914062]  [G loss: 0.268347, acc: 0.828125]\n",
      "3938: [D loss: 0.158998, acc: 0.953125]  [G loss: 0.738487, acc: 0.656250]\n",
      "3939: [D loss: 0.118887, acc: 0.953125]  [G loss: 0.592101, acc: 0.750000]\n",
      "3940: [D loss: 0.140823, acc: 0.929688]  [G loss: 1.121191, acc: 0.578125]\n",
      "3941: [D loss: 0.308703, acc: 0.890625]  [G loss: 0.145751, acc: 0.937500]\n",
      "3942: [D loss: 0.416032, acc: 0.828125]  [G loss: 1.895827, acc: 0.343750]\n",
      "3943: [D loss: 0.157450, acc: 0.914062]  [G loss: 0.607983, acc: 0.781250]\n",
      "3944: [D loss: 0.167427, acc: 0.953125]  [G loss: 0.902642, acc: 0.609375]\n",
      "3945: [D loss: 0.109649, acc: 0.960938]  [G loss: 0.773364, acc: 0.625000]\n",
      "3946: [D loss: 0.072513, acc: 0.984375]  [G loss: 0.671505, acc: 0.640625]\n",
      "3947: [D loss: 0.114250, acc: 0.968750]  [G loss: 0.373562, acc: 0.781250]\n",
      "3948: [D loss: 0.132638, acc: 0.929688]  [G loss: 0.595826, acc: 0.703125]\n",
      "3949: [D loss: 0.185373, acc: 0.921875]  [G loss: 0.172735, acc: 0.906250]\n",
      "3950: [D loss: 0.247059, acc: 0.867188]  [G loss: 3.903289, acc: 0.093750]\n",
      "3951: [D loss: 0.405410, acc: 0.851562]  [G loss: 0.718598, acc: 0.640625]\n",
      "3952: [D loss: 0.140701, acc: 0.960938]  [G loss: 0.440948, acc: 0.750000]\n",
      "3953: [D loss: 0.108464, acc: 0.960938]  [G loss: 0.162776, acc: 0.953125]\n",
      "3954: [D loss: 0.176646, acc: 0.937500]  [G loss: 0.734600, acc: 0.671875]\n",
      "3955: [D loss: 0.191729, acc: 0.921875]  [G loss: 0.696817, acc: 0.625000]\n",
      "3956: [D loss: 0.117240, acc: 0.953125]  [G loss: 0.817583, acc: 0.562500]\n",
      "3957: [D loss: 0.091149, acc: 0.968750]  [G loss: 0.534201, acc: 0.718750]\n",
      "3958: [D loss: 0.187989, acc: 0.937500]  [G loss: 0.739345, acc: 0.640625]\n",
      "3959: [D loss: 0.155137, acc: 0.929688]  [G loss: 0.765742, acc: 0.656250]\n",
      "3960: [D loss: 0.148133, acc: 0.953125]  [G loss: 0.792252, acc: 0.671875]\n",
      "3961: [D loss: 0.138829, acc: 0.945312]  [G loss: 2.613466, acc: 0.171875]\n",
      "3962: [D loss: 0.100394, acc: 0.960938]  [G loss: 1.506067, acc: 0.453125]\n",
      "3963: [D loss: 0.186048, acc: 0.914062]  [G loss: 0.541733, acc: 0.687500]\n",
      "3964: [D loss: 0.243575, acc: 0.890625]  [G loss: 4.457446, acc: 0.015625]\n",
      "3965: [D loss: 0.392478, acc: 0.851562]  [G loss: 0.954117, acc: 0.625000]\n",
      "3966: [D loss: 0.338096, acc: 0.859375]  [G loss: 2.723162, acc: 0.203125]\n",
      "3967: [D loss: 0.305687, acc: 0.875000]  [G loss: 1.076964, acc: 0.578125]\n",
      "3968: [D loss: 0.169432, acc: 0.906250]  [G loss: 1.744470, acc: 0.296875]\n",
      "3969: [D loss: 0.283931, acc: 0.898438]  [G loss: 0.991094, acc: 0.609375]\n",
      "3970: [D loss: 0.161887, acc: 0.937500]  [G loss: 1.351645, acc: 0.437500]\n",
      "3971: [D loss: 0.216027, acc: 0.898438]  [G loss: 0.692545, acc: 0.671875]\n",
      "3972: [D loss: 0.230306, acc: 0.890625]  [G loss: 0.196806, acc: 0.906250]\n",
      "3973: [D loss: 0.319549, acc: 0.859375]  [G loss: 1.962466, acc: 0.296875]\n",
      "3974: [D loss: 0.247665, acc: 0.882812]  [G loss: 0.457424, acc: 0.765625]\n",
      "3975: [D loss: 0.195104, acc: 0.921875]  [G loss: 0.876517, acc: 0.640625]\n",
      "3976: [D loss: 0.145811, acc: 0.921875]  [G loss: 0.452505, acc: 0.796875]\n",
      "3977: [D loss: 0.142648, acc: 0.945312]  [G loss: 0.433799, acc: 0.781250]\n",
      "3978: [D loss: 0.152922, acc: 0.921875]  [G loss: 0.220383, acc: 0.906250]\n",
      "3979: [D loss: 0.156658, acc: 0.937500]  [G loss: 0.676274, acc: 0.671875]\n",
      "3980: [D loss: 0.229488, acc: 0.859375]  [G loss: 0.870164, acc: 0.671875]\n",
      "3981: [D loss: 0.183388, acc: 0.929688]  [G loss: 0.350386, acc: 0.796875]\n",
      "3982: [D loss: 0.185528, acc: 0.921875]  [G loss: 1.139427, acc: 0.562500]\n",
      "3983: [D loss: 0.226213, acc: 0.929688]  [G loss: 0.202519, acc: 0.921875]\n",
      "3984: [D loss: 0.398801, acc: 0.828125]  [G loss: 1.295454, acc: 0.453125]\n",
      "3985: [D loss: 0.268945, acc: 0.898438]  [G loss: 0.302513, acc: 0.843750]\n",
      "3986: [D loss: 0.235934, acc: 0.898438]  [G loss: 0.781091, acc: 0.671875]\n",
      "3987: [D loss: 0.129787, acc: 0.921875]  [G loss: 0.284374, acc: 0.859375]\n",
      "3988: [D loss: 0.157187, acc: 0.945312]  [G loss: 0.652125, acc: 0.703125]\n",
      "3989: [D loss: 0.224386, acc: 0.882812]  [G loss: 0.176746, acc: 0.921875]\n",
      "3990: [D loss: 0.333712, acc: 0.882812]  [G loss: 0.856893, acc: 0.609375]\n",
      "3991: [D loss: 0.351531, acc: 0.859375]  [G loss: 0.221545, acc: 0.921875]\n",
      "3992: [D loss: 0.189981, acc: 0.921875]  [G loss: 0.742574, acc: 0.640625]\n",
      "3993: [D loss: 0.195064, acc: 0.929688]  [G loss: 0.451737, acc: 0.734375]\n",
      "3994: [D loss: 0.209516, acc: 0.898438]  [G loss: 0.249281, acc: 0.828125]\n",
      "3995: [D loss: 0.203696, acc: 0.929688]  [G loss: 0.450646, acc: 0.765625]\n",
      "3996: [D loss: 0.158126, acc: 0.929688]  [G loss: 0.258359, acc: 0.875000]\n",
      "3997: [D loss: 0.188587, acc: 0.929688]  [G loss: 0.535602, acc: 0.703125]\n",
      "3998: [D loss: 0.144063, acc: 0.953125]  [G loss: 0.167128, acc: 0.921875]\n",
      "3999: [D loss: 0.426536, acc: 0.843750]  [G loss: 1.273167, acc: 0.593750]\n",
      "4000: [D loss: 0.351915, acc: 0.867188]  [G loss: 0.210074, acc: 0.890625]\n",
      "4001: [D loss: 0.278998, acc: 0.835938]  [G loss: 0.855145, acc: 0.640625]\n",
      "4002: [D loss: 0.222890, acc: 0.906250]  [G loss: 0.530912, acc: 0.765625]\n",
      "4003: [D loss: 0.209506, acc: 0.929688]  [G loss: 0.498060, acc: 0.671875]\n",
      "4004: [D loss: 0.113921, acc: 0.968750]  [G loss: 0.298231, acc: 0.843750]\n",
      "4005: [D loss: 0.197843, acc: 0.890625]  [G loss: 0.455353, acc: 0.796875]\n",
      "4006: [D loss: 0.263712, acc: 0.914062]  [G loss: 0.397307, acc: 0.859375]\n",
      "4007: [D loss: 0.156464, acc: 0.929688]  [G loss: 0.476076, acc: 0.765625]\n",
      "4008: [D loss: 0.150275, acc: 0.937500]  [G loss: 0.486494, acc: 0.781250]\n",
      "4009: [D loss: 0.145016, acc: 0.937500]  [G loss: 0.311302, acc: 0.843750]\n",
      "4010: [D loss: 0.182308, acc: 0.937500]  [G loss: 0.386616, acc: 0.812500]\n",
      "4011: [D loss: 0.166448, acc: 0.906250]  [G loss: 1.284327, acc: 0.500000]\n",
      "4012: [D loss: 0.155374, acc: 0.921875]  [G loss: 0.266819, acc: 0.843750]\n",
      "4013: [D loss: 0.076042, acc: 0.976562]  [G loss: 0.371561, acc: 0.843750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4014: [D loss: 0.148046, acc: 0.914062]  [G loss: 1.029467, acc: 0.593750]\n",
      "4015: [D loss: 0.213485, acc: 0.937500]  [G loss: 0.441163, acc: 0.781250]\n",
      "4016: [D loss: 0.169449, acc: 0.914062]  [G loss: 0.449962, acc: 0.750000]\n",
      "4017: [D loss: 0.123093, acc: 0.937500]  [G loss: 0.281324, acc: 0.843750]\n",
      "4018: [D loss: 0.158495, acc: 0.937500]  [G loss: 1.220670, acc: 0.375000]\n",
      "4019: [D loss: 0.177581, acc: 0.937500]  [G loss: 0.105803, acc: 0.953125]\n",
      "4020: [D loss: 0.063968, acc: 0.984375]  [G loss: 0.063007, acc: 1.000000]\n",
      "4021: [D loss: 0.052965, acc: 0.984375]  [G loss: 0.080219, acc: 0.953125]\n",
      "4022: [D loss: 0.085380, acc: 0.968750]  [G loss: 0.295340, acc: 0.843750]\n",
      "4023: [D loss: 0.135076, acc: 0.953125]  [G loss: 0.088778, acc: 0.968750]\n",
      "4024: [D loss: 0.187725, acc: 0.914062]  [G loss: 0.352993, acc: 0.812500]\n",
      "4025: [D loss: 0.068070, acc: 0.968750]  [G loss: 0.051769, acc: 0.984375]\n",
      "4026: [D loss: 0.191255, acc: 0.921875]  [G loss: 2.922492, acc: 0.234375]\n",
      "4027: [D loss: 0.180613, acc: 0.921875]  [G loss: 0.355034, acc: 0.796875]\n",
      "4028: [D loss: 0.090969, acc: 0.968750]  [G loss: 0.325665, acc: 0.796875]\n",
      "4029: [D loss: 0.104801, acc: 0.953125]  [G loss: 0.185320, acc: 0.921875]\n",
      "4030: [D loss: 0.073844, acc: 0.976562]  [G loss: 0.096888, acc: 0.968750]\n",
      "4031: [D loss: 0.137247, acc: 0.937500]  [G loss: 0.317647, acc: 0.843750]\n",
      "4032: [D loss: 0.132871, acc: 0.945312]  [G loss: 0.506911, acc: 0.734375]\n",
      "4033: [D loss: 0.155603, acc: 0.945312]  [G loss: 0.221491, acc: 0.906250]\n",
      "4034: [D loss: 0.189588, acc: 0.914062]  [G loss: 0.784464, acc: 0.781250]\n",
      "4035: [D loss: 0.225429, acc: 0.898438]  [G loss: 2.953723, acc: 0.187500]\n",
      "4036: [D loss: 0.098232, acc: 0.953125]  [G loss: 0.914405, acc: 0.500000]\n",
      "4037: [D loss: 0.091113, acc: 0.976562]  [G loss: 0.136716, acc: 0.953125]\n",
      "4038: [D loss: 0.176303, acc: 0.898438]  [G loss: 1.494295, acc: 0.437500]\n",
      "4039: [D loss: 0.194063, acc: 0.945312]  [G loss: 0.285940, acc: 0.843750]\n",
      "4040: [D loss: 0.134273, acc: 0.937500]  [G loss: 0.311624, acc: 0.812500]\n",
      "4041: [D loss: 0.139617, acc: 0.929688]  [G loss: 2.218658, acc: 0.296875]\n",
      "4042: [D loss: 0.133544, acc: 0.937500]  [G loss: 0.721792, acc: 0.703125]\n",
      "4043: [D loss: 0.433315, acc: 0.851562]  [G loss: 2.493870, acc: 0.187500]\n",
      "4044: [D loss: 0.188929, acc: 0.929688]  [G loss: 0.467335, acc: 0.750000]\n",
      "4045: [D loss: 0.161067, acc: 0.929688]  [G loss: 1.183503, acc: 0.453125]\n",
      "4046: [D loss: 0.067934, acc: 0.976562]  [G loss: 0.526728, acc: 0.750000]\n",
      "4047: [D loss: 0.091191, acc: 0.984375]  [G loss: 0.552823, acc: 0.765625]\n",
      "4048: [D loss: 0.156164, acc: 0.937500]  [G loss: 0.912983, acc: 0.578125]\n",
      "4049: [D loss: 0.059702, acc: 0.984375]  [G loss: 0.411347, acc: 0.828125]\n",
      "4050: [D loss: 0.095095, acc: 0.976562]  [G loss: 0.157277, acc: 0.968750]\n",
      "4051: [D loss: 0.097022, acc: 0.953125]  [G loss: 0.369033, acc: 0.843750]\n",
      "4052: [D loss: 0.193375, acc: 0.937500]  [G loss: 0.100320, acc: 0.953125]\n",
      "4053: [D loss: 0.156135, acc: 0.929688]  [G loss: 0.566447, acc: 0.796875]\n",
      "4054: [D loss: 0.125920, acc: 0.945312]  [G loss: 0.160034, acc: 0.921875]\n",
      "4055: [D loss: 0.074079, acc: 0.968750]  [G loss: 0.110614, acc: 0.968750]\n",
      "4056: [D loss: 0.056004, acc: 0.984375]  [G loss: 0.234758, acc: 0.906250]\n",
      "4057: [D loss: 0.021848, acc: 0.992188]  [G loss: 0.220064, acc: 0.875000]\n",
      "4058: [D loss: 0.035154, acc: 0.992188]  [G loss: 0.468192, acc: 0.812500]\n",
      "4059: [D loss: 0.058131, acc: 0.976562]  [G loss: 0.100717, acc: 0.937500]\n",
      "4060: [D loss: 0.042085, acc: 0.984375]  [G loss: 0.713174, acc: 0.703125]\n",
      "4061: [D loss: 0.045808, acc: 0.984375]  [G loss: 1.549080, acc: 0.562500]\n",
      "4062: [D loss: 0.130331, acc: 0.953125]  [G loss: 0.124453, acc: 0.937500]\n",
      "4063: [D loss: 0.242119, acc: 0.898438]  [G loss: 1.402139, acc: 0.531250]\n",
      "4064: [D loss: 0.143737, acc: 0.929688]  [G loss: 0.306830, acc: 0.812500]\n",
      "4065: [D loss: 0.083705, acc: 0.968750]  [G loss: 0.868931, acc: 0.625000]\n",
      "4066: [D loss: 0.145183, acc: 0.929688]  [G loss: 0.731924, acc: 0.734375]\n",
      "4067: [D loss: 0.067811, acc: 0.968750]  [G loss: 0.178257, acc: 0.906250]\n",
      "4068: [D loss: 0.271684, acc: 0.875000]  [G loss: 3.092259, acc: 0.312500]\n",
      "4069: [D loss: 0.339010, acc: 0.898438]  [G loss: 0.096167, acc: 0.968750]\n",
      "4070: [D loss: 0.687045, acc: 0.773438]  [G loss: 3.381576, acc: 0.125000]\n",
      "4071: [D loss: 0.323365, acc: 0.867188]  [G loss: 0.422661, acc: 0.781250]\n",
      "4072: [D loss: 0.376652, acc: 0.867188]  [G loss: 1.173086, acc: 0.484375]\n",
      "4073: [D loss: 0.208940, acc: 0.921875]  [G loss: 0.450814, acc: 0.781250]\n",
      "4074: [D loss: 0.234456, acc: 0.898438]  [G loss: 1.279274, acc: 0.484375]\n",
      "4075: [D loss: 0.243025, acc: 0.906250]  [G loss: 0.277109, acc: 0.843750]\n",
      "4076: [D loss: 0.102218, acc: 0.960938]  [G loss: 0.258539, acc: 0.859375]\n",
      "4077: [D loss: 0.094763, acc: 0.953125]  [G loss: 0.746962, acc: 0.671875]\n",
      "4078: [D loss: 0.059690, acc: 0.992188]  [G loss: 0.365884, acc: 0.781250]\n",
      "4079: [D loss: 0.102515, acc: 0.953125]  [G loss: 0.575915, acc: 0.703125]\n",
      "4080: [D loss: 0.276820, acc: 0.906250]  [G loss: 0.655215, acc: 0.640625]\n",
      "4081: [D loss: 0.286974, acc: 0.875000]  [G loss: 0.788927, acc: 0.671875]\n",
      "4082: [D loss: 0.240851, acc: 0.898438]  [G loss: 0.398201, acc: 0.828125]\n",
      "4083: [D loss: 0.212182, acc: 0.867188]  [G loss: 1.935023, acc: 0.343750]\n",
      "4084: [D loss: 0.260498, acc: 0.882812]  [G loss: 0.281638, acc: 0.875000]\n",
      "4085: [D loss: 0.165198, acc: 0.929688]  [G loss: 1.500817, acc: 0.468750]\n",
      "4086: [D loss: 0.215135, acc: 0.914062]  [G loss: 0.430214, acc: 0.843750]\n",
      "4087: [D loss: 0.161887, acc: 0.945312]  [G loss: 0.645256, acc: 0.718750]\n",
      "4088: [D loss: 0.131009, acc: 0.937500]  [G loss: 0.298844, acc: 0.828125]\n",
      "4089: [D loss: 0.159920, acc: 0.929688]  [G loss: 0.548058, acc: 0.734375]\n",
      "4090: [D loss: 0.110247, acc: 0.960938]  [G loss: 0.265354, acc: 0.828125]\n",
      "4091: [D loss: 0.193921, acc: 0.921875]  [G loss: 5.004746, acc: 0.046875]\n",
      "4092: [D loss: 0.178297, acc: 0.914062]  [G loss: 0.757846, acc: 0.718750]\n",
      "4093: [D loss: 0.132023, acc: 0.937500]  [G loss: 1.105021, acc: 0.500000]\n",
      "4094: [D loss: 0.179880, acc: 0.921875]  [G loss: 0.604883, acc: 0.765625]\n",
      "4095: [D loss: 0.212591, acc: 0.921875]  [G loss: 2.237306, acc: 0.203125]\n",
      "4096: [D loss: 0.258441, acc: 0.921875]  [G loss: 0.257554, acc: 0.875000]\n",
      "4097: [D loss: 0.495552, acc: 0.820312]  [G loss: 2.416925, acc: 0.171875]\n",
      "4098: [D loss: 0.236133, acc: 0.914062]  [G loss: 0.619160, acc: 0.796875]\n",
      "4099: [D loss: 0.075834, acc: 0.976562]  [G loss: 0.301676, acc: 0.890625]\n",
      "4100: [D loss: 0.416281, acc: 0.820312]  [G loss: 1.101019, acc: 0.640625]\n",
      "4101: [D loss: 0.478307, acc: 0.835938]  [G loss: 0.232938, acc: 0.875000]\n",
      "4102: [D loss: 0.363264, acc: 0.859375]  [G loss: 1.198471, acc: 0.453125]\n",
      "4103: [D loss: 0.183416, acc: 0.937500]  [G loss: 0.788265, acc: 0.640625]\n",
      "4104: [D loss: 0.181752, acc: 0.921875]  [G loss: 0.321922, acc: 0.843750]\n",
      "4105: [D loss: 0.275472, acc: 0.882812]  [G loss: 0.412916, acc: 0.781250]\n",
      "4106: [D loss: 0.205634, acc: 0.906250]  [G loss: 0.365899, acc: 0.765625]\n",
      "4107: [D loss: 0.142254, acc: 0.937500]  [G loss: 0.253962, acc: 0.906250]\n",
      "4108: [D loss: 0.182843, acc: 0.921875]  [G loss: 0.275043, acc: 0.875000]\n",
      "4109: [D loss: 0.220862, acc: 0.898438]  [G loss: 1.667366, acc: 0.343750]\n",
      "4110: [D loss: 0.302198, acc: 0.875000]  [G loss: 0.135910, acc: 0.937500]\n",
      "4111: [D loss: 0.122926, acc: 0.960938]  [G loss: 0.210080, acc: 0.921875]\n",
      "4112: [D loss: 0.160799, acc: 0.953125]  [G loss: 0.238016, acc: 0.906250]\n",
      "4113: [D loss: 0.110424, acc: 0.968750]  [G loss: 0.210364, acc: 0.906250]\n",
      "4114: [D loss: 0.195958, acc: 0.882812]  [G loss: 0.515703, acc: 0.734375]\n",
      "4115: [D loss: 0.154070, acc: 0.953125]  [G loss: 0.348797, acc: 0.812500]\n",
      "4116: [D loss: 0.199417, acc: 0.921875]  [G loss: 1.244277, acc: 0.531250]\n",
      "4117: [D loss: 0.213467, acc: 0.937500]  [G loss: 0.403118, acc: 0.812500]\n",
      "4118: [D loss: 0.328874, acc: 0.851562]  [G loss: 4.342058, acc: 0.015625]\n",
      "4119: [D loss: 0.426564, acc: 0.867188]  [G loss: 0.711370, acc: 0.671875]\n",
      "4120: [D loss: 0.239276, acc: 0.921875]  [G loss: 0.231661, acc: 0.875000]\n",
      "4121: [D loss: 0.161115, acc: 0.945312]  [G loss: 0.459042, acc: 0.812500]\n",
      "4122: [D loss: 0.263313, acc: 0.914062]  [G loss: 0.519132, acc: 0.750000]\n",
      "4123: [D loss: 0.220990, acc: 0.898438]  [G loss: 0.603504, acc: 0.765625]\n",
      "4124: [D loss: 0.200007, acc: 0.914062]  [G loss: 1.433940, acc: 0.359375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4125: [D loss: 0.340043, acc: 0.851562]  [G loss: 0.196892, acc: 0.875000]\n",
      "4126: [D loss: 0.308129, acc: 0.859375]  [G loss: 0.884308, acc: 0.609375]\n",
      "4127: [D loss: 0.194078, acc: 0.906250]  [G loss: 0.217155, acc: 0.875000]\n",
      "4128: [D loss: 0.144548, acc: 0.929688]  [G loss: 0.882780, acc: 0.484375]\n",
      "4129: [D loss: 0.241372, acc: 0.898438]  [G loss: 0.459929, acc: 0.765625]\n",
      "4130: [D loss: 0.122078, acc: 0.945312]  [G loss: 0.233125, acc: 0.843750]\n",
      "4131: [D loss: 0.244210, acc: 0.898438]  [G loss: 2.845811, acc: 0.062500]\n",
      "4132: [D loss: 0.218206, acc: 0.906250]  [G loss: 0.592868, acc: 0.671875]\n",
      "4133: [D loss: 0.176461, acc: 0.914062]  [G loss: 0.296341, acc: 0.796875]\n",
      "4134: [D loss: 0.257647, acc: 0.898438]  [G loss: 1.999052, acc: 0.171875]\n",
      "4135: [D loss: 0.208093, acc: 0.929688]  [G loss: 0.384173, acc: 0.828125]\n",
      "4136: [D loss: 0.153521, acc: 0.937500]  [G loss: 0.232571, acc: 0.906250]\n",
      "4137: [D loss: 0.062417, acc: 0.976562]  [G loss: 0.144402, acc: 0.937500]\n",
      "4138: [D loss: 0.141711, acc: 0.945312]  [G loss: 0.346506, acc: 0.812500]\n",
      "4139: [D loss: 0.102995, acc: 0.960938]  [G loss: 0.095999, acc: 0.953125]\n",
      "4140: [D loss: 0.173758, acc: 0.953125]  [G loss: 0.682677, acc: 0.687500]\n",
      "4141: [D loss: 0.190329, acc: 0.953125]  [G loss: 0.494699, acc: 0.750000]\n",
      "4142: [D loss: 0.163208, acc: 0.937500]  [G loss: 0.948600, acc: 0.593750]\n",
      "4143: [D loss: 0.081027, acc: 0.968750]  [G loss: 0.445787, acc: 0.750000]\n",
      "4144: [D loss: 0.096715, acc: 0.960938]  [G loss: 0.052006, acc: 0.968750]\n",
      "4145: [D loss: 0.523017, acc: 0.851562]  [G loss: 1.806973, acc: 0.250000]\n",
      "4146: [D loss: 0.236075, acc: 0.898438]  [G loss: 0.046166, acc: 0.984375]\n",
      "4147: [D loss: 0.165500, acc: 0.937500]  [G loss: 0.350749, acc: 0.781250]\n",
      "4148: [D loss: 0.147789, acc: 0.937500]  [G loss: 0.150973, acc: 0.906250]\n",
      "4149: [D loss: 0.201574, acc: 0.921875]  [G loss: 0.339283, acc: 0.796875]\n",
      "4150: [D loss: 0.121912, acc: 0.960938]  [G loss: 0.079938, acc: 0.968750]\n",
      "4151: [D loss: 0.166827, acc: 0.906250]  [G loss: 0.409766, acc: 0.781250]\n",
      "4152: [D loss: 0.189551, acc: 0.914062]  [G loss: 1.147529, acc: 0.546875]\n",
      "4153: [D loss: 0.253467, acc: 0.906250]  [G loss: 0.076131, acc: 0.984375]\n",
      "4154: [D loss: 0.287194, acc: 0.875000]  [G loss: 1.207891, acc: 0.390625]\n",
      "4155: [D loss: 0.212055, acc: 0.921875]  [G loss: 0.295590, acc: 0.828125]\n",
      "4156: [D loss: 0.143850, acc: 0.929688]  [G loss: 0.293729, acc: 0.843750]\n",
      "4157: [D loss: 0.064017, acc: 0.992188]  [G loss: 0.297201, acc: 0.843750]\n",
      "4158: [D loss: 0.205417, acc: 0.921875]  [G loss: 3.235834, acc: 0.078125]\n",
      "4159: [D loss: 0.318353, acc: 0.898438]  [G loss: 0.390590, acc: 0.828125]\n",
      "4160: [D loss: 0.387689, acc: 0.867188]  [G loss: 3.060040, acc: 0.078125]\n",
      "4161: [D loss: 0.387638, acc: 0.882812]  [G loss: 0.562877, acc: 0.671875]\n",
      "4162: [D loss: 0.135984, acc: 0.960938]  [G loss: 0.448697, acc: 0.734375]\n",
      "4163: [D loss: 0.250540, acc: 0.867188]  [G loss: 1.484229, acc: 0.296875]\n",
      "4164: [D loss: 0.170428, acc: 0.945312]  [G loss: 0.862472, acc: 0.593750]\n",
      "4165: [D loss: 0.127434, acc: 0.937500]  [G loss: 0.397783, acc: 0.781250]\n",
      "4166: [D loss: 0.108800, acc: 0.968750]  [G loss: 0.705002, acc: 0.703125]\n",
      "4167: [D loss: 0.223975, acc: 0.914062]  [G loss: 0.338672, acc: 0.796875]\n",
      "4168: [D loss: 0.224983, acc: 0.914062]  [G loss: 0.986317, acc: 0.453125]\n",
      "4169: [D loss: 0.141191, acc: 0.953125]  [G loss: 0.206437, acc: 0.906250]\n",
      "4170: [D loss: 0.103087, acc: 0.968750]  [G loss: 0.559957, acc: 0.718750]\n",
      "4171: [D loss: 0.147689, acc: 0.937500]  [G loss: 0.208989, acc: 0.890625]\n",
      "4172: [D loss: 0.212953, acc: 0.898438]  [G loss: 1.210245, acc: 0.437500]\n",
      "4173: [D loss: 0.196035, acc: 0.929688]  [G loss: 0.278796, acc: 0.859375]\n",
      "4174: [D loss: 0.242745, acc: 0.906250]  [G loss: 1.643491, acc: 0.171875]\n",
      "4175: [D loss: 0.335573, acc: 0.851562]  [G loss: 0.405390, acc: 0.734375]\n",
      "4176: [D loss: 0.132021, acc: 0.929688]  [G loss: 0.608269, acc: 0.687500]\n",
      "4177: [D loss: 0.069889, acc: 0.976562]  [G loss: 0.921185, acc: 0.515625]\n",
      "4178: [D loss: 0.134967, acc: 0.953125]  [G loss: 0.286338, acc: 0.890625]\n",
      "4179: [D loss: 0.150032, acc: 0.945312]  [G loss: 0.425391, acc: 0.812500]\n",
      "4180: [D loss: 0.199366, acc: 0.921875]  [G loss: 2.994224, acc: 0.156250]\n",
      "4181: [D loss: 0.523746, acc: 0.828125]  [G loss: 0.285604, acc: 0.890625]\n",
      "4182: [D loss: 0.381788, acc: 0.820312]  [G loss: 1.560180, acc: 0.437500]\n",
      "4183: [D loss: 0.279295, acc: 0.914062]  [G loss: 0.779823, acc: 0.625000]\n",
      "4184: [D loss: 0.075498, acc: 0.984375]  [G loss: 0.515732, acc: 0.765625]\n",
      "4185: [D loss: 0.192168, acc: 0.898438]  [G loss: 0.701516, acc: 0.656250]\n",
      "4186: [D loss: 0.120421, acc: 0.937500]  [G loss: 0.386072, acc: 0.843750]\n",
      "4187: [D loss: 0.157150, acc: 0.921875]  [G loss: 0.739597, acc: 0.640625]\n",
      "4188: [D loss: 0.181271, acc: 0.945312]  [G loss: 0.486881, acc: 0.765625]\n",
      "4189: [D loss: 0.140253, acc: 0.937500]  [G loss: 0.485225, acc: 0.781250]\n",
      "4190: [D loss: 0.114705, acc: 0.953125]  [G loss: 0.427926, acc: 0.750000]\n",
      "4191: [D loss: 0.109145, acc: 0.937500]  [G loss: 0.734202, acc: 0.687500]\n",
      "4192: [D loss: 0.089734, acc: 0.968750]  [G loss: 0.471416, acc: 0.828125]\n",
      "4193: [D loss: 0.232675, acc: 0.898438]  [G loss: 0.797085, acc: 0.750000]\n",
      "4194: [D loss: 0.142188, acc: 0.937500]  [G loss: 0.379949, acc: 0.843750]\n",
      "4195: [D loss: 0.215522, acc: 0.921875]  [G loss: 1.359525, acc: 0.609375]\n",
      "4196: [D loss: 0.144318, acc: 0.921875]  [G loss: 0.436752, acc: 0.750000]\n",
      "4197: [D loss: 0.221553, acc: 0.921875]  [G loss: 1.166538, acc: 0.562500]\n",
      "4198: [D loss: 0.238504, acc: 0.898438]  [G loss: 0.458368, acc: 0.734375]\n",
      "4199: [D loss: 0.130692, acc: 0.953125]  [G loss: 0.696169, acc: 0.687500]\n",
      "4200: [D loss: 0.174990, acc: 0.937500]  [G loss: 0.267075, acc: 0.812500]\n",
      "4201: [D loss: 0.123580, acc: 0.945312]  [G loss: 0.674161, acc: 0.703125]\n",
      "4202: [D loss: 0.088314, acc: 0.968750]  [G loss: 0.390649, acc: 0.812500]\n",
      "4203: [D loss: 0.176628, acc: 0.929688]  [G loss: 1.290815, acc: 0.515625]\n",
      "4204: [D loss: 0.306524, acc: 0.882812]  [G loss: 0.169306, acc: 0.937500]\n",
      "4205: [D loss: 0.219970, acc: 0.945312]  [G loss: 0.728977, acc: 0.671875]\n",
      "4206: [D loss: 0.087021, acc: 0.960938]  [G loss: 0.395480, acc: 0.812500]\n",
      "4207: [D loss: 0.097413, acc: 0.968750]  [G loss: 0.419060, acc: 0.796875]\n",
      "4208: [D loss: 0.134781, acc: 0.960938]  [G loss: 0.414253, acc: 0.765625]\n",
      "4209: [D loss: 0.170722, acc: 0.953125]  [G loss: 1.212725, acc: 0.453125]\n",
      "4210: [D loss: 0.115669, acc: 0.945312]  [G loss: 0.304872, acc: 0.843750]\n",
      "4211: [D loss: 0.087317, acc: 0.968750]  [G loss: 0.576467, acc: 0.750000]\n",
      "4212: [D loss: 0.133896, acc: 0.953125]  [G loss: 0.757590, acc: 0.609375]\n",
      "4213: [D loss: 0.075014, acc: 0.976562]  [G loss: 0.659754, acc: 0.671875]\n",
      "4214: [D loss: 0.195750, acc: 0.937500]  [G loss: 1.018246, acc: 0.640625]\n",
      "4215: [D loss: 0.152069, acc: 0.921875]  [G loss: 0.261672, acc: 0.859375]\n",
      "4216: [D loss: 0.374686, acc: 0.882812]  [G loss: 2.412178, acc: 0.234375]\n",
      "4217: [D loss: 0.223929, acc: 0.914062]  [G loss: 0.596295, acc: 0.671875]\n",
      "4218: [D loss: 0.048460, acc: 0.976562]  [G loss: 0.264176, acc: 0.859375]\n",
      "4219: [D loss: 0.067083, acc: 0.976562]  [G loss: 0.372070, acc: 0.765625]\n",
      "4220: [D loss: 0.100602, acc: 0.960938]  [G loss: 0.614121, acc: 0.671875]\n",
      "4221: [D loss: 0.106033, acc: 0.968750]  [G loss: 0.194090, acc: 0.906250]\n",
      "4222: [D loss: 0.104036, acc: 0.945312]  [G loss: 0.590054, acc: 0.718750]\n",
      "4223: [D loss: 0.063429, acc: 0.976562]  [G loss: 0.712569, acc: 0.687500]\n",
      "4224: [D loss: 0.061464, acc: 0.984375]  [G loss: 0.371261, acc: 0.781250]\n",
      "4225: [D loss: 0.124336, acc: 0.945312]  [G loss: 0.853073, acc: 0.734375]\n",
      "4226: [D loss: 0.084333, acc: 0.960938]  [G loss: 0.247261, acc: 0.859375]\n",
      "4227: [D loss: 0.030139, acc: 1.000000]  [G loss: 0.210476, acc: 0.906250]\n",
      "4228: [D loss: 0.016958, acc: 1.000000]  [G loss: 0.187219, acc: 0.921875]\n",
      "4229: [D loss: 0.136947, acc: 0.945312]  [G loss: 0.378920, acc: 0.828125]\n",
      "4230: [D loss: 0.103396, acc: 0.953125]  [G loss: 1.902623, acc: 0.406250]\n",
      "4231: [D loss: 0.221539, acc: 0.906250]  [G loss: 0.184903, acc: 0.921875]\n",
      "4232: [D loss: 0.255468, acc: 0.921875]  [G loss: 0.980815, acc: 0.671875]\n",
      "4233: [D loss: 0.154701, acc: 0.921875]  [G loss: 0.855253, acc: 0.687500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4234: [D loss: 0.081927, acc: 0.968750]  [G loss: 0.983089, acc: 0.515625]\n",
      "4235: [D loss: 0.186070, acc: 0.929688]  [G loss: 0.980364, acc: 0.546875]\n",
      "4236: [D loss: 0.159843, acc: 0.914062]  [G loss: 0.105982, acc: 0.968750]\n",
      "4237: [D loss: 0.139918, acc: 0.929688]  [G loss: 0.953976, acc: 0.546875]\n",
      "4238: [D loss: 0.070352, acc: 0.968750]  [G loss: 0.717653, acc: 0.625000]\n",
      "4239: [D loss: 0.127289, acc: 0.945312]  [G loss: 0.821476, acc: 0.671875]\n",
      "4240: [D loss: 0.049156, acc: 0.984375]  [G loss: 0.845100, acc: 0.703125]\n",
      "4241: [D loss: 0.072063, acc: 0.968750]  [G loss: 0.336850, acc: 0.828125]\n",
      "4242: [D loss: 0.107532, acc: 0.937500]  [G loss: 1.067199, acc: 0.515625]\n",
      "4243: [D loss: 0.106236, acc: 0.945312]  [G loss: 0.239156, acc: 0.875000]\n",
      "4244: [D loss: 0.195027, acc: 0.929688]  [G loss: 2.765634, acc: 0.390625]\n",
      "4245: [D loss: 0.107348, acc: 0.960938]  [G loss: 0.492879, acc: 0.812500]\n",
      "4246: [D loss: 0.059645, acc: 0.976562]  [G loss: 0.489367, acc: 0.687500]\n",
      "4247: [D loss: 0.166828, acc: 0.921875]  [G loss: 1.954439, acc: 0.265625]\n",
      "4248: [D loss: 0.385124, acc: 0.835938]  [G loss: 0.172521, acc: 0.921875]\n",
      "4249: [D loss: 0.222459, acc: 0.859375]  [G loss: 1.904121, acc: 0.406250]\n",
      "4250: [D loss: 0.253043, acc: 0.882812]  [G loss: 2.517670, acc: 0.203125]\n",
      "4251: [D loss: 0.189601, acc: 0.937500]  [G loss: 0.618760, acc: 0.703125]\n",
      "4252: [D loss: 0.349235, acc: 0.859375]  [G loss: 2.110820, acc: 0.453125]\n",
      "4253: [D loss: 0.279001, acc: 0.890625]  [G loss: 0.606081, acc: 0.765625]\n",
      "4254: [D loss: 0.155037, acc: 0.921875]  [G loss: 0.828964, acc: 0.546875]\n",
      "4255: [D loss: 0.124141, acc: 0.937500]  [G loss: 1.647371, acc: 0.328125]\n",
      "4256: [D loss: 0.157265, acc: 0.929688]  [G loss: 0.231302, acc: 0.890625]\n",
      "4257: [D loss: 0.277870, acc: 0.867188]  [G loss: 1.208488, acc: 0.500000]\n",
      "4258: [D loss: 0.191219, acc: 0.937500]  [G loss: 0.344645, acc: 0.828125]\n",
      "4259: [D loss: 0.222067, acc: 0.906250]  [G loss: 0.982033, acc: 0.546875]\n",
      "4260: [D loss: 0.306200, acc: 0.875000]  [G loss: 0.691444, acc: 0.656250]\n",
      "4261: [D loss: 0.199720, acc: 0.921875]  [G loss: 0.394522, acc: 0.796875]\n",
      "4262: [D loss: 0.133768, acc: 0.945312]  [G loss: 0.821450, acc: 0.671875]\n",
      "4263: [D loss: 0.189405, acc: 0.906250]  [G loss: 0.268575, acc: 0.843750]\n",
      "4264: [D loss: 0.118321, acc: 0.960938]  [G loss: 1.106854, acc: 0.578125]\n",
      "4265: [D loss: 0.246587, acc: 0.914062]  [G loss: 0.553922, acc: 0.718750]\n",
      "4266: [D loss: 0.178228, acc: 0.929688]  [G loss: 1.274450, acc: 0.515625]\n",
      "4267: [D loss: 0.290100, acc: 0.882812]  [G loss: 0.044648, acc: 1.000000]\n",
      "4268: [D loss: 0.344435, acc: 0.882812]  [G loss: 2.683044, acc: 0.343750]\n",
      "4269: [D loss: 0.367249, acc: 0.914062]  [G loss: 0.476307, acc: 0.750000]\n",
      "4270: [D loss: 0.242198, acc: 0.914062]  [G loss: 0.566488, acc: 0.718750]\n",
      "4271: [D loss: 0.133090, acc: 0.953125]  [G loss: 0.528072, acc: 0.703125]\n",
      "4272: [D loss: 0.246262, acc: 0.921875]  [G loss: 1.464175, acc: 0.343750]\n",
      "4273: [D loss: 0.160681, acc: 0.937500]  [G loss: 0.726897, acc: 0.734375]\n",
      "4274: [D loss: 0.131247, acc: 0.937500]  [G loss: 0.332613, acc: 0.843750]\n",
      "4275: [D loss: 0.099680, acc: 0.968750]  [G loss: 0.508474, acc: 0.765625]\n",
      "4276: [D loss: 0.095141, acc: 0.976562]  [G loss: 0.426267, acc: 0.781250]\n",
      "4277: [D loss: 0.091676, acc: 0.976562]  [G loss: 0.696665, acc: 0.656250]\n",
      "4278: [D loss: 0.126108, acc: 0.945312]  [G loss: 0.880364, acc: 0.687500]\n",
      "4279: [D loss: 0.220161, acc: 0.898438]  [G loss: 0.371347, acc: 0.765625]\n",
      "4280: [D loss: 0.177595, acc: 0.921875]  [G loss: 0.607670, acc: 0.703125]\n",
      "4281: [D loss: 0.077899, acc: 0.968750]  [G loss: 0.129790, acc: 0.937500]\n",
      "4282: [D loss: 0.147832, acc: 0.921875]  [G loss: 1.190884, acc: 0.515625]\n",
      "4283: [D loss: 0.082869, acc: 0.976562]  [G loss: 0.636536, acc: 0.671875]\n",
      "4284: [D loss: 0.285580, acc: 0.882812]  [G loss: 2.168434, acc: 0.250000]\n",
      "4285: [D loss: 0.447313, acc: 0.828125]  [G loss: 0.199106, acc: 0.937500]\n",
      "4286: [D loss: 0.718777, acc: 0.804688]  [G loss: 3.758305, acc: 0.187500]\n",
      "4287: [D loss: 0.177943, acc: 0.937500]  [G loss: 1.516675, acc: 0.390625]\n",
      "4288: [D loss: 0.130787, acc: 0.953125]  [G loss: 0.741237, acc: 0.687500]\n",
      "4289: [D loss: 0.231366, acc: 0.914062]  [G loss: 1.299889, acc: 0.375000]\n",
      "4290: [D loss: 0.128098, acc: 0.937500]  [G loss: 0.768982, acc: 0.640625]\n",
      "4291: [D loss: 0.125904, acc: 0.937500]  [G loss: 1.272669, acc: 0.406250]\n",
      "4292: [D loss: 0.113319, acc: 0.953125]  [G loss: 0.534801, acc: 0.656250]\n",
      "4293: [D loss: 0.123000, acc: 0.945312]  [G loss: 0.337983, acc: 0.875000]\n",
      "4294: [D loss: 0.191749, acc: 0.914062]  [G loss: 0.851680, acc: 0.562500]\n",
      "4295: [D loss: 0.239065, acc: 0.898438]  [G loss: 0.664447, acc: 0.656250]\n",
      "4296: [D loss: 0.090362, acc: 0.960938]  [G loss: 0.896967, acc: 0.546875]\n",
      "4297: [D loss: 0.041470, acc: 0.992188]  [G loss: 0.997699, acc: 0.531250]\n",
      "4298: [D loss: 0.249752, acc: 0.890625]  [G loss: 0.342677, acc: 0.828125]\n",
      "4299: [D loss: 0.187167, acc: 0.914062]  [G loss: 0.902182, acc: 0.609375]\n",
      "4300: [D loss: 0.167280, acc: 0.898438]  [G loss: 0.245278, acc: 0.875000]\n",
      "4301: [D loss: 0.192007, acc: 0.914062]  [G loss: 1.500711, acc: 0.265625]\n",
      "4302: [D loss: 0.243619, acc: 0.882812]  [G loss: 0.304961, acc: 0.843750]\n",
      "4303: [D loss: 0.228639, acc: 0.898438]  [G loss: 1.164745, acc: 0.562500]\n",
      "4304: [D loss: 0.148032, acc: 0.921875]  [G loss: 1.058929, acc: 0.546875]\n",
      "4305: [D loss: 0.173721, acc: 0.937500]  [G loss: 0.834067, acc: 0.640625]\n",
      "4306: [D loss: 0.102395, acc: 0.984375]  [G loss: 0.541105, acc: 0.734375]\n",
      "4307: [D loss: 0.073918, acc: 0.984375]  [G loss: 0.611378, acc: 0.734375]\n",
      "4308: [D loss: 0.098869, acc: 0.968750]  [G loss: 0.832437, acc: 0.562500]\n",
      "4309: [D loss: 0.066057, acc: 0.984375]  [G loss: 0.862093, acc: 0.578125]\n",
      "4310: [D loss: 0.212099, acc: 0.914062]  [G loss: 0.864926, acc: 0.609375]\n",
      "4311: [D loss: 0.114482, acc: 0.937500]  [G loss: 0.992574, acc: 0.500000]\n",
      "4312: [D loss: 0.222972, acc: 0.906250]  [G loss: 0.214605, acc: 0.921875]\n",
      "4313: [D loss: 0.200961, acc: 0.914062]  [G loss: 1.051048, acc: 0.625000]\n",
      "4314: [D loss: 0.241075, acc: 0.898438]  [G loss: 0.760563, acc: 0.609375]\n",
      "4315: [D loss: 0.202241, acc: 0.921875]  [G loss: 0.495684, acc: 0.750000]\n",
      "4316: [D loss: 0.119515, acc: 0.953125]  [G loss: 0.438706, acc: 0.781250]\n",
      "4317: [D loss: 0.187979, acc: 0.929688]  [G loss: 0.245691, acc: 0.859375]\n",
      "4318: [D loss: 0.069777, acc: 0.976562]  [G loss: 0.485727, acc: 0.796875]\n",
      "4319: [D loss: 0.200253, acc: 0.914062]  [G loss: 0.825238, acc: 0.671875]\n",
      "4320: [D loss: 0.078299, acc: 0.968750]  [G loss: 0.506573, acc: 0.796875]\n",
      "4321: [D loss: 0.199597, acc: 0.906250]  [G loss: 0.849392, acc: 0.625000]\n",
      "4322: [D loss: 0.152852, acc: 0.953125]  [G loss: 1.135357, acc: 0.625000]\n",
      "4323: [D loss: 0.086893, acc: 0.968750]  [G loss: 0.242257, acc: 0.906250]\n",
      "4324: [D loss: 0.251051, acc: 0.906250]  [G loss: 3.004667, acc: 0.343750]\n",
      "4325: [D loss: 0.294751, acc: 0.851562]  [G loss: 0.326784, acc: 0.796875]\n",
      "4326: [D loss: 0.512179, acc: 0.828125]  [G loss: 2.029804, acc: 0.343750]\n",
      "4327: [D loss: 0.305182, acc: 0.875000]  [G loss: 0.178513, acc: 0.906250]\n",
      "4328: [D loss: 0.355175, acc: 0.812500]  [G loss: 1.958052, acc: 0.343750]\n",
      "4329: [D loss: 0.120267, acc: 0.960938]  [G loss: 0.981959, acc: 0.562500]\n",
      "4330: [D loss: 0.101662, acc: 0.945312]  [G loss: 0.625171, acc: 0.687500]\n",
      "4331: [D loss: 0.141575, acc: 0.953125]  [G loss: 0.524384, acc: 0.703125]\n",
      "4332: [D loss: 0.090984, acc: 0.968750]  [G loss: 0.443066, acc: 0.781250]\n",
      "4333: [D loss: 0.246047, acc: 0.906250]  [G loss: 2.116326, acc: 0.453125]\n",
      "4334: [D loss: 0.321436, acc: 0.875000]  [G loss: 0.802641, acc: 0.609375]\n",
      "4335: [D loss: 0.209063, acc: 0.898438]  [G loss: 0.380109, acc: 0.812500]\n",
      "4336: [D loss: 0.269163, acc: 0.890625]  [G loss: 0.766909, acc: 0.671875]\n",
      "4337: [D loss: 0.215951, acc: 0.890625]  [G loss: 0.246007, acc: 0.890625]\n",
      "4338: [D loss: 0.132253, acc: 0.937500]  [G loss: 0.321714, acc: 0.859375]\n",
      "4339: [D loss: 0.081397, acc: 0.976562]  [G loss: 0.358791, acc: 0.796875]\n",
      "4340: [D loss: 0.180210, acc: 0.914062]  [G loss: 0.299273, acc: 0.843750]\n",
      "4341: [D loss: 0.102057, acc: 0.960938]  [G loss: 0.302035, acc: 0.796875]\n",
      "4342: [D loss: 0.089170, acc: 0.976562]  [G loss: 0.604436, acc: 0.812500]\n",
      "4343: [D loss: 0.090146, acc: 0.968750]  [G loss: 0.232691, acc: 0.875000]\n",
      "4344: [D loss: 0.088573, acc: 0.960938]  [G loss: 0.184350, acc: 0.890625]\n",
      "4345: [D loss: 0.168740, acc: 0.945312]  [G loss: 1.545994, acc: 0.546875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4346: [D loss: 0.292601, acc: 0.882812]  [G loss: 0.172020, acc: 0.906250]\n",
      "4347: [D loss: 0.259441, acc: 0.875000]  [G loss: 1.545052, acc: 0.500000]\n",
      "4348: [D loss: 0.130313, acc: 0.937500]  [G loss: 0.445292, acc: 0.750000]\n",
      "4349: [D loss: 0.163805, acc: 0.953125]  [G loss: 0.560851, acc: 0.750000]\n",
      "4350: [D loss: 0.110984, acc: 0.960938]  [G loss: 0.589195, acc: 0.687500]\n",
      "4351: [D loss: 0.165438, acc: 0.937500]  [G loss: 0.130331, acc: 0.968750]\n",
      "4352: [D loss: 0.154355, acc: 0.921875]  [G loss: 2.028227, acc: 0.218750]\n",
      "4353: [D loss: 0.136620, acc: 0.953125]  [G loss: 0.421598, acc: 0.765625]\n",
      "4354: [D loss: 0.150236, acc: 0.929688]  [G loss: 0.576446, acc: 0.750000]\n",
      "4355: [D loss: 0.244394, acc: 0.945312]  [G loss: 0.510079, acc: 0.796875]\n",
      "4356: [D loss: 0.051707, acc: 0.976562]  [G loss: 0.231498, acc: 0.890625]\n",
      "4357: [D loss: 0.044619, acc: 0.992188]  [G loss: 0.507507, acc: 0.812500]\n",
      "4358: [D loss: 0.161668, acc: 0.929688]  [G loss: 0.656106, acc: 0.703125]\n",
      "4359: [D loss: 0.113557, acc: 0.937500]  [G loss: 0.163947, acc: 0.921875]\n",
      "4360: [D loss: 0.131268, acc: 0.953125]  [G loss: 0.714753, acc: 0.734375]\n",
      "4361: [D loss: 0.116482, acc: 0.945312]  [G loss: 0.078247, acc: 0.968750]\n",
      "4362: [D loss: 0.172885, acc: 0.921875]  [G loss: 2.667727, acc: 0.171875]\n",
      "4363: [D loss: 0.302299, acc: 0.859375]  [G loss: 0.027827, acc: 1.000000]\n",
      "4364: [D loss: 0.061525, acc: 0.992188]  [G loss: 0.126317, acc: 0.953125]\n",
      "4365: [D loss: 0.104244, acc: 0.968750]  [G loss: 0.117154, acc: 0.968750]\n",
      "4366: [D loss: 0.072748, acc: 0.976562]  [G loss: 0.165742, acc: 0.921875]\n",
      "4367: [D loss: 0.051107, acc: 0.984375]  [G loss: 0.059970, acc: 0.968750]\n",
      "4368: [D loss: 0.040113, acc: 0.976562]  [G loss: 0.139264, acc: 0.937500]\n",
      "4369: [D loss: 0.127969, acc: 0.960938]  [G loss: 0.094471, acc: 0.953125]\n",
      "4370: [D loss: 0.060004, acc: 0.984375]  [G loss: 0.207640, acc: 0.937500]\n",
      "4371: [D loss: 0.084173, acc: 0.960938]  [G loss: 0.066089, acc: 0.968750]\n",
      "4372: [D loss: 0.071167, acc: 0.976562]  [G loss: 0.062729, acc: 0.984375]\n",
      "4373: [D loss: 0.027012, acc: 1.000000]  [G loss: 0.042051, acc: 0.984375]\n",
      "4374: [D loss: 0.098894, acc: 0.960938]  [G loss: 0.262708, acc: 0.890625]\n",
      "4375: [D loss: 0.114102, acc: 0.960938]  [G loss: 0.036510, acc: 1.000000]\n",
      "4376: [D loss: 0.115660, acc: 0.953125]  [G loss: 0.300929, acc: 0.890625]\n",
      "4377: [D loss: 0.175625, acc: 0.937500]  [G loss: 0.688547, acc: 0.656250]\n",
      "4378: [D loss: 0.121578, acc: 0.937500]  [G loss: 0.021730, acc: 1.000000]\n",
      "4379: [D loss: 0.222894, acc: 0.890625]  [G loss: 2.477067, acc: 0.343750]\n",
      "4380: [D loss: 0.221405, acc: 0.929688]  [G loss: 0.064389, acc: 0.984375]\n",
      "4381: [D loss: 0.193626, acc: 0.937500]  [G loss: 0.380422, acc: 0.734375]\n",
      "4382: [D loss: 0.237064, acc: 0.890625]  [G loss: 0.217140, acc: 0.890625]\n",
      "4383: [D loss: 0.340194, acc: 0.835938]  [G loss: 4.166310, acc: 0.046875]\n",
      "4384: [D loss: 0.653536, acc: 0.773438]  [G loss: 0.299019, acc: 0.875000]\n",
      "4385: [D loss: 0.697600, acc: 0.796875]  [G loss: 1.939246, acc: 0.281250]\n",
      "4386: [D loss: 0.235694, acc: 0.914062]  [G loss: 0.375882, acc: 0.796875]\n",
      "4387: [D loss: 0.212101, acc: 0.921875]  [G loss: 1.065611, acc: 0.515625]\n",
      "4388: [D loss: 0.297907, acc: 0.828125]  [G loss: 1.101771, acc: 0.484375]\n",
      "4389: [D loss: 0.113226, acc: 0.953125]  [G loss: 0.916075, acc: 0.484375]\n",
      "4390: [D loss: 0.277418, acc: 0.867188]  [G loss: 1.440786, acc: 0.375000]\n",
      "4391: [D loss: 0.169048, acc: 0.937500]  [G loss: 0.240208, acc: 0.859375]\n",
      "4392: [D loss: 0.108613, acc: 0.976562]  [G loss: 0.336866, acc: 0.843750]\n",
      "4393: [D loss: 0.238571, acc: 0.937500]  [G loss: 1.064719, acc: 0.500000]\n",
      "4394: [D loss: 0.095548, acc: 0.968750]  [G loss: 0.220901, acc: 0.890625]\n",
      "4395: [D loss: 0.065009, acc: 0.992188]  [G loss: 0.195967, acc: 0.921875]\n",
      "4396: [D loss: 0.231882, acc: 0.929688]  [G loss: 0.625603, acc: 0.640625]\n",
      "4397: [D loss: 0.118413, acc: 0.960938]  [G loss: 0.169042, acc: 0.906250]\n",
      "4398: [D loss: 0.437694, acc: 0.875000]  [G loss: 1.818571, acc: 0.250000]\n",
      "4399: [D loss: 0.376955, acc: 0.835938]  [G loss: 1.289479, acc: 0.484375]\n",
      "4400: [D loss: 0.231035, acc: 0.890625]  [G loss: 0.504441, acc: 0.734375]\n",
      "4401: [D loss: 0.267356, acc: 0.875000]  [G loss: 1.857193, acc: 0.343750]\n",
      "4402: [D loss: 0.251262, acc: 0.898438]  [G loss: 0.432249, acc: 0.750000]\n",
      "4403: [D loss: 0.201145, acc: 0.906250]  [G loss: 1.477921, acc: 0.406250]\n",
      "4404: [D loss: 0.164180, acc: 0.921875]  [G loss: 0.637200, acc: 0.625000]\n",
      "4405: [D loss: 0.152122, acc: 0.921875]  [G loss: 0.409174, acc: 0.718750]\n",
      "4406: [D loss: 0.177775, acc: 0.929688]  [G loss: 1.292142, acc: 0.468750]\n",
      "4407: [D loss: 0.169676, acc: 0.929688]  [G loss: 0.267224, acc: 0.843750]\n",
      "4408: [D loss: 0.216367, acc: 0.906250]  [G loss: 1.165673, acc: 0.531250]\n",
      "4409: [D loss: 0.302462, acc: 0.875000]  [G loss: 0.140523, acc: 0.921875]\n",
      "4410: [D loss: 0.334101, acc: 0.898438]  [G loss: 0.890472, acc: 0.578125]\n",
      "4411: [D loss: 0.242385, acc: 0.898438]  [G loss: 0.209805, acc: 0.906250]\n",
      "4412: [D loss: 0.224916, acc: 0.867188]  [G loss: 0.532070, acc: 0.687500]\n",
      "4413: [D loss: 0.184374, acc: 0.921875]  [G loss: 0.240578, acc: 0.859375]\n",
      "4414: [D loss: 0.212738, acc: 0.906250]  [G loss: 0.537737, acc: 0.734375]\n",
      "4415: [D loss: 0.200746, acc: 0.937500]  [G loss: 0.592642, acc: 0.750000]\n",
      "4416: [D loss: 0.108050, acc: 0.976562]  [G loss: 0.670314, acc: 0.703125]\n",
      "4417: [D loss: 0.119807, acc: 0.960938]  [G loss: 0.301615, acc: 0.843750]\n",
      "4418: [D loss: 0.304092, acc: 0.906250]  [G loss: 1.252662, acc: 0.609375]\n",
      "4419: [D loss: 0.219300, acc: 0.890625]  [G loss: 0.345250, acc: 0.812500]\n",
      "4420: [D loss: 0.280699, acc: 0.875000]  [G loss: 1.660339, acc: 0.296875]\n",
      "4421: [D loss: 0.226309, acc: 0.898438]  [G loss: 0.364466, acc: 0.781250]\n",
      "4422: [D loss: 0.336150, acc: 0.882812]  [G loss: 1.007729, acc: 0.562500]\n",
      "4423: [D loss: 0.320976, acc: 0.851562]  [G loss: 0.153482, acc: 0.921875]\n",
      "4424: [D loss: 0.621566, acc: 0.734375]  [G loss: 1.641095, acc: 0.406250]\n",
      "4425: [D loss: 0.341530, acc: 0.859375]  [G loss: 0.573066, acc: 0.656250]\n",
      "4426: [D loss: 0.264037, acc: 0.859375]  [G loss: 0.331601, acc: 0.812500]\n",
      "4427: [D loss: 0.203454, acc: 0.906250]  [G loss: 0.260191, acc: 0.843750]\n",
      "4428: [D loss: 0.254479, acc: 0.906250]  [G loss: 0.490904, acc: 0.781250]\n",
      "4429: [D loss: 0.190040, acc: 0.929688]  [G loss: 0.351677, acc: 0.843750]\n",
      "4430: [D loss: 0.174009, acc: 0.921875]  [G loss: 0.401373, acc: 0.781250]\n",
      "4431: [D loss: 0.119488, acc: 0.937500]  [G loss: 0.429516, acc: 0.781250]\n",
      "4432: [D loss: 0.180047, acc: 0.929688]  [G loss: 0.473215, acc: 0.765625]\n",
      "4433: [D loss: 0.279435, acc: 0.898438]  [G loss: 0.494922, acc: 0.781250]\n",
      "4434: [D loss: 0.205139, acc: 0.937500]  [G loss: 0.204583, acc: 0.890625]\n",
      "4435: [D loss: 0.308700, acc: 0.898438]  [G loss: 1.336251, acc: 0.437500]\n",
      "4436: [D loss: 0.230680, acc: 0.890625]  [G loss: 0.370727, acc: 0.796875]\n",
      "4437: [D loss: 0.310713, acc: 0.851562]  [G loss: 0.776138, acc: 0.671875]\n",
      "4438: [D loss: 0.202639, acc: 0.937500]  [G loss: 0.362258, acc: 0.828125]\n",
      "4439: [D loss: 0.204828, acc: 0.929688]  [G loss: 0.833827, acc: 0.703125]\n",
      "4440: [D loss: 0.327512, acc: 0.859375]  [G loss: 0.123911, acc: 0.937500]\n",
      "4441: [D loss: 0.288784, acc: 0.875000]  [G loss: 0.544700, acc: 0.671875]\n",
      "4442: [D loss: 0.169999, acc: 0.945312]  [G loss: 0.341187, acc: 0.843750]\n",
      "4443: [D loss: 0.181247, acc: 0.914062]  [G loss: 0.220645, acc: 0.890625]\n",
      "4444: [D loss: 0.236264, acc: 0.859375]  [G loss: 0.801231, acc: 0.625000]\n",
      "4445: [D loss: 0.308814, acc: 0.859375]  [G loss: 0.617477, acc: 0.640625]\n",
      "4446: [D loss: 0.242181, acc: 0.914062]  [G loss: 0.693429, acc: 0.687500]\n",
      "4447: [D loss: 0.272006, acc: 0.867188]  [G loss: 0.072950, acc: 0.968750]\n",
      "4448: [D loss: 0.283956, acc: 0.867188]  [G loss: 1.323055, acc: 0.453125]\n",
      "4449: [D loss: 0.252034, acc: 0.875000]  [G loss: 0.182055, acc: 0.890625]\n",
      "4450: [D loss: 0.274021, acc: 0.867188]  [G loss: 0.583050, acc: 0.687500]\n",
      "4451: [D loss: 0.166733, acc: 0.953125]  [G loss: 0.420792, acc: 0.781250]\n",
      "4452: [D loss: 0.268644, acc: 0.906250]  [G loss: 1.033998, acc: 0.468750]\n",
      "4453: [D loss: 0.235114, acc: 0.890625]  [G loss: 0.264540, acc: 0.875000]\n",
      "4454: [D loss: 0.194666, acc: 0.898438]  [G loss: 0.651074, acc: 0.656250]\n",
      "4455: [D loss: 0.204738, acc: 0.921875]  [G loss: 0.200747, acc: 0.875000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4456: [D loss: 0.176573, acc: 0.937500]  [G loss: 0.542431, acc: 0.781250]\n",
      "4457: [D loss: 0.122736, acc: 0.976562]  [G loss: 0.649825, acc: 0.703125]\n",
      "4458: [D loss: 0.157398, acc: 0.937500]  [G loss: 0.328165, acc: 0.796875]\n",
      "4459: [D loss: 0.180852, acc: 0.914062]  [G loss: 0.767163, acc: 0.609375]\n",
      "4460: [D loss: 0.144545, acc: 0.960938]  [G loss: 0.465088, acc: 0.812500]\n",
      "4461: [D loss: 0.217327, acc: 0.890625]  [G loss: 1.708748, acc: 0.437500]\n",
      "4462: [D loss: 0.212256, acc: 0.906250]  [G loss: 0.393460, acc: 0.796875]\n",
      "4463: [D loss: 0.280154, acc: 0.851562]  [G loss: 1.108575, acc: 0.515625]\n",
      "4464: [D loss: 0.291260, acc: 0.859375]  [G loss: 0.241625, acc: 0.875000]\n",
      "4465: [D loss: 0.240902, acc: 0.859375]  [G loss: 1.316453, acc: 0.390625]\n",
      "4466: [D loss: 0.226840, acc: 0.906250]  [G loss: 0.329066, acc: 0.875000]\n",
      "4467: [D loss: 0.215354, acc: 0.890625]  [G loss: 1.044699, acc: 0.625000]\n",
      "4468: [D loss: 0.139947, acc: 0.914062]  [G loss: 0.390210, acc: 0.781250]\n",
      "4469: [D loss: 0.278150, acc: 0.890625]  [G loss: 1.134990, acc: 0.656250]\n",
      "4470: [D loss: 0.311864, acc: 0.882812]  [G loss: 0.432424, acc: 0.718750]\n",
      "4471: [D loss: 0.308693, acc: 0.843750]  [G loss: 1.203014, acc: 0.453125]\n",
      "4472: [D loss: 0.199644, acc: 0.914062]  [G loss: 0.430715, acc: 0.750000]\n",
      "4473: [D loss: 0.299259, acc: 0.859375]  [G loss: 0.599169, acc: 0.671875]\n",
      "4474: [D loss: 0.254507, acc: 0.906250]  [G loss: 0.294385, acc: 0.781250]\n",
      "4475: [D loss: 0.134715, acc: 0.937500]  [G loss: 0.270850, acc: 0.875000]\n",
      "4476: [D loss: 0.181468, acc: 0.914062]  [G loss: 0.253534, acc: 0.875000]\n",
      "4477: [D loss: 0.136304, acc: 0.929688]  [G loss: 0.543156, acc: 0.828125]\n",
      "4478: [D loss: 0.328194, acc: 0.859375]  [G loss: 0.301170, acc: 0.828125]\n",
      "4479: [D loss: 0.108449, acc: 0.960938]  [G loss: 0.467436, acc: 0.765625]\n",
      "4480: [D loss: 0.195038, acc: 0.937500]  [G loss: 0.445809, acc: 0.781250]\n",
      "4481: [D loss: 0.105843, acc: 0.968750]  [G loss: 0.538886, acc: 0.687500]\n",
      "4482: [D loss: 0.146901, acc: 0.937500]  [G loss: 0.710941, acc: 0.734375]\n",
      "4483: [D loss: 0.212908, acc: 0.906250]  [G loss: 0.181165, acc: 0.906250]\n",
      "4484: [D loss: 0.182797, acc: 0.921875]  [G loss: 0.698856, acc: 0.718750]\n",
      "4485: [D loss: 0.312181, acc: 0.898438]  [G loss: 0.411794, acc: 0.750000]\n",
      "4486: [D loss: 0.100919, acc: 0.953125]  [G loss: 0.490688, acc: 0.765625]\n",
      "4487: [D loss: 0.233911, acc: 0.906250]  [G loss: 0.535959, acc: 0.812500]\n",
      "4488: [D loss: 0.199775, acc: 0.898438]  [G loss: 0.862223, acc: 0.578125]\n",
      "4489: [D loss: 0.260167, acc: 0.882812]  [G loss: 0.476364, acc: 0.734375]\n",
      "4490: [D loss: 0.179130, acc: 0.929688]  [G loss: 1.025446, acc: 0.562500]\n",
      "4491: [D loss: 0.182238, acc: 0.914062]  [G loss: 0.382889, acc: 0.828125]\n",
      "4492: [D loss: 0.253520, acc: 0.851562]  [G loss: 1.081345, acc: 0.531250]\n",
      "4493: [D loss: 0.258090, acc: 0.890625]  [G loss: 0.490058, acc: 0.718750]\n",
      "4494: [D loss: 0.132028, acc: 0.929688]  [G loss: 0.759512, acc: 0.625000]\n",
      "4495: [D loss: 0.169514, acc: 0.929688]  [G loss: 0.491629, acc: 0.734375]\n",
      "4496: [D loss: 0.196937, acc: 0.890625]  [G loss: 1.073213, acc: 0.562500]\n",
      "4497: [D loss: 0.170595, acc: 0.914062]  [G loss: 0.620480, acc: 0.625000]\n",
      "4498: [D loss: 0.125536, acc: 0.945312]  [G loss: 0.983057, acc: 0.609375]\n",
      "4499: [D loss: 0.268509, acc: 0.906250]  [G loss: 0.564859, acc: 0.703125]\n",
      "4500: [D loss: 0.264226, acc: 0.914062]  [G loss: 0.620100, acc: 0.718750]\n",
      "4501: [D loss: 0.236813, acc: 0.906250]  [G loss: 0.425413, acc: 0.796875]\n",
      "4502: [D loss: 0.172296, acc: 0.937500]  [G loss: 0.838794, acc: 0.625000]\n",
      "4503: [D loss: 0.226971, acc: 0.898438]  [G loss: 0.559627, acc: 0.718750]\n",
      "4504: [D loss: 0.200258, acc: 0.890625]  [G loss: 0.665257, acc: 0.687500]\n",
      "4505: [D loss: 0.165832, acc: 0.929688]  [G loss: 0.288785, acc: 0.828125]\n",
      "4506: [D loss: 0.295470, acc: 0.906250]  [G loss: 1.334323, acc: 0.484375]\n",
      "4507: [D loss: 0.385287, acc: 0.898438]  [G loss: 0.154490, acc: 0.953125]\n",
      "4508: [D loss: 0.447335, acc: 0.828125]  [G loss: 1.779955, acc: 0.406250]\n",
      "4509: [D loss: 0.363446, acc: 0.843750]  [G loss: 0.281360, acc: 0.890625]\n",
      "4510: [D loss: 0.233848, acc: 0.890625]  [G loss: 0.996069, acc: 0.593750]\n",
      "4511: [D loss: 0.324712, acc: 0.875000]  [G loss: 0.627280, acc: 0.687500]\n",
      "4512: [D loss: 0.140450, acc: 0.953125]  [G loss: 0.645063, acc: 0.640625]\n",
      "4513: [D loss: 0.210447, acc: 0.921875]  [G loss: 0.252744, acc: 0.875000]\n",
      "4514: [D loss: 0.217995, acc: 0.906250]  [G loss: 0.811231, acc: 0.609375]\n",
      "4515: [D loss: 0.172201, acc: 0.937500]  [G loss: 0.441827, acc: 0.734375]\n",
      "4516: [D loss: 0.242584, acc: 0.890625]  [G loss: 0.688576, acc: 0.671875]\n",
      "4517: [D loss: 0.175162, acc: 0.945312]  [G loss: 0.441886, acc: 0.796875]\n",
      "4518: [D loss: 0.199401, acc: 0.914062]  [G loss: 0.549785, acc: 0.718750]\n",
      "4519: [D loss: 0.153248, acc: 0.953125]  [G loss: 0.383426, acc: 0.812500]\n",
      "4520: [D loss: 0.129686, acc: 0.945312]  [G loss: 1.151886, acc: 0.468750]\n",
      "4521: [D loss: 0.167805, acc: 0.929688]  [G loss: 1.215926, acc: 0.515625]\n",
      "4522: [D loss: 0.268227, acc: 0.882812]  [G loss: 0.166750, acc: 0.953125]\n",
      "4523: [D loss: 0.121501, acc: 0.937500]  [G loss: 0.670981, acc: 0.703125]\n",
      "4524: [D loss: 0.103488, acc: 0.929688]  [G loss: 0.204955, acc: 0.921875]\n",
      "4525: [D loss: 0.163274, acc: 0.945312]  [G loss: 0.472419, acc: 0.750000]\n",
      "4526: [D loss: 0.081616, acc: 0.976562]  [G loss: 0.239657, acc: 0.890625]\n",
      "4527: [D loss: 0.206328, acc: 0.929688]  [G loss: 0.785900, acc: 0.640625]\n",
      "4528: [D loss: 0.169357, acc: 0.929688]  [G loss: 0.665947, acc: 0.703125]\n",
      "4529: [D loss: 0.190624, acc: 0.929688]  [G loss: 0.535456, acc: 0.734375]\n",
      "4530: [D loss: 0.270992, acc: 0.890625]  [G loss: 3.935751, acc: 0.125000]\n",
      "4531: [D loss: 0.256153, acc: 0.882812]  [G loss: 1.464368, acc: 0.328125]\n",
      "4532: [D loss: 0.177312, acc: 0.914062]  [G loss: 0.724200, acc: 0.593750]\n",
      "4533: [D loss: 0.242118, acc: 0.898438]  [G loss: 0.495551, acc: 0.765625]\n",
      "4534: [D loss: 0.184326, acc: 0.914062]  [G loss: 0.191725, acc: 0.906250]\n",
      "4535: [D loss: 0.215759, acc: 0.914062]  [G loss: 0.781268, acc: 0.562500]\n",
      "4536: [D loss: 0.220270, acc: 0.906250]  [G loss: 0.204911, acc: 0.890625]\n",
      "4537: [D loss: 0.194623, acc: 0.906250]  [G loss: 1.282821, acc: 0.593750]\n",
      "4538: [D loss: 0.394842, acc: 0.843750]  [G loss: 0.251167, acc: 0.875000]\n",
      "4539: [D loss: 0.593909, acc: 0.757812]  [G loss: 2.804656, acc: 0.156250]\n",
      "4540: [D loss: 0.198466, acc: 0.929688]  [G loss: 0.732338, acc: 0.671875]\n",
      "4541: [D loss: 0.303624, acc: 0.914062]  [G loss: 0.833481, acc: 0.703125]\n",
      "4542: [D loss: 0.238995, acc: 0.890625]  [G loss: 0.378751, acc: 0.765625]\n",
      "4543: [D loss: 0.163061, acc: 0.914062]  [G loss: 0.498145, acc: 0.750000]\n",
      "4544: [D loss: 0.124482, acc: 0.953125]  [G loss: 0.572170, acc: 0.750000]\n",
      "4545: [D loss: 0.161728, acc: 0.914062]  [G loss: 0.341375, acc: 0.812500]\n",
      "4546: [D loss: 0.142379, acc: 0.945312]  [G loss: 0.530577, acc: 0.765625]\n",
      "4547: [D loss: 0.182885, acc: 0.921875]  [G loss: 0.210369, acc: 0.890625]\n",
      "4548: [D loss: 0.204243, acc: 0.898438]  [G loss: 0.746764, acc: 0.750000]\n",
      "4549: [D loss: 0.241659, acc: 0.906250]  [G loss: 0.364537, acc: 0.843750]\n",
      "4550: [D loss: 0.122481, acc: 0.945312]  [G loss: 0.363566, acc: 0.796875]\n",
      "4551: [D loss: 0.123279, acc: 0.953125]  [G loss: 0.470361, acc: 0.796875]\n",
      "4552: [D loss: 0.146210, acc: 0.953125]  [G loss: 0.349551, acc: 0.781250]\n",
      "4553: [D loss: 0.194089, acc: 0.937500]  [G loss: 1.827490, acc: 0.421875]\n",
      "4554: [D loss: 0.106199, acc: 0.953125]  [G loss: 0.938588, acc: 0.546875]\n",
      "4555: [D loss: 0.264759, acc: 0.898438]  [G loss: 0.148646, acc: 0.984375]\n",
      "4556: [D loss: 0.307667, acc: 0.882812]  [G loss: 1.176070, acc: 0.515625]\n",
      "4557: [D loss: 0.206190, acc: 0.898438]  [G loss: 0.264884, acc: 0.890625]\n",
      "4558: [D loss: 0.134027, acc: 0.929688]  [G loss: 1.045091, acc: 0.578125]\n",
      "4559: [D loss: 0.263740, acc: 0.890625]  [G loss: 0.825161, acc: 0.640625]\n",
      "4560: [D loss: 0.151042, acc: 0.937500]  [G loss: 0.524357, acc: 0.781250]\n",
      "4561: [D loss: 0.143274, acc: 0.945312]  [G loss: 0.605720, acc: 0.734375]\n",
      "4562: [D loss: 0.221379, acc: 0.914062]  [G loss: 1.193776, acc: 0.484375]\n",
      "4563: [D loss: 0.258492, acc: 0.882812]  [G loss: 0.228905, acc: 0.890625]\n",
      "4564: [D loss: 0.229020, acc: 0.882812]  [G loss: 1.148341, acc: 0.593750]\n",
      "4565: [D loss: 0.237116, acc: 0.898438]  [G loss: 0.282030, acc: 0.875000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4566: [D loss: 0.161095, acc: 0.937500]  [G loss: 1.169919, acc: 0.468750]\n",
      "4567: [D loss: 0.117359, acc: 0.937500]  [G loss: 0.291981, acc: 0.890625]\n",
      "4568: [D loss: 0.341403, acc: 0.867188]  [G loss: 0.572523, acc: 0.750000]\n",
      "4569: [D loss: 0.156199, acc: 0.937500]  [G loss: 0.425039, acc: 0.812500]\n",
      "4570: [D loss: 0.282560, acc: 0.859375]  [G loss: 0.559354, acc: 0.734375]\n",
      "4571: [D loss: 0.190639, acc: 0.929688]  [G loss: 0.676807, acc: 0.656250]\n",
      "4572: [D loss: 0.179321, acc: 0.914062]  [G loss: 1.061572, acc: 0.453125]\n",
      "4573: [D loss: 0.155535, acc: 0.937500]  [G loss: 0.277292, acc: 0.875000]\n",
      "4574: [D loss: 0.388644, acc: 0.820312]  [G loss: 1.891750, acc: 0.328125]\n",
      "4575: [D loss: 0.384524, acc: 0.828125]  [G loss: 0.166319, acc: 0.921875]\n",
      "4576: [D loss: 0.322376, acc: 0.859375]  [G loss: 1.449085, acc: 0.312500]\n",
      "4577: [D loss: 0.219358, acc: 0.906250]  [G loss: 0.299098, acc: 0.875000]\n",
      "4578: [D loss: 0.186591, acc: 0.914062]  [G loss: 0.645339, acc: 0.718750]\n",
      "4579: [D loss: 0.160876, acc: 0.945312]  [G loss: 0.567619, acc: 0.718750]\n",
      "4580: [D loss: 0.131919, acc: 0.953125]  [G loss: 0.318447, acc: 0.875000]\n",
      "4581: [D loss: 0.146132, acc: 0.945312]  [G loss: 0.952865, acc: 0.656250]\n",
      "4582: [D loss: 0.130098, acc: 0.968750]  [G loss: 0.340863, acc: 0.890625]\n",
      "4583: [D loss: 0.103795, acc: 0.976562]  [G loss: 0.177865, acc: 0.937500]\n",
      "4584: [D loss: 0.079803, acc: 0.992188]  [G loss: 0.205537, acc: 0.906250]\n",
      "4585: [D loss: 0.085853, acc: 0.960938]  [G loss: 0.309297, acc: 0.859375]\n",
      "4586: [D loss: 0.084780, acc: 0.976562]  [G loss: 0.350235, acc: 0.875000]\n",
      "4587: [D loss: 0.075233, acc: 0.976562]  [G loss: 0.221328, acc: 0.906250]\n",
      "4588: [D loss: 0.114624, acc: 0.953125]  [G loss: 0.737107, acc: 0.687500]\n",
      "4589: [D loss: 0.092896, acc: 0.960938]  [G loss: 1.277344, acc: 0.515625]\n",
      "4590: [D loss: 0.110365, acc: 0.937500]  [G loss: 0.126992, acc: 0.921875]\n",
      "4591: [D loss: 0.183051, acc: 0.914062]  [G loss: 1.745652, acc: 0.500000]\n",
      "4592: [D loss: 0.271424, acc: 0.867188]  [G loss: 0.307757, acc: 0.843750]\n",
      "4593: [D loss: 0.267803, acc: 0.921875]  [G loss: 2.463754, acc: 0.109375]\n",
      "4594: [D loss: 0.197382, acc: 0.906250]  [G loss: 0.313843, acc: 0.859375]\n",
      "4595: [D loss: 0.199021, acc: 0.898438]  [G loss: 2.093018, acc: 0.265625]\n",
      "4596: [D loss: 0.114731, acc: 0.953125]  [G loss: 0.640209, acc: 0.703125]\n",
      "4597: [D loss: 0.122183, acc: 0.960938]  [G loss: 0.715347, acc: 0.703125]\n",
      "4598: [D loss: 0.160880, acc: 0.929688]  [G loss: 1.293581, acc: 0.578125]\n",
      "4599: [D loss: 0.181469, acc: 0.921875]  [G loss: 0.380367, acc: 0.859375]\n",
      "4600: [D loss: 0.352730, acc: 0.882812]  [G loss: 3.705079, acc: 0.062500]\n",
      "4601: [D loss: 0.448712, acc: 0.820312]  [G loss: 0.500178, acc: 0.828125]\n",
      "4602: [D loss: 0.448120, acc: 0.820312]  [G loss: 0.965968, acc: 0.578125]\n",
      "4603: [D loss: 0.171521, acc: 0.914062]  [G loss: 0.627588, acc: 0.765625]\n",
      "4604: [D loss: 0.159783, acc: 0.945312]  [G loss: 0.740519, acc: 0.656250]\n",
      "4605: [D loss: 0.114826, acc: 0.960938]  [G loss: 0.527293, acc: 0.703125]\n",
      "4606: [D loss: 0.107673, acc: 0.968750]  [G loss: 0.674248, acc: 0.687500]\n",
      "4607: [D loss: 0.228270, acc: 0.921875]  [G loss: 0.393968, acc: 0.765625]\n",
      "4608: [D loss: 0.134427, acc: 0.945312]  [G loss: 0.809941, acc: 0.671875]\n",
      "4609: [D loss: 0.139296, acc: 0.945312]  [G loss: 0.559590, acc: 0.687500]\n",
      "4610: [D loss: 0.099985, acc: 0.953125]  [G loss: 0.441122, acc: 0.781250]\n",
      "4611: [D loss: 0.106638, acc: 0.960938]  [G loss: 1.005933, acc: 0.640625]\n",
      "4612: [D loss: 0.064766, acc: 0.976562]  [G loss: 1.507440, acc: 0.453125]\n",
      "4613: [D loss: 0.072493, acc: 0.976562]  [G loss: 1.150497, acc: 0.515625]\n",
      "4614: [D loss: 0.198640, acc: 0.921875]  [G loss: 0.960853, acc: 0.578125]\n",
      "4615: [D loss: 0.122606, acc: 0.945312]  [G loss: 0.404971, acc: 0.781250]\n",
      "4616: [D loss: 0.095688, acc: 0.960938]  [G loss: 0.293794, acc: 0.890625]\n",
      "4617: [D loss: 0.124871, acc: 0.945312]  [G loss: 0.911060, acc: 0.625000]\n",
      "4618: [D loss: 0.200095, acc: 0.921875]  [G loss: 0.171490, acc: 0.921875]\n",
      "4619: [D loss: 0.301778, acc: 0.898438]  [G loss: 3.770260, acc: 0.140625]\n",
      "4620: [D loss: 0.235155, acc: 0.914062]  [G loss: 1.417487, acc: 0.500000]\n",
      "4621: [D loss: 0.151221, acc: 0.960938]  [G loss: 1.162984, acc: 0.500000]\n",
      "4622: [D loss: 0.129351, acc: 0.945312]  [G loss: 1.172813, acc: 0.468750]\n",
      "4623: [D loss: 0.161031, acc: 0.921875]  [G loss: 0.123530, acc: 0.953125]\n",
      "4624: [D loss: 0.134756, acc: 0.921875]  [G loss: 0.706843, acc: 0.718750]\n",
      "4625: [D loss: 0.243253, acc: 0.921875]  [G loss: 0.249401, acc: 0.843750]\n",
      "4626: [D loss: 0.280018, acc: 0.867188]  [G loss: 2.917484, acc: 0.171875]\n",
      "4627: [D loss: 0.297539, acc: 0.875000]  [G loss: 0.761506, acc: 0.578125]\n",
      "4628: [D loss: 0.203664, acc: 0.921875]  [G loss: 1.088971, acc: 0.593750]\n",
      "4629: [D loss: 0.203598, acc: 0.914062]  [G loss: 1.066491, acc: 0.562500]\n",
      "4630: [D loss: 0.148089, acc: 0.929688]  [G loss: 0.256871, acc: 0.875000]\n",
      "4631: [D loss: 0.270967, acc: 0.898438]  [G loss: 1.207881, acc: 0.546875]\n",
      "4632: [D loss: 0.181641, acc: 0.937500]  [G loss: 0.457583, acc: 0.812500]\n",
      "4633: [D loss: 0.155619, acc: 0.953125]  [G loss: 0.608200, acc: 0.656250]\n",
      "4634: [D loss: 0.109513, acc: 0.945312]  [G loss: 0.585376, acc: 0.656250]\n",
      "4635: [D loss: 0.144957, acc: 0.937500]  [G loss: 0.326446, acc: 0.828125]\n",
      "4636: [D loss: 0.114963, acc: 0.953125]  [G loss: 0.349287, acc: 0.828125]\n",
      "4637: [D loss: 0.105687, acc: 0.953125]  [G loss: 0.300859, acc: 0.781250]\n",
      "4638: [D loss: 0.148456, acc: 0.953125]  [G loss: 0.536239, acc: 0.703125]\n",
      "4639: [D loss: 0.175966, acc: 0.953125]  [G loss: 0.795512, acc: 0.609375]\n",
      "4640: [D loss: 0.123970, acc: 0.937500]  [G loss: 0.602004, acc: 0.765625]\n",
      "4641: [D loss: 0.199336, acc: 0.929688]  [G loss: 0.136932, acc: 0.953125]\n",
      "4642: [D loss: 0.108618, acc: 0.960938]  [G loss: 1.021902, acc: 0.562500]\n",
      "4643: [D loss: 0.192941, acc: 0.929688]  [G loss: 0.202576, acc: 0.875000]\n",
      "4644: [D loss: 0.148146, acc: 0.937500]  [G loss: 1.859064, acc: 0.250000]\n",
      "4645: [D loss: 0.253079, acc: 0.906250]  [G loss: 0.259744, acc: 0.890625]\n",
      "4646: [D loss: 0.112468, acc: 0.945312]  [G loss: 0.449063, acc: 0.781250]\n",
      "4647: [D loss: 0.120814, acc: 0.945312]  [G loss: 0.268760, acc: 0.890625]\n",
      "4648: [D loss: 0.098961, acc: 0.960938]  [G loss: 0.784117, acc: 0.718750]\n",
      "4649: [D loss: 0.151964, acc: 0.945312]  [G loss: 0.307628, acc: 0.890625]\n",
      "4650: [D loss: 0.155473, acc: 0.945312]  [G loss: 0.610849, acc: 0.765625]\n",
      "4651: [D loss: 0.109527, acc: 0.953125]  [G loss: 0.325547, acc: 0.843750]\n",
      "4652: [D loss: 0.092885, acc: 0.945312]  [G loss: 0.391352, acc: 0.812500]\n",
      "4653: [D loss: 0.145989, acc: 0.945312]  [G loss: 1.738735, acc: 0.468750]\n",
      "4654: [D loss: 0.375732, acc: 0.875000]  [G loss: 0.064705, acc: 0.984375]\n",
      "4655: [D loss: 0.187030, acc: 0.929688]  [G loss: 0.699551, acc: 0.718750]\n",
      "4656: [D loss: 0.187608, acc: 0.898438]  [G loss: 0.719732, acc: 0.687500]\n",
      "4657: [D loss: 0.168946, acc: 0.929688]  [G loss: 0.628673, acc: 0.718750]\n",
      "4658: [D loss: 0.142767, acc: 0.945312]  [G loss: 0.328375, acc: 0.859375]\n",
      "4659: [D loss: 0.164694, acc: 0.929688]  [G loss: 1.315302, acc: 0.468750]\n",
      "4660: [D loss: 0.147847, acc: 0.921875]  [G loss: 0.198329, acc: 0.921875]\n",
      "4661: [D loss: 0.203159, acc: 0.921875]  [G loss: 1.269021, acc: 0.593750]\n",
      "4662: [D loss: 0.158793, acc: 0.937500]  [G loss: 0.517985, acc: 0.750000]\n",
      "4663: [D loss: 0.239180, acc: 0.914062]  [G loss: 1.150586, acc: 0.593750]\n",
      "4664: [D loss: 0.142853, acc: 0.968750]  [G loss: 0.582686, acc: 0.687500]\n",
      "4665: [D loss: 0.210681, acc: 0.914062]  [G loss: 0.632321, acc: 0.734375]\n",
      "4666: [D loss: 0.051379, acc: 0.992188]  [G loss: 0.917076, acc: 0.609375]\n",
      "4667: [D loss: 0.152008, acc: 0.937500]  [G loss: 0.478348, acc: 0.750000]\n",
      "4668: [D loss: 0.149412, acc: 0.945312]  [G loss: 1.096624, acc: 0.546875]\n",
      "4669: [D loss: 0.078527, acc: 0.968750]  [G loss: 0.786550, acc: 0.625000]\n",
      "4670: [D loss: 0.162100, acc: 0.929688]  [G loss: 0.477248, acc: 0.734375]\n",
      "4671: [D loss: 0.179475, acc: 0.914062]  [G loss: 1.225750, acc: 0.468750]\n",
      "4672: [D loss: 0.292714, acc: 0.906250]  [G loss: 0.262214, acc: 0.859375]\n",
      "4673: [D loss: 0.178177, acc: 0.937500]  [G loss: 0.671353, acc: 0.718750]\n",
      "4674: [D loss: 0.096970, acc: 0.953125]  [G loss: 0.536399, acc: 0.703125]\n",
      "4675: [D loss: 0.133422, acc: 0.960938]  [G loss: 0.421755, acc: 0.828125]\n",
      "4676: [D loss: 0.094764, acc: 0.960938]  [G loss: 0.284714, acc: 0.859375]\n",
      "4677: [D loss: 0.142700, acc: 0.929688]  [G loss: 1.350516, acc: 0.453125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4678: [D loss: 0.208759, acc: 0.890625]  [G loss: 0.188763, acc: 0.937500]\n",
      "4679: [D loss: 0.158909, acc: 0.929688]  [G loss: 0.949984, acc: 0.656250]\n",
      "4680: [D loss: 0.314578, acc: 0.898438]  [G loss: 0.140443, acc: 0.937500]\n",
      "4681: [D loss: 0.308253, acc: 0.898438]  [G loss: 1.726291, acc: 0.406250]\n",
      "4682: [D loss: 0.268494, acc: 0.882812]  [G loss: 0.251534, acc: 0.875000]\n",
      "4683: [D loss: 0.279134, acc: 0.898438]  [G loss: 0.818492, acc: 0.609375]\n",
      "4684: [D loss: 0.209324, acc: 0.906250]  [G loss: 0.214860, acc: 0.906250]\n",
      "4685: [D loss: 0.116100, acc: 0.953125]  [G loss: 0.588003, acc: 0.687500]\n",
      "4686: [D loss: 0.188008, acc: 0.929688]  [G loss: 0.090410, acc: 0.984375]\n",
      "4687: [D loss: 0.206251, acc: 0.906250]  [G loss: 0.586341, acc: 0.718750]\n",
      "4688: [D loss: 0.225083, acc: 0.906250]  [G loss: 0.108143, acc: 0.953125]\n",
      "4689: [D loss: 0.121496, acc: 0.976562]  [G loss: 0.348980, acc: 0.828125]\n",
      "4690: [D loss: 0.118731, acc: 0.953125]  [G loss: 0.417985, acc: 0.796875]\n",
      "4691: [D loss: 0.176143, acc: 0.921875]  [G loss: 1.015804, acc: 0.593750]\n",
      "4692: [D loss: 0.187682, acc: 0.937500]  [G loss: 0.176649, acc: 0.937500]\n",
      "4693: [D loss: 0.176517, acc: 0.953125]  [G loss: 0.757076, acc: 0.640625]\n",
      "4694: [D loss: 0.246568, acc: 0.914062]  [G loss: 0.187645, acc: 0.890625]\n",
      "4695: [D loss: 0.218513, acc: 0.914062]  [G loss: 1.978284, acc: 0.171875]\n",
      "4696: [D loss: 0.126378, acc: 0.937500]  [G loss: 0.214933, acc: 0.890625]\n",
      "4697: [D loss: 0.177394, acc: 0.937500]  [G loss: 0.530040, acc: 0.765625]\n",
      "4698: [D loss: 0.287152, acc: 0.914062]  [G loss: 0.355882, acc: 0.796875]\n",
      "4699: [D loss: 0.291316, acc: 0.875000]  [G loss: 1.492016, acc: 0.453125]\n",
      "4700: [D loss: 0.290520, acc: 0.875000]  [G loss: 0.215685, acc: 0.890625]\n",
      "4701: [D loss: 0.235223, acc: 0.906250]  [G loss: 0.471474, acc: 0.781250]\n",
      "4702: [D loss: 0.247002, acc: 0.890625]  [G loss: 1.096251, acc: 0.671875]\n",
      "4703: [D loss: 0.250309, acc: 0.898438]  [G loss: 0.299141, acc: 0.812500]\n",
      "4704: [D loss: 0.133537, acc: 0.953125]  [G loss: 0.368953, acc: 0.812500]\n",
      "4705: [D loss: 0.096696, acc: 0.960938]  [G loss: 0.215916, acc: 0.890625]\n",
      "4706: [D loss: 0.136600, acc: 0.945312]  [G loss: 0.362069, acc: 0.796875]\n",
      "4707: [D loss: 0.083933, acc: 0.953125]  [G loss: 0.219659, acc: 0.843750]\n",
      "4708: [D loss: 0.140053, acc: 0.945312]  [G loss: 0.337795, acc: 0.828125]\n",
      "4709: [D loss: 0.141546, acc: 0.945312]  [G loss: 0.173764, acc: 0.937500]\n",
      "4710: [D loss: 0.113077, acc: 0.945312]  [G loss: 0.443590, acc: 0.750000]\n",
      "4711: [D loss: 0.107337, acc: 0.976562]  [G loss: 0.281738, acc: 0.828125]\n",
      "4712: [D loss: 0.099736, acc: 0.960938]  [G loss: 0.425585, acc: 0.765625]\n",
      "4713: [D loss: 0.115409, acc: 0.953125]  [G loss: 0.238816, acc: 0.859375]\n",
      "4714: [D loss: 0.195295, acc: 0.906250]  [G loss: 0.436430, acc: 0.828125]\n",
      "4715: [D loss: 0.171918, acc: 0.921875]  [G loss: 0.152642, acc: 0.890625]\n",
      "4716: [D loss: 0.194901, acc: 0.921875]  [G loss: 0.560898, acc: 0.734375]\n",
      "4717: [D loss: 0.159248, acc: 0.929688]  [G loss: 0.366263, acc: 0.781250]\n",
      "4718: [D loss: 0.109118, acc: 0.953125]  [G loss: 0.758469, acc: 0.609375]\n",
      "4719: [D loss: 0.314429, acc: 0.882812]  [G loss: 0.641927, acc: 0.796875]\n",
      "4720: [D loss: 0.147621, acc: 0.945312]  [G loss: 0.261035, acc: 0.875000]\n",
      "4721: [D loss: 0.185209, acc: 0.929688]  [G loss: 1.420377, acc: 0.484375]\n",
      "4722: [D loss: 0.221349, acc: 0.898438]  [G loss: 0.191244, acc: 0.937500]\n",
      "4723: [D loss: 0.376972, acc: 0.835938]  [G loss: 4.528174, acc: 0.015625]\n",
      "4724: [D loss: 0.432651, acc: 0.875000]  [G loss: 0.828707, acc: 0.562500]\n",
      "4725: [D loss: 0.162097, acc: 0.921875]  [G loss: 1.061215, acc: 0.437500]\n",
      "4726: [D loss: 0.224015, acc: 0.937500]  [G loss: 0.288088, acc: 0.859375]\n",
      "4727: [D loss: 0.213015, acc: 0.906250]  [G loss: 1.201950, acc: 0.515625]\n",
      "4728: [D loss: 0.159251, acc: 0.945312]  [G loss: 0.442250, acc: 0.750000]\n",
      "4729: [D loss: 0.163071, acc: 0.937500]  [G loss: 0.606901, acc: 0.656250]\n",
      "4730: [D loss: 0.193297, acc: 0.945312]  [G loss: 0.628609, acc: 0.718750]\n",
      "4731: [D loss: 0.182957, acc: 0.914062]  [G loss: 0.601401, acc: 0.687500]\n",
      "4732: [D loss: 0.193421, acc: 0.937500]  [G loss: 0.274282, acc: 0.875000]\n",
      "4733: [D loss: 0.254940, acc: 0.906250]  [G loss: 1.658268, acc: 0.250000]\n",
      "4734: [D loss: 0.251886, acc: 0.898438]  [G loss: 0.396059, acc: 0.796875]\n",
      "4735: [D loss: 0.112341, acc: 0.968750]  [G loss: 0.975601, acc: 0.500000]\n",
      "4736: [D loss: 0.197671, acc: 0.937500]  [G loss: 0.534320, acc: 0.703125]\n",
      "4737: [D loss: 0.205963, acc: 0.921875]  [G loss: 0.567157, acc: 0.625000]\n",
      "4738: [D loss: 0.223691, acc: 0.898438]  [G loss: 0.978195, acc: 0.609375]\n",
      "4739: [D loss: 0.227307, acc: 0.906250]  [G loss: 0.273920, acc: 0.875000]\n",
      "4740: [D loss: 0.141708, acc: 0.960938]  [G loss: 0.821110, acc: 0.640625]\n",
      "4741: [D loss: 0.198472, acc: 0.953125]  [G loss: 0.293834, acc: 0.890625]\n",
      "4742: [D loss: 0.140523, acc: 0.929688]  [G loss: 1.814361, acc: 0.265625]\n",
      "4743: [D loss: 0.245847, acc: 0.882812]  [G loss: 0.212178, acc: 0.906250]\n",
      "4744: [D loss: 0.281165, acc: 0.875000]  [G loss: 1.893879, acc: 0.250000]\n",
      "4745: [D loss: 0.257948, acc: 0.890625]  [G loss: 0.369445, acc: 0.796875]\n",
      "4746: [D loss: 0.110043, acc: 0.953125]  [G loss: 1.033395, acc: 0.531250]\n",
      "4747: [D loss: 0.050009, acc: 0.984375]  [G loss: 0.594705, acc: 0.640625]\n",
      "4748: [D loss: 0.154511, acc: 0.937500]  [G loss: 0.218990, acc: 0.921875]\n",
      "4749: [D loss: 0.177119, acc: 0.937500]  [G loss: 0.626147, acc: 0.734375]\n",
      "4750: [D loss: 0.160745, acc: 0.945312]  [G loss: 0.393790, acc: 0.812500]\n",
      "4751: [D loss: 0.226279, acc: 0.882812]  [G loss: 0.649090, acc: 0.656250]\n",
      "4752: [D loss: 0.211408, acc: 0.914062]  [G loss: 0.128097, acc: 0.953125]\n",
      "4753: [D loss: 0.236923, acc: 0.921875]  [G loss: 1.159133, acc: 0.453125]\n",
      "4754: [D loss: 0.196554, acc: 0.937500]  [G loss: 0.275829, acc: 0.906250]\n",
      "4755: [D loss: 0.113065, acc: 0.976562]  [G loss: 0.447240, acc: 0.812500]\n",
      "4756: [D loss: 0.129735, acc: 0.960938]  [G loss: 0.270183, acc: 0.875000]\n",
      "4757: [D loss: 0.111615, acc: 0.960938]  [G loss: 0.305442, acc: 0.906250]\n",
      "4758: [D loss: 0.122575, acc: 0.953125]  [G loss: 0.270190, acc: 0.875000]\n",
      "4759: [D loss: 0.106720, acc: 0.953125]  [G loss: 0.190395, acc: 0.937500]\n",
      "4760: [D loss: 0.154851, acc: 0.945312]  [G loss: 1.121051, acc: 0.578125]\n",
      "4761: [D loss: 0.234588, acc: 0.898438]  [G loss: 0.170271, acc: 0.937500]\n",
      "4762: [D loss: 0.224470, acc: 0.906250]  [G loss: 0.846534, acc: 0.640625]\n",
      "4763: [D loss: 0.148467, acc: 0.921875]  [G loss: 0.270488, acc: 0.937500]\n",
      "4764: [D loss: 0.248570, acc: 0.882812]  [G loss: 1.494955, acc: 0.609375]\n",
      "4765: [D loss: 0.239922, acc: 0.914062]  [G loss: 0.136233, acc: 0.921875]\n",
      "4766: [D loss: 0.333231, acc: 0.890625]  [G loss: 0.328791, acc: 0.828125]\n",
      "4767: [D loss: 0.215750, acc: 0.921875]  [G loss: 0.120514, acc: 0.968750]\n",
      "4768: [D loss: 0.110146, acc: 0.953125]  [G loss: 0.031556, acc: 1.000000]\n",
      "4769: [D loss: 0.247963, acc: 0.859375]  [G loss: 1.017185, acc: 0.453125]\n",
      "4770: [D loss: 0.478321, acc: 0.789062]  [G loss: 2.851637, acc: 0.078125]\n",
      "4771: [D loss: 0.408809, acc: 0.859375]  [G loss: 0.379472, acc: 0.828125]\n",
      "4772: [D loss: 0.233526, acc: 0.898438]  [G loss: 2.328103, acc: 0.187500]\n",
      "4773: [D loss: 0.198233, acc: 0.921875]  [G loss: 1.031130, acc: 0.531250]\n",
      "4774: [D loss: 0.259841, acc: 0.898438]  [G loss: 0.959224, acc: 0.484375]\n",
      "4775: [D loss: 0.146029, acc: 0.945312]  [G loss: 0.735352, acc: 0.546875]\n",
      "4776: [D loss: 0.148543, acc: 0.929688]  [G loss: 0.402969, acc: 0.765625]\n",
      "4777: [D loss: 0.289315, acc: 0.906250]  [G loss: 1.237015, acc: 0.468750]\n",
      "4778: [D loss: 0.302902, acc: 0.843750]  [G loss: 0.352748, acc: 0.828125]\n",
      "4779: [D loss: 0.261238, acc: 0.867188]  [G loss: 0.957407, acc: 0.593750]\n",
      "4780: [D loss: 0.177827, acc: 0.937500]  [G loss: 0.483023, acc: 0.718750]\n",
      "4781: [D loss: 0.240286, acc: 0.890625]  [G loss: 0.644764, acc: 0.734375]\n",
      "4782: [D loss: 0.186356, acc: 0.921875]  [G loss: 0.160359, acc: 0.921875]\n",
      "4783: [D loss: 0.112438, acc: 0.953125]  [G loss: 0.381602, acc: 0.828125]\n",
      "4784: [D loss: 0.143609, acc: 0.945312]  [G loss: 0.153449, acc: 0.921875]\n",
      "4785: [D loss: 0.138879, acc: 0.960938]  [G loss: 0.362991, acc: 0.796875]\n",
      "4786: [D loss: 0.107319, acc: 0.945312]  [G loss: 0.273650, acc: 0.859375]\n",
      "4787: [D loss: 0.099160, acc: 0.960938]  [G loss: 0.292521, acc: 0.890625]\n",
      "4788: [D loss: 0.077948, acc: 0.968750]  [G loss: 0.116491, acc: 0.968750]\n",
      "4789: [D loss: 0.217758, acc: 0.929688]  [G loss: 0.642905, acc: 0.640625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4790: [D loss: 0.162932, acc: 0.953125]  [G loss: 0.703982, acc: 0.656250]\n",
      "4791: [D loss: 0.126720, acc: 0.945312]  [G loss: 0.171635, acc: 0.953125]\n",
      "4792: [D loss: 0.103227, acc: 0.968750]  [G loss: 0.531349, acc: 0.718750]\n",
      "4793: [D loss: 0.264890, acc: 0.882812]  [G loss: 1.027540, acc: 0.765625]\n",
      "4794: [D loss: 0.317342, acc: 0.882812]  [G loss: 1.647686, acc: 0.453125]\n",
      "4795: [D loss: 0.183885, acc: 0.929688]  [G loss: 0.301648, acc: 0.843750]\n",
      "4796: [D loss: 0.266838, acc: 0.914062]  [G loss: 1.629438, acc: 0.343750]\n",
      "4797: [D loss: 0.275255, acc: 0.906250]  [G loss: 0.448975, acc: 0.781250]\n",
      "4798: [D loss: 0.162707, acc: 0.945312]  [G loss: 1.093750, acc: 0.593750]\n",
      "4799: [D loss: 0.206716, acc: 0.914062]  [G loss: 0.247188, acc: 0.890625]\n",
      "4800: [D loss: 0.239698, acc: 0.890625]  [G loss: 1.648729, acc: 0.453125]\n",
      "4801: [D loss: 0.162148, acc: 0.914062]  [G loss: 0.462947, acc: 0.812500]\n",
      "4802: [D loss: 0.149118, acc: 0.937500]  [G loss: 1.404349, acc: 0.500000]\n",
      "4803: [D loss: 0.174556, acc: 0.906250]  [G loss: 0.341400, acc: 0.781250]\n",
      "4804: [D loss: 0.096992, acc: 0.953125]  [G loss: 0.516397, acc: 0.718750]\n",
      "4805: [D loss: 0.117083, acc: 0.953125]  [G loss: 0.354436, acc: 0.843750]\n",
      "4806: [D loss: 0.126513, acc: 0.953125]  [G loss: 0.925239, acc: 0.671875]\n",
      "4807: [D loss: 0.127828, acc: 0.929688]  [G loss: 0.439040, acc: 0.812500]\n",
      "4808: [D loss: 0.113072, acc: 0.960938]  [G loss: 0.214367, acc: 0.843750]\n",
      "4809: [D loss: 0.124965, acc: 0.929688]  [G loss: 0.561993, acc: 0.734375]\n",
      "4810: [D loss: 0.158369, acc: 0.929688]  [G loss: 1.102841, acc: 0.640625]\n",
      "4811: [D loss: 0.139181, acc: 0.921875]  [G loss: 0.793762, acc: 0.640625]\n",
      "4812: [D loss: 0.445041, acc: 0.835938]  [G loss: 0.925421, acc: 0.671875]\n",
      "4813: [D loss: 0.272091, acc: 0.882812]  [G loss: 1.367688, acc: 0.546875]\n",
      "4814: [D loss: 0.285306, acc: 0.867188]  [G loss: 0.144406, acc: 0.937500]\n",
      "4815: [D loss: 0.273601, acc: 0.875000]  [G loss: 0.883196, acc: 0.562500]\n",
      "4816: [D loss: 0.186316, acc: 0.937500]  [G loss: 0.302626, acc: 0.890625]\n",
      "4817: [D loss: 0.208045, acc: 0.921875]  [G loss: 0.944474, acc: 0.718750]\n",
      "4818: [D loss: 0.188007, acc: 0.921875]  [G loss: 0.205812, acc: 0.890625]\n",
      "4819: [D loss: 0.129572, acc: 0.960938]  [G loss: 0.721080, acc: 0.703125]\n",
      "4820: [D loss: 0.110180, acc: 0.953125]  [G loss: 0.287138, acc: 0.859375]\n",
      "4821: [D loss: 0.085705, acc: 0.968750]  [G loss: 0.343749, acc: 0.828125]\n",
      "4822: [D loss: 0.120754, acc: 0.953125]  [G loss: 0.313117, acc: 0.843750]\n",
      "4823: [D loss: 0.072325, acc: 0.984375]  [G loss: 0.165841, acc: 0.937500]\n",
      "4824: [D loss: 0.141840, acc: 0.937500]  [G loss: 0.346106, acc: 0.875000]\n",
      "4825: [D loss: 0.113152, acc: 0.953125]  [G loss: 0.322448, acc: 0.875000]\n",
      "4826: [D loss: 0.114552, acc: 0.945312]  [G loss: 1.005033, acc: 0.609375]\n",
      "4827: [D loss: 0.370325, acc: 0.898438]  [G loss: 0.126493, acc: 0.937500]\n",
      "4828: [D loss: 0.390302, acc: 0.875000]  [G loss: 1.562891, acc: 0.484375]\n",
      "4829: [D loss: 0.129092, acc: 0.945312]  [G loss: 0.423899, acc: 0.828125]\n",
      "4830: [D loss: 0.209258, acc: 0.937500]  [G loss: 0.391434, acc: 0.781250]\n",
      "4831: [D loss: 0.093600, acc: 0.976562]  [G loss: 0.179951, acc: 0.890625]\n",
      "4832: [D loss: 0.106317, acc: 0.968750]  [G loss: 0.291563, acc: 0.828125]\n",
      "4833: [D loss: 0.266542, acc: 0.906250]  [G loss: 0.488908, acc: 0.796875]\n",
      "4834: [D loss: 0.198294, acc: 0.898438]  [G loss: 0.101670, acc: 0.953125]\n",
      "4835: [D loss: 0.132424, acc: 0.953125]  [G loss: 0.784104, acc: 0.656250]\n",
      "4836: [D loss: 0.226404, acc: 0.882812]  [G loss: 0.095173, acc: 0.953125]\n",
      "4837: [D loss: 0.271314, acc: 0.906250]  [G loss: 1.123620, acc: 0.484375]\n",
      "4838: [D loss: 0.194339, acc: 0.921875]  [G loss: 0.344995, acc: 0.812500]\n",
      "4839: [D loss: 0.167380, acc: 0.921875]  [G loss: 0.337619, acc: 0.859375]\n",
      "4840: [D loss: 0.095389, acc: 0.960938]  [G loss: 0.180270, acc: 0.906250]\n",
      "4841: [D loss: 0.132868, acc: 0.945312]  [G loss: 0.868207, acc: 0.734375]\n",
      "4842: [D loss: 0.123770, acc: 0.960938]  [G loss: 0.193614, acc: 0.921875]\n",
      "4843: [D loss: 0.157613, acc: 0.945312]  [G loss: 0.313534, acc: 0.875000]\n",
      "4844: [D loss: 0.161252, acc: 0.921875]  [G loss: 0.271990, acc: 0.859375]\n",
      "4845: [D loss: 0.139481, acc: 0.960938]  [G loss: 0.616312, acc: 0.718750]\n",
      "4846: [D loss: 0.117519, acc: 0.960938]  [G loss: 0.495781, acc: 0.765625]\n",
      "4847: [D loss: 0.073007, acc: 0.960938]  [G loss: 0.283120, acc: 0.875000]\n",
      "4848: [D loss: 0.119196, acc: 0.945312]  [G loss: 0.219073, acc: 0.906250]\n",
      "4849: [D loss: 0.230132, acc: 0.906250]  [G loss: 1.613365, acc: 0.484375]\n",
      "4850: [D loss: 0.175038, acc: 0.929688]  [G loss: 0.263120, acc: 0.843750]\n",
      "4851: [D loss: 0.490374, acc: 0.859375]  [G loss: 2.476155, acc: 0.156250]\n",
      "4852: [D loss: 0.356329, acc: 0.882812]  [G loss: 0.366253, acc: 0.828125]\n",
      "4853: [D loss: 0.266233, acc: 0.882812]  [G loss: 0.852818, acc: 0.640625]\n",
      "4854: [D loss: 0.255015, acc: 0.882812]  [G loss: 0.264798, acc: 0.859375]\n",
      "4855: [D loss: 0.101850, acc: 0.945312]  [G loss: 0.441694, acc: 0.734375]\n",
      "4856: [D loss: 0.123498, acc: 0.953125]  [G loss: 0.713539, acc: 0.734375]\n",
      "4857: [D loss: 0.105257, acc: 0.960938]  [G loss: 0.391840, acc: 0.734375]\n",
      "4858: [D loss: 0.080992, acc: 0.984375]  [G loss: 0.346861, acc: 0.781250]\n",
      "4859: [D loss: 0.243169, acc: 0.906250]  [G loss: 0.171195, acc: 0.937500]\n",
      "4860: [D loss: 0.247234, acc: 0.890625]  [G loss: 1.015902, acc: 0.578125]\n",
      "4861: [D loss: 0.211007, acc: 0.937500]  [G loss: 0.295320, acc: 0.875000]\n",
      "4862: [D loss: 0.164058, acc: 0.937500]  [G loss: 0.259813, acc: 0.828125]\n",
      "4863: [D loss: 0.189748, acc: 0.921875]  [G loss: 0.255655, acc: 0.859375]\n",
      "4864: [D loss: 0.188284, acc: 0.929688]  [G loss: 1.456717, acc: 0.531250]\n",
      "4865: [D loss: 0.336685, acc: 0.867188]  [G loss: 0.272794, acc: 0.859375]\n",
      "4866: [D loss: 0.116983, acc: 0.960938]  [G loss: 0.659933, acc: 0.703125]\n",
      "4867: [D loss: 0.133791, acc: 0.953125]  [G loss: 0.301713, acc: 0.828125]\n",
      "4868: [D loss: 0.167678, acc: 0.929688]  [G loss: 0.147486, acc: 0.953125]\n",
      "4869: [D loss: 0.102651, acc: 0.945312]  [G loss: 0.430321, acc: 0.843750]\n",
      "4870: [D loss: 0.164658, acc: 0.929688]  [G loss: 1.433448, acc: 0.625000]\n",
      "4871: [D loss: 0.215177, acc: 0.906250]  [G loss: 0.188908, acc: 0.921875]\n",
      "4872: [D loss: 0.447719, acc: 0.859375]  [G loss: 2.227457, acc: 0.265625]\n",
      "4873: [D loss: 0.395156, acc: 0.843750]  [G loss: 0.621078, acc: 0.718750]\n",
      "4874: [D loss: 0.181227, acc: 0.914062]  [G loss: 1.485104, acc: 0.531250]\n",
      "4875: [D loss: 0.374904, acc: 0.875000]  [G loss: 0.290751, acc: 0.859375]\n",
      "4876: [D loss: 0.166128, acc: 0.921875]  [G loss: 0.689840, acc: 0.687500]\n",
      "4877: [D loss: 0.144984, acc: 0.929688]  [G loss: 0.537184, acc: 0.703125]\n",
      "4878: [D loss: 0.111279, acc: 0.968750]  [G loss: 0.327093, acc: 0.828125]\n",
      "4879: [D loss: 0.116840, acc: 0.953125]  [G loss: 0.128428, acc: 0.937500]\n",
      "4880: [D loss: 0.308162, acc: 0.906250]  [G loss: 0.840970, acc: 0.765625]\n",
      "4881: [D loss: 0.328453, acc: 0.859375]  [G loss: 1.543804, acc: 0.468750]\n",
      "4882: [D loss: 0.318002, acc: 0.890625]  [G loss: 0.290561, acc: 0.843750]\n",
      "4883: [D loss: 0.365742, acc: 0.843750]  [G loss: 1.045286, acc: 0.640625]\n",
      "4884: [D loss: 0.157901, acc: 0.945312]  [G loss: 0.861963, acc: 0.671875]\n",
      "4885: [D loss: 0.148641, acc: 0.929688]  [G loss: 0.419872, acc: 0.781250]\n",
      "4886: [D loss: 0.165036, acc: 0.937500]  [G loss: 0.424964, acc: 0.828125]\n",
      "4887: [D loss: 0.191017, acc: 0.921875]  [G loss: 0.293537, acc: 0.906250]\n",
      "4888: [D loss: 0.204879, acc: 0.921875]  [G loss: 0.987944, acc: 0.593750]\n",
      "4889: [D loss: 0.274926, acc: 0.882812]  [G loss: 0.180753, acc: 0.953125]\n",
      "4890: [D loss: 0.209962, acc: 0.914062]  [G loss: 0.710828, acc: 0.656250]\n",
      "4891: [D loss: 0.095539, acc: 0.960938]  [G loss: 0.313574, acc: 0.875000]\n",
      "4892: [D loss: 0.173426, acc: 0.929688]  [G loss: 1.127500, acc: 0.531250]\n",
      "4893: [D loss: 0.218836, acc: 0.921875]  [G loss: 0.390134, acc: 0.843750]\n",
      "4894: [D loss: 0.227791, acc: 0.890625]  [G loss: 2.646348, acc: 0.281250]\n",
      "4895: [D loss: 0.491655, acc: 0.820312]  [G loss: 0.157415, acc: 0.921875]\n",
      "4896: [D loss: 0.555530, acc: 0.843750]  [G loss: 1.202608, acc: 0.515625]\n",
      "4897: [D loss: 0.226764, acc: 0.890625]  [G loss: 0.262084, acc: 0.875000]\n",
      "4898: [D loss: 0.191558, acc: 0.929688]  [G loss: 0.862114, acc: 0.609375]\n",
      "4899: [D loss: 0.153897, acc: 0.921875]  [G loss: 0.745116, acc: 0.671875]\n",
      "4900: [D loss: 0.157138, acc: 0.921875]  [G loss: 0.167004, acc: 0.937500]\n",
      "4901: [D loss: 0.188778, acc: 0.929688]  [G loss: 0.773617, acc: 0.656250]\n",
      "4902: [D loss: 0.199668, acc: 0.937500]  [G loss: 0.224574, acc: 0.890625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903: [D loss: 0.149392, acc: 0.945312]  [G loss: 0.569951, acc: 0.750000]\n",
      "4904: [D loss: 0.164613, acc: 0.953125]  [G loss: 0.500448, acc: 0.750000]\n",
      "4905: [D loss: 0.149404, acc: 0.921875]  [G loss: 0.162930, acc: 0.968750]\n",
      "4906: [D loss: 0.212102, acc: 0.906250]  [G loss: 2.149575, acc: 0.140625]\n",
      "4907: [D loss: 0.338340, acc: 0.859375]  [G loss: 0.286103, acc: 0.859375]\n",
      "4908: [D loss: 0.201611, acc: 0.898438]  [G loss: 0.775758, acc: 0.625000]\n",
      "4909: [D loss: 0.178261, acc: 0.937500]  [G loss: 0.318594, acc: 0.859375]\n",
      "4910: [D loss: 0.212344, acc: 0.921875]  [G loss: 0.497514, acc: 0.703125]\n",
      "4911: [D loss: 0.151441, acc: 0.921875]  [G loss: 0.390425, acc: 0.843750]\n",
      "4912: [D loss: 0.285875, acc: 0.875000]  [G loss: 1.117645, acc: 0.484375]\n",
      "4913: [D loss: 0.155546, acc: 0.906250]  [G loss: 0.605871, acc: 0.734375]\n",
      "4914: [D loss: 0.158591, acc: 0.929688]  [G loss: 0.564814, acc: 0.750000]\n",
      "4915: [D loss: 0.253240, acc: 0.914062]  [G loss: 0.306207, acc: 0.859375]\n",
      "4916: [D loss: 0.174254, acc: 0.945312]  [G loss: 0.422370, acc: 0.796875]\n",
      "4917: [D loss: 0.176162, acc: 0.929688]  [G loss: 0.874643, acc: 0.703125]\n",
      "4918: [D loss: 0.193328, acc: 0.945312]  [G loss: 0.542152, acc: 0.750000]\n",
      "4919: [D loss: 0.281736, acc: 0.882812]  [G loss: 2.250817, acc: 0.125000]\n",
      "4920: [D loss: 0.154610, acc: 0.937500]  [G loss: 0.762911, acc: 0.640625]\n",
      "4921: [D loss: 0.232787, acc: 0.882812]  [G loss: 0.618175, acc: 0.781250]\n",
      "4922: [D loss: 0.177860, acc: 0.929688]  [G loss: 1.011474, acc: 0.531250]\n",
      "4923: [D loss: 0.321489, acc: 0.898438]  [G loss: 0.263928, acc: 0.843750]\n",
      "4924: [D loss: 0.438726, acc: 0.804688]  [G loss: 2.684682, acc: 0.078125]\n",
      "4925: [D loss: 0.326313, acc: 0.875000]  [G loss: 0.492891, acc: 0.703125]\n",
      "4926: [D loss: 0.311458, acc: 0.828125]  [G loss: 1.570910, acc: 0.375000]\n",
      "4927: [D loss: 0.223938, acc: 0.914062]  [G loss: 0.338806, acc: 0.796875]\n",
      "4928: [D loss: 0.300436, acc: 0.890625]  [G loss: 1.152995, acc: 0.562500]\n",
      "4929: [D loss: 0.196171, acc: 0.929688]  [G loss: 0.575650, acc: 0.718750]\n",
      "4930: [D loss: 0.198576, acc: 0.937500]  [G loss: 0.883024, acc: 0.531250]\n",
      "4931: [D loss: 0.183404, acc: 0.921875]  [G loss: 0.379042, acc: 0.765625]\n",
      "4932: [D loss: 0.264573, acc: 0.875000]  [G loss: 1.062240, acc: 0.421875]\n",
      "4933: [D loss: 0.239342, acc: 0.906250]  [G loss: 0.407767, acc: 0.875000]\n",
      "4934: [D loss: 0.239600, acc: 0.921875]  [G loss: 1.028224, acc: 0.593750]\n",
      "4935: [D loss: 0.244135, acc: 0.875000]  [G loss: 0.377266, acc: 0.812500]\n",
      "4936: [D loss: 0.232742, acc: 0.921875]  [G loss: 0.308304, acc: 0.796875]\n",
      "4937: [D loss: 0.196422, acc: 0.898438]  [G loss: 0.550541, acc: 0.765625]\n",
      "4938: [D loss: 0.249175, acc: 0.906250]  [G loss: 0.585828, acc: 0.687500]\n",
      "4939: [D loss: 0.109986, acc: 0.953125]  [G loss: 0.184819, acc: 0.968750]\n",
      "4940: [D loss: 0.351351, acc: 0.867188]  [G loss: 1.705637, acc: 0.296875]\n",
      "4941: [D loss: 0.346111, acc: 0.835938]  [G loss: 0.275515, acc: 0.875000]\n",
      "4942: [D loss: 0.239999, acc: 0.906250]  [G loss: 1.942231, acc: 0.250000]\n",
      "4943: [D loss: 0.206412, acc: 0.929688]  [G loss: 0.797268, acc: 0.640625]\n",
      "4944: [D loss: 0.252062, acc: 0.929688]  [G loss: 0.649557, acc: 0.718750]\n",
      "4945: [D loss: 0.156574, acc: 0.937500]  [G loss: 0.500473, acc: 0.765625]\n",
      "4946: [D loss: 0.212122, acc: 0.914062]  [G loss: 0.905906, acc: 0.625000]\n",
      "4947: [D loss: 0.151438, acc: 0.937500]  [G loss: 0.589619, acc: 0.703125]\n",
      "4948: [D loss: 0.142959, acc: 0.953125]  [G loss: 0.336987, acc: 0.859375]\n",
      "4949: [D loss: 0.173048, acc: 0.921875]  [G loss: 0.822082, acc: 0.687500]\n",
      "4950: [D loss: 0.225766, acc: 0.890625]  [G loss: 0.287046, acc: 0.828125]\n",
      "4951: [D loss: 0.297781, acc: 0.882812]  [G loss: 1.378968, acc: 0.437500]\n",
      "4952: [D loss: 0.175414, acc: 0.937500]  [G loss: 0.526312, acc: 0.765625]\n",
      "4953: [D loss: 0.197413, acc: 0.921875]  [G loss: 0.382492, acc: 0.843750]\n",
      "4954: [D loss: 0.431397, acc: 0.820312]  [G loss: 1.019423, acc: 0.703125]\n",
      "4955: [D loss: 0.217260, acc: 0.906250]  [G loss: 0.500732, acc: 0.828125]\n",
      "4956: [D loss: 0.207907, acc: 0.898438]  [G loss: 0.697114, acc: 0.734375]\n",
      "4957: [D loss: 0.220984, acc: 0.914062]  [G loss: 0.371514, acc: 0.812500]\n",
      "4958: [D loss: 0.195511, acc: 0.937500]  [G loss: 0.361514, acc: 0.765625]\n",
      "4959: [D loss: 0.204925, acc: 0.929688]  [G loss: 0.874387, acc: 0.593750]\n",
      "4960: [D loss: 0.116236, acc: 0.968750]  [G loss: 0.582857, acc: 0.734375]\n",
      "4961: [D loss: 0.244604, acc: 0.890625]  [G loss: 0.601472, acc: 0.687500]\n",
      "4962: [D loss: 0.190756, acc: 0.921875]  [G loss: 0.547173, acc: 0.765625]\n",
      "4963: [D loss: 0.268535, acc: 0.890625]  [G loss: 0.523253, acc: 0.734375]\n",
      "4964: [D loss: 0.214347, acc: 0.906250]  [G loss: 0.855746, acc: 0.750000]\n",
      "4965: [D loss: 0.165509, acc: 0.921875]  [G loss: 0.476270, acc: 0.718750]\n",
      "4966: [D loss: 0.120541, acc: 0.968750]  [G loss: 0.476925, acc: 0.781250]\n",
      "4967: [D loss: 0.224623, acc: 0.875000]  [G loss: 0.616146, acc: 0.781250]\n",
      "4968: [D loss: 0.285832, acc: 0.898438]  [G loss: 0.256981, acc: 0.828125]\n",
      "4969: [D loss: 0.398473, acc: 0.867188]  [G loss: 1.563190, acc: 0.437500]\n",
      "4970: [D loss: 0.452366, acc: 0.859375]  [G loss: 0.216012, acc: 0.890625]\n",
      "4971: [D loss: 0.273931, acc: 0.914062]  [G loss: 1.373621, acc: 0.562500]\n",
      "4972: [D loss: 0.181154, acc: 0.906250]  [G loss: 0.465979, acc: 0.796875]\n",
      "4973: [D loss: 0.188395, acc: 0.921875]  [G loss: 0.598616, acc: 0.671875]\n",
      "4974: [D loss: 0.165868, acc: 0.929688]  [G loss: 0.437583, acc: 0.765625]\n",
      "4975: [D loss: 0.166567, acc: 0.937500]  [G loss: 0.611195, acc: 0.671875]\n",
      "4976: [D loss: 0.190547, acc: 0.921875]  [G loss: 0.355937, acc: 0.828125]\n",
      "4977: [D loss: 0.074087, acc: 0.992188]  [G loss: 0.307058, acc: 0.859375]\n",
      "4978: [D loss: 0.286556, acc: 0.898438]  [G loss: 0.771972, acc: 0.750000]\n",
      "4979: [D loss: 0.193678, acc: 0.914062]  [G loss: 0.337219, acc: 0.843750]\n",
      "4980: [D loss: 0.136805, acc: 0.937500]  [G loss: 0.886284, acc: 0.640625]\n",
      "4981: [D loss: 0.146613, acc: 0.937500]  [G loss: 0.234803, acc: 0.921875]\n",
      "4982: [D loss: 0.186257, acc: 0.921875]  [G loss: 1.143379, acc: 0.500000]\n",
      "4983: [D loss: 0.147615, acc: 0.937500]  [G loss: 0.521230, acc: 0.781250]\n",
      "4984: [D loss: 0.147314, acc: 0.953125]  [G loss: 0.464845, acc: 0.734375]\n",
      "4985: [D loss: 0.274667, acc: 0.882812]  [G loss: 0.807695, acc: 0.671875]\n",
      "4986: [D loss: 0.229787, acc: 0.929688]  [G loss: 0.156738, acc: 0.906250]\n",
      "4987: [D loss: 0.471672, acc: 0.843750]  [G loss: 2.840302, acc: 0.343750]\n",
      "4988: [D loss: 0.362551, acc: 0.843750]  [G loss: 1.155052, acc: 0.609375]\n",
      "4989: [D loss: 0.092142, acc: 0.976562]  [G loss: 0.277161, acc: 0.859375]\n",
      "4990: [D loss: 0.228325, acc: 0.882812]  [G loss: 0.420434, acc: 0.718750]\n",
      "4991: [D loss: 0.129673, acc: 0.945312]  [G loss: 0.725195, acc: 0.640625]\n",
      "4992: [D loss: 0.104664, acc: 0.960938]  [G loss: 0.385271, acc: 0.812500]\n",
      "4993: [D loss: 0.223775, acc: 0.914062]  [G loss: 0.319052, acc: 0.859375]\n",
      "4994: [D loss: 0.168724, acc: 0.937500]  [G loss: 0.503266, acc: 0.796875]\n",
      "4995: [D loss: 0.088116, acc: 0.960938]  [G loss: 0.315002, acc: 0.843750]\n",
      "4996: [D loss: 0.235572, acc: 0.937500]  [G loss: 1.002766, acc: 0.718750]\n",
      "4997: [D loss: 0.321803, acc: 0.898438]  [G loss: 0.319228, acc: 0.843750]\n",
      "4998: [D loss: 0.101090, acc: 0.960938]  [G loss: 0.455695, acc: 0.796875]\n",
      "4999: [D loss: 0.127570, acc: 0.953125]  [G loss: 0.177040, acc: 0.937500]\n"
     ]
    }
   ],
   "source": [
    "def train_on_steps(X_train,DM,AM,G,steps,batch_size):\n",
    "    history = {\"d\":[],\"g\":[]}\n",
    "    for e in range(train_steps):\n",
    "        # Make generative images\n",
    "        image_batch = X_train[np.random.randint(0,X_train.shape[0],size=batch_size),:,:,:] #sample images from real data\n",
    "        noise_gen = np.random.uniform(-1,1,size=[batch_size,input_dim]) #sample image from generated data\n",
    "        generated_images = G.predict(noise_gen) #fake images\n",
    "        # Train discriminator on generated images\n",
    "        X = np.concatenate((image_batch, generated_images))\n",
    "        #create labels\n",
    "        y = np.ones([2*batch_size,1])\n",
    "        y[batch_size:,:] = 0\n",
    "        d_loss  = DM.train_on_batch(X,y)\n",
    "        history[\"d\"].append(d_loss)\n",
    "        # train Generator-Discriminator stack on input noise to non-generated output class\n",
    "        noise_tr = np.random.uniform(-1,1,size=[batch_size,input_dim])\n",
    "        y = np.ones([batch_size, 1])\n",
    "        g_loss = AM.train_on_batch(noise_tr, y)\n",
    "        history[\"g\"].append(g_loss)\n",
    "        log_mesg = \"%d: [D loss: %f, acc: %f]\" % (e, d_loss[0], d_loss[1])\n",
    "        log_mesg = \"%s  [G loss: %f, acc: %f]\" % (log_mesg, g_loss[0], g_loss[1])\n",
    "        print(log_mesg)\n",
    "    return history\n",
    "train_steps = 5000 #or few if  you want\n",
    "hist = train_on_steps(X_train,DM,AM,G,train_steps,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(hist['d'])+1)[1:],hist['d'],label='loss', linewidth=2)\n",
    "\n",
    "\n",
    "plt.legend(['loss','acc'])\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epochs')\n",
    "plt.grid(1)\n",
    "plt.title('Discriminador')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(hist['g'])+1)[1:],hist['g'],label='loss', linewidth=2)\n",
    "\n",
    "\n",
    "plt.legend(['loss','acc'])\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('epochs')\n",
    "plt.grid(1)\n",
    "plt.title('Generador')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Se generan nuevos datos artificialmente a través del modelo generador G ya entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 10\n",
    "noise = np.random.uniform(-1.0, 1.0, size=[N, input_dim]) \n",
    "images = G.predict(noise)\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(images.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    image = images[i, :, :, :]\n",
    "    image = np.reshape(image, [img_rows, img_cols])\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
